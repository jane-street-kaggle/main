{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TabNet-Training: Attentive Interpretable Tabular Learning\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-training/notebook\n",
    "\n",
    "# TabNet-Inference\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install rtdl_num_embeddings delu rtdl_revisiting_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import rtdl_num_embeddings\n",
    "from rtdl_num_embeddings import compute_bins\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import delu\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "from tanm_reference import Model, make_parameter_groups\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "import joblib\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "feature_train_list = [f\"feature_{idx:02d}\" for idx in range(79)] \n",
    "target_col = \"responder_6\"\n",
    "feature_train = feature_train_list \\\n",
    "                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n",
    "\n",
    "start_dt = 800\n",
    "end_dt = 1577\n",
    "\n",
    "feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "feature_cont = [item for item in feature_train if item not in feature_cat]\n",
    "std_feature = [i for i in feature_train_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "\n",
    "# batch_size = 2048\n",
    "batch_size = 8192\n",
    "num_epochs = 4\n",
    "\n",
    "data_stats = joblib.load(\"/kaggle/input/jane-street-data-preprocessing/data_stats.pkl\")\n",
    "means = data_stats['mean']\n",
    "stds = data_stats['std']\n",
    "\n",
    "def standardize(df, feature_cols, means, stds):\n",
    "    return df.with_columns([\n",
    "        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pl.scan_parquet(\"/kaggle/input/jane-street-data-preprocessing/training.parquet\")\n",
    "valid_original = pl.scan_parquet(\"/kaggle/input/jane-street-data-preprocessing/validation.parquet\")\n",
    "all_original = pl.concat([train_original, valid_original])\n",
    "\n",
    "# def get_category_mapping(df, column):\n",
    "#     unique_values = df.select([column]).unique().collect().to_series()\n",
    "#     return {cat: idx for idx, cat in enumerate(unique_values)}\n",
    "# category_mappings = {col: get_category_mapping(all_original, col) for col in feature_cat + ['symbol_id']}\n",
    "\n",
    "category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    " 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    " 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    " 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    " 'time_id' : {i : i for i in range(968)}}\n",
    "\n",
    "\n",
    "def encode_column(df, column, mapping):\n",
    "    def encode_category(category):\n",
    "        return mapping.get(category, -1)  \n",
    "    \n",
    "    return df.with_columns(\n",
    "        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n",
    "    )\n",
    "\n",
    "for col in feature_cat + ['symbol_id', 'time_id']:\n",
    "    train_original = encode_column(train_original, col, category_mappings[col])\n",
    "    valid_original = encode_column(valid_original, col, category_mappings[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = train_original \\\n",
    "             .filter((pl.col(\"date_id\") >= start_dt) & (pl.col(\"date_id\") <= end_dt)) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data2 = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") <= end_dt) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data = pl.concat([train_data1, train_data2])\n",
    "valid_data = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") > end_dt)\\\n",
    "             .sort(['date_id', 'time_id'])\\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_tensor = torch.tensor(train_data.collect().to_numpy(), dtype=torch.float32)\n",
    "valid_data_tensor = torch.tensor(valid_data.collect().to_numpy(), dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(train_data_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(valid_data_tensor)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "all_data = False\n",
    "if all_data:\n",
    "    train_ds = ConcatDataset([train_ds, valid_ds])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont_features = 85\n",
    "# n_cont_features = 89\n",
    "n_cat_features = 5\n",
    "n_classes = None\n",
    "cat_cardinalities = [23, 10, 32, 40, 969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true))\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TabM\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# TabM-mini with the piecewise-linear embeddings.\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins_input = train_data_tensor[:, :-4][:, [col for col in range(train_data_tensor[:, :-4].shape[1]) if col not in [9, 10, 11]]]\n",
    "# bins = compute_bins(bins_input[torch.randperm(len(bins_input))[:1000000]], ...)\n",
    "\n",
    "# del bins_input\n",
    "# gc.collect()\n",
    "\n",
    "k = 32 # é›†æˆè¾“å‡ºçš„æ•°é‡\n",
    "model = Model(\n",
    "    n_num_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=n_classes,\n",
    "    backbone={        \n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 ,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.25,\n",
    "    },\n",
    "    bins=bins,\n",
    "    # num_embeddings=(\n",
    "    #     None\n",
    "    #     if bins is None\n",
    "    #     else {\n",
    "    #         'type': 'PiecewiseLinearEmbeddings',\n",
    "    #         'd_embedding': 16,\n",
    "    #         'activation': True,\n",
    "    #         'version': 'B',\n",
    "    #     }\n",
    "    # ),\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        # {\n",
    "        #     'type': 'PeriodicEmbeddings',\n",
    "        #     'd_embedding': 16,\n",
    "        #     'lite':True,\n",
    "        # }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=k,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    # Instead of model.parameters(),\n",
    "    make_parameter_groups(model),\n",
    "    lr=1e-4,\n",
    "    weight_decay=5e-3 ,\n",
    ")\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "class R2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "# loss_fn = nn.HuberLoss(delta=0.2)\n",
    "loss_fn = R2Loss()\n",
    "\n",
    "timer = delu.tools.Timer()\n",
    "patience = 5\n",
    "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
    "best = {\n",
    "    \"val\": -math.inf,\n",
    "    \"epoch\": -1,\n",
    "}\n",
    "timer.run()\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "    weighted_residual_sum = np.sum(residuals)\n",
    "\n",
    "    # Calculate weighted sum of squared true values (denominator)\n",
    "    weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "\n",
    "    # Calculate weighted R2\n",
    "    r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "\n",
    "    return r2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Training\n",
    "    train_pred_list = []\n",
    "    with tqdm(train_dl, total=len(train_dl), leave=True) as phar:\n",
    "        for train_tensor in phar:\n",
    "            optimizer.zero_grad()\n",
    "            X_input = train_tensor[0][:, :-4].to(device)\n",
    "            y_input = train_tensor[0][:, -4].to(device)\n",
    "            w_input = train_tensor[0][:, -3].to(device)\n",
    "\n",
    "            \n",
    "            symbol_input = train_tensor[0][:, -2].to(device)\n",
    "            time_input = train_tensor[0][:, -1].to(device)\n",
    "                \n",
    "            x_cont_input = X_input[:, [col for col in range(X_input.shape[1]) if col not in [9, 10, 11]]]\n",
    "            x_cont_input = x_cont_input + torch.randn_like(x_cont_input) * 0.035\n",
    "            \n",
    "            x_cat_input = X_input[:, [9, 10, 11]]\n",
    "            x_cat_input = (torch.concat([x_cat_input, symbol_input.unsqueeze(-1), time_input.unsqueeze(-1)], axis=1)).to(torch.int64)\n",
    "\n",
    "            \n",
    "\n",
    "            output = model(x_cont_input, x_cat_input).squeeze(-1)\n",
    "            loss = loss_fn(output.flatten(0, 1), y_input.repeat_interleave(k))\n",
    "\n",
    "            train_pred_list.append((output.mean(1), y_input, w_input))\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            phar.set_postfix(\n",
    "                OrderedDict(\n",
    "                    epoch=f'{epoch+1}/{num_epochs}',\n",
    "                    loss=f'{loss.item():.6f}',\n",
    "                    lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                )\n",
    "            )\n",
    "            phar.update(1)\n",
    "\n",
    "    weights_train = torch.cat([x[2] for x in train_pred_list]).cpu().numpy()\n",
    "    y_train = torch.cat([x[1] for x in train_pred_list]).cpu().numpy()\n",
    "    prob_train = torch.cat([x[0] for x in train_pred_list]).detach().cpu().numpy()\n",
    "    train_r2 = r2_val(y_train, prob_train, weights_train)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_pred_list = []\n",
    "    for valid_tensor in tqdm(valid_dl):\n",
    "        X_valid = valid_tensor[0][:, :-4].to(device)\n",
    "        y_valid = valid_tensor[0][:, -4].to(device)\n",
    "        w_valid = valid_tensor[0][:, -3].to(device)\n",
    "        symbol_valid = valid_tensor[0][:, -2].to(device)\n",
    "        time_valid = valid_tensor[0][:, -1].to(device)\n",
    "        \n",
    "        x_cont_valid = X_valid[:, [col for col in range(X_valid.shape[1]) if col not in [9, 10, 11]]]\n",
    "        x_cont_valid = x_cont_valid + torch.randn_like(x_cont_valid) * 0.035\n",
    "        \n",
    "        x_cat_valid = X_valid[:, [9, 10, 11]]\n",
    "        x_cat_valid = (torch.concat([x_cat_valid, symbol_valid.unsqueeze(-1),time_valid.unsqueeze(-1)], axis=1)).to(torch.int64)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_cont_valid, x_cat_valid).squeeze(-1)\n",
    "    \n",
    "        # val_loss = loss_fn(y_pred.squeeze(-1).squeeze(-1).cpu().detach(), y_valid)\n",
    "        val_loss = loss_fn(y_pred.flatten(0, 1), y_valid.repeat_interleave(k))\n",
    "        valid_loss_list.append(val_loss)\n",
    "        valid_pred_list.append((y_pred.mean(1), y_valid, w_valid))\n",
    "    \n",
    "    valid_loss_mean = sum(valid_loss_list) / len(valid_loss_list)\n",
    "    # val_r2 = r2_score(y_valid_data, torch.cat(valid_pred_list).numpy(), sample_weight=w_valid_data)\n",
    "\n",
    "    weights_eval = torch.cat([x[2] for x in valid_pred_list]).cpu().numpy()\n",
    "    y_eval = torch.cat([x[1] for x in valid_pred_list]).cpu().numpy()\n",
    "    prob_eval = torch.cat([x[0] for x in valid_pred_list]).cpu().numpy()\n",
    "    val_r2 = r2_val(y_eval, prob_eval, weights_eval)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: train_r2 = {train_r2:.6f}, val_loss_mean={valid_loss_mean:.6f}, val_r2={val_r2:.6f}, [time] {timer}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if val_r2 > best[\"val\"]:\n",
    "        print(\"ðŸŒ¸ New best epoch! ðŸŒ¸\")\n",
    "        best = {\"val\": val_r2, \"epoch\": epoch}\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'r2': val_r2,\n",
    "        }\n",
    "        torch.save(checkpoint, f'epoch{epoch}_r2_{val_r2}.pt')\n",
    "    print()\n",
    "    \n",
    "    early_stopping.update(val_r2)\n",
    "    if early_stopping.should_stop():\n",
    "        print(\"Early stop\")\n",
    "        break\n",
    "\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # 'r2': val_r2,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
