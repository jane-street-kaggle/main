{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TabNet-Training: Attentive Interpretable Tabular Learning\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-training/notebook\n",
    "\n",
    "# TabNet-Inference\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install rtdl_num_embeddings delu rtdl_revisiting_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import rtdl_num_embeddings\n",
    "from rtdl_num_embeddings import compute_bins\n",
    "import rtdl_revisiting_models\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import delu\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "import joblib\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/kaggle/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "feature_train_list = [f\"feature_{idx:02d}\" for idx in range(79)] \n",
    "target_col = \"responder_6\"\n",
    "feature_train = feature_train_list \\\n",
    "                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n",
    "\n",
    "start_dt = 800\n",
    "end_dt = 1577\n",
    "\n",
    "feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "feature_cont = [item for item in feature_train if item not in feature_cat]\n",
    "std_feature = [i for i in feature_train_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "\n",
    "batch_size = 1024\n",
    "# batch_size = 8192\n",
    "num_epochs = 20\n",
    "\n",
    "data_stats = joblib.load(\"/kaggle/input/jane-street-data-preprocessing/data_stats.pkl\")\n",
    "means = data_stats['mean']\n",
    "stds = data_stats['std']\n",
    "\n",
    "def standardize(df, feature_cols, means, stds):\n",
    "    return df.with_columns([\n",
    "        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/training.parquet\")\n",
    "valid_original = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet\")\n",
    "all_original = pl.concat([train_original, valid_original])\n",
    "\n",
    "def get_category_mapping(df, column):\n",
    "    unique_values = df.select([column]).unique().collect().to_series()\n",
    "    return {cat: idx for idx, cat in enumerate(unique_values)}\n",
    "category_mappings = {col: get_category_mapping(all_original, col) for col in feature_cat + ['symbol_id', 'time_id']}\n",
    "\n",
    "_category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    " 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    " 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    " 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    " 'time_id' : {i : i for i in range(968)}}\n",
    "\n",
    "def encode_column(df, column, mapping):\n",
    "    def encode_category(category):\n",
    "        return mapping.get(category, -1)  \n",
    "    \n",
    "    return df.with_columns(\n",
    "        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n",
    "    )\n",
    "\n",
    "for col in feature_cat + ['symbol_id', 'time_id']:\n",
    "    train_original = encode_column(train_original, col, category_mappings[col])\n",
    "    valid_original = encode_column(valid_original, col, category_mappings[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = train_original \\\n",
    "             .filter((pl.col(\"date_id\") >= start_dt) & (pl.col(\"date_id\") <= end_dt)) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data2 = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") <= end_dt) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data = pl.concat([train_data1, train_data2])\n",
    "valid_data = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") > end_dt)\\\n",
    "             .sort(['date_id', 'time_id'])\\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor = torch.tensor(train_data.collect().to_numpy(), dtype=torch.float32)\n",
    "valid_data_tensor = torch.tensor(valid_data.collect().to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=1, pin_memory=False, shuffle=True)\n",
    "valid_ds = TensorDataset(valid_data_tensor)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=1, pin_memory=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = False\n",
    "if all_data:\n",
    "    train_ds = ConcatDataset([train_ds, valid_ds])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont_features = 85\n",
    "n_cat_features = 5\n",
    "n_classes = None\n",
    "cat_cardinalities = [23, 10, 32, 40, 969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ...,  True, False,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins_input = train_data_tensor[:, :-4][:, [col for col in range(train_data_tensor[:, :-4].shape[1]) if col not in [9, 10, 11]]]\n",
    "nan_mask = torch.isnan(bins_input)\n",
    "inf_mask = torch.isinf(bins_input)\n",
    "valid_rows = ~(nan_mask.any(dim=1) | inf_mask.any(dim=1))\n",
    "valid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bins_input_clean = bins_input[valid_rows][:1_000_000]\n",
    "bins = compute_bins(bins_input_clean , n_bins=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parameter_groups(model):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # Gradientê°€ í•„ìš” ì—†ëŠ” ê²½ìš° ìƒëžµ\n",
    "        if 'cont_embeddings' in name or 'bias' in name:\n",
    "            # Embedding ë ˆì´ì–´ì™€ biasì—ëŠ” weight decayë¥¼ ì ìš©í•˜ì§€ ì•ŠìŒ\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ì—ëŠ” weight decay ì ìš©\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {'params': no_decay, 'weight_decay': 0.0},\n",
    "        {'params': decay, 'weight_decay': 5e-3}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features: int,\n",
    "        cat_cardinalities: list[int],\n",
    "        bins: Optional[list[Tensor]],\n",
    "        mlp_kwargs: dict,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.cat_cardinalities = cat_cardinalities\n",
    "        # The total representation size for categorical features\n",
    "        # == the sum of one-hot representation sizes\n",
    "        # == the sum of the numbers of distinct values of all features.\n",
    "        d_cat = sum(cat_cardinalities)\n",
    "\n",
    "        # Choose any of the embeddings below.\n",
    "\n",
    "        # d_embedding = 24\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "        #     n_cont_features, d_embedding, lite=False\n",
    "        # )\n",
    "        # d_num = n_cont_features * d_embedding\n",
    "\n",
    "        # assert bins is not None\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEncoding(bins)\n",
    "        # d_num = sum(len(b) - 1 for b in bins)\n",
    "\n",
    "        assert bins is not None\n",
    "        d_embedding = 8\n",
    "        self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "            bins, d_embedding, activation=False, version='B'\n",
    "        )\n",
    "        d_num = n_cont_features * d_embedding\n",
    "\n",
    "        # d_embedding = 32\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.LinearReLUEmbeddings(\n",
    "        #     n_cont_features, d_embedding\n",
    "        # )\n",
    "        # d_num = n_cont_features * d_embedding\n",
    "\n",
    "        self.backbone = rtdl_revisiting_models.MLP(d_in=d_num + d_cat, **mlp_kwargs)\n",
    "\n",
    "    def forward(self, x_cont: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        x = []\n",
    "\n",
    "        # Step 1. Embed the continuous features.\n",
    "        # Flattening is needed for MLP-like models.\n",
    "        x.append(self.cont_embeddings(x_cont).flatten(1))\n",
    "\n",
    "        # Step 2. Encode the categorical features using any strategy.\n",
    "        if x_cat is not None:\n",
    "            x.extend(\n",
    "                F.one_hot(column, cardinality)\n",
    "                for column, cardinality in zip(x_cat.T, self.cat_cardinalities)\n",
    "            )\n",
    "\n",
    "        # Step 3. Assemble the vector input for the backbone.\n",
    "        x = torch.column_stack(x)\n",
    "\n",
    "        # Step 4. Apply the backbone.\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = 'regression'\n",
    "model = Model(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    bins=bins,\n",
    "    mlp_kwargs={        \n",
    "        'n_blocks': 3,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.25,\n",
    "        'd_out': n_classes if task_type == 'multiclass' else 1,\n",
    "    },\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    # make_parameter_groups(model),\n",
    "    lr=1e-4,\n",
    "    weight_decay=5e-3 ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true))\n",
    "        return torch.mean(loss)\n",
    "# loss_fn = nn.HuberLoss(delta=0.2)\n",
    "loss_fn = R2Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "timer = delu.tools.Timer()\n",
    "patience = 5\n",
    "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
    "best = {\n",
    "    \"val\": -math.inf,\n",
    "    \"epoch\": -1,\n",
    "}\n",
    "timer.run()\n",
    "\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "    weighted_residual_sum = np.sum(residuals)\n",
    "\n",
    "    # Calculate weighted sum of squared true values (denominator)\n",
    "    weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "\n",
    "    # Calculate weighted R2\n",
    "    r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export POLARS_ALLOW_FORKING_THREAD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 11502/26687 [04:26<06:51, 36.94it/s, epoch=1/4, loss=0.882956, lr=1.000e-04]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_pred_list = []\n",
    "    with tqdm(train_dl, total=len(train_dl), leave=True) as phar:\n",
    "        i = 0\n",
    "        for train_tensor in phar:\n",
    "            optimizer.zero_grad()\n",
    "            X_input = train_tensor[0][:, :-4].to(device)\n",
    "            y_input = train_tensor[0][:, -4].to(device)\n",
    "            w_input = train_tensor[0][:, -3].to(device)\n",
    "            symbol_input    = train_tensor[0][:, -2].to(device)\n",
    "            time_input      = train_tensor[0][:, -1].to(device)            \n",
    "            x_cont_input = X_input[:, [col for col in range(X_input.shape[1]) if col not in [9, 10, 11]]]\n",
    "\n",
    "            x_cat_input = X_input[:, [9, 10, 11]]\n",
    "            x_cat_input = torch.concat(\n",
    "                [x_cat_input, symbol_input.unsqueeze(-1), time_input.unsqueeze(-1)], axis=1\n",
    "            ).to(torch.int64)\n",
    "\n",
    "            # Replace NaNs/Infs in x_cont_input with 0\n",
    "            x_cont_input = torch.where(torch.isnan(x_cont_input) | torch.isinf(x_cont_input), torch.tensor(0.0, device=x_cont_input.device), x_cont_input)\n",
    "            output = model(x_cont_input, x_cat_input).squeeze(-1)\n",
    "            loss = loss_fn(output, y_input)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                model.parameters(), \n",
    "                clip_value=1.0,\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                train_pred_list.append((output, y_input, w_input))\n",
    "                phar.set_postfix(\n",
    "                    OrderedDict(\n",
    "                        epoch=f'{epoch + 1}/{num_epochs}',\n",
    "                        loss=f'{loss.item():.6f}',\n",
    "                        lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                    )\n",
    "                )\n",
    "            phar.update(1)\n",
    "            i += 1\n",
    "\n",
    "    weights_train = torch.cat([x[2] for x in train_pred_list]).cpu().numpy()\n",
    "    y_train = torch.cat([x[1] for x in train_pred_list]).cpu().numpy()\n",
    "    prob_train = torch.cat([x[0] for x in train_pred_list]).detach().cpu().numpy()\n",
    "    train_r2 = r2_val(y_train, prob_train, weights_train)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_pred_list = []\n",
    "    with tqdm(valid_dl, total=len(valid_dl), leave=True) as phar:\n",
    "        for valid_tensor in phar:\n",
    "            X_valid = valid_tensor[0][:, :-4].to(device)\n",
    "            y_valid = valid_tensor[0][:, -4].to(device)\n",
    "            w_valid = valid_tensor[0][:, -3].to(device)\n",
    "            symbol_valid = valid_tensor[0][:, -2].to(device)\n",
    "            time_valid = valid_tensor[0][:, -1].to(device)\n",
    "            \n",
    "            x_cont_valid = X_valid[:, [col for col in range(X_valid.shape[1]) if col not in [9, 10, 11]]]\n",
    "            x_cont_valid = torch.where(\n",
    "                torch.isnan(x_cont_valid) | torch.isinf(x_cont_valid),\n",
    "                torch.tensor(0.0, device=x_cont_valid.device), \n",
    "            x_cont_valid)\n",
    "            \n",
    "            x_cat_valid = X_valid[:, [9, 10, 11]]\n",
    "            x_cat_valid = (torch.concat(\n",
    "                [x_cat_valid, symbol_valid.unsqueeze(-1),time_valid.unsqueeze(-1)], axis=1)\n",
    "            ).to(torch.int64)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_cont_valid, x_cat_valid).squeeze(-1)\n",
    "\n",
    "            val_loss = loss_fn(y_pred, y_valid)\n",
    "            valid_loss_list.append(val_loss)\n",
    "            valid_pred_list.append((y_pred, y_valid, w_valid))\n",
    "            phar.set_postfix(\n",
    "                OrderedDict(\n",
    "                    epoch=f'{epoch + 1}/{num_epochs}',\n",
    "                    val_loss=f'{val_loss.item():.6f}',\n",
    "                    lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                )\n",
    "            )\n",
    "            phar.update(1)\n",
    "            i += 1\n",
    "\n",
    "        valid_loss_mean = sum(valid_loss_list) / len(valid_loss_list)\n",
    "        weights_eval = torch.cat([x[2] for x in valid_pred_list]).cpu().numpy()\n",
    "        y_eval = torch.cat([x[1] for x in valid_pred_list]).cpu().numpy()\n",
    "        prob_eval = torch.cat([x[0] for x in valid_pred_list]).cpu().numpy()\n",
    "        val_r2 = r2_val(y_eval, prob_eval, weights_eval)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: train_r2 = {train_r2:.6f}, val_loss_mean={valid_loss_mean:.6f}, val_r2={val_r2:.6f}, [time] {timer}\")\n",
    "\n",
    "        if val_r2 > best[\"val\"]:\n",
    "            print(\"ðŸŒ¸ New best epoch! ðŸŒ¸\")\n",
    "            best = {\"val\": val_r2, \"epoch\": epoch}\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'r2': val_r2,\n",
    "        }\n",
    "        torch.save(checkpoint, f'epoch{epoch}_r2_{val_r2}.pt')\n",
    "    print()\n",
    "    early_stopping.update(val_r2)\n",
    "    if early_stopping.should_stop():\n",
    "        print(\"Early stop\")\n",
    "        break\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'r2': val_r2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
