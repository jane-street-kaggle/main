{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":6.297035,"end_time":"2025-01-12T12:27:28.568151","exception":false,"start_time":"2025-01-12T12:27:22.271116","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install rtdl_num_embeddings -q --no-index --find-links=/kaggle/input/jane-street-import/rtdl_num_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":20.131559,"end_time":"2025-01-12T12:27:48.707377","exception":false,"start_time":"2025-01-12T12:27:28.575818","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# imports\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","import pickle\n","import gc\n","import warnings\n","import math\n","from collections import OrderedDict\n","\n","# plotting and progress bar\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","# PyTorch Lightning\n","from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n","\n","# Scikit-learn\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import VotingRegressor\n","\n","# Gradient Boosting Models\n","import lightgbm as lgb\n","from lightgbm import LGBMRegressor\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor\n","\n","# Miscellaneous\n","import sys\n","from tanm_reference import Model, make_parameter_groups\n","import joblib\n","import dill\n","\n","# Brian Add\n","import statistics as stat\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","import copy\n","\n","# Kaggle Evaluation\n","import kaggle_evaluation.jane_street_inference_server\n","\n","# Warnings and Polars table settings\n","warnings.filterwarnings(\"ignore\")\n","pd.options.display.max_columns = None\n","pl.Config.set_tbl_rows(100)\n","pl.Config.set_tbl_cols(400)\n","pl.Config.set_fmt_table_cell_list_len(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.015025,"end_time":"2025-01-12T12:27:48.729845","exception":false,"start_time":"2025-01-12T12:27:48.714820","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class CONFIG:\n","    seed = 42\n","    target_col = \"responder_6\"\n","    feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","    \n","    model_paths = [\n","        \"/kaggle/input/js-xs-nn-trained-model\",\n","    ]\n","    debug = False\n","    lag_cols_rename = { f\"responder_{idx}_lag_1\" : f\"responder_{idx}\" for idx in range(9)}\n","    lag_target_cols_name = [f\"responder_{idx}\" for idx in range(9)]\n","    lag_cols_original = [\"date_id\", \"time_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n","    model_path = \"/kaggle/input/janestreet-public-model/xgb_001.pkl\"\n","    lag_ndays = 4\n","    all_cols = [\"date_id\", \"symbol_id\", \"time_id\", \"weight\"] + [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)] + [target_col]\n","    test_cols = [\"row_id\", \"date_id\", \"symbol_id\", \"time_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)] + [target_col]\n","    feature_colss = [\"symbol_id\", \"time_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","    #기억해두자 온라인 리트레인 feature cols를 feature colss로 바꿨다.\n","    only_features = [\"row_id\", \"date_id\", \"symbol_id\", \"time_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]\n","    only_lags = [\"row_id\", \"date_id\", \"symbol_id\", \"time_id\"] + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n","    data_paths = [\"/kaggle/input/lgbm-model-training/lgbm_model_0.json\",\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet/\"]\n","    retrain = True\n","    EVAL = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.012418,"end_time":"2025-01-12T12:27:48.749408","exception":false,"start_time":"2025-01-12T12:27:48.736990","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_agg_list(day, columns):\n","    agg_mean_list = [pl.col(c).mean().name.suffix(f\"_mean_{day}d\") for c in columns]\n","    agg_std_list = [pl.col(c).std().name.suffix(f\"_std_{day}d\") for c in columns]\n","    agg_max_list = [pl.col(c).max().name.suffix(f\"_max_{day}d\") for c in columns]\n","    agg_last_list = [pl.col(c).last().name.suffix(f\"_last_{day}d\") for c in columns]\n","    agg_list = agg_mean_list + agg_std_list + agg_max_list + agg_last_list\n","    return agg_list"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.019838,"end_time":"2025-01-12T12:27:48.776699","exception":false,"start_time":"2025-01-12T12:27:48.756861","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Custom R2 metric for validation\n","def r2_val(y_true, y_pred, sample_weight):\n","    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n","    return r2\n","\n","\n","class NN(LightningModule):\n","    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        layers = []\n","        in_dim = input_dim\n","        for i, hidden_dim in enumerate(hidden_dims):\n","            layers.append(nn.BatchNorm1d(in_dim))\n","            if i > 0:\n","                layers.append(nn.SiLU())\n","            if i < len(dropouts):\n","                layers.append(nn.Dropout(dropouts[i]))\n","            layers.append(nn.Linear(in_dim, hidden_dim))\n","            # layers.append(nn.ReLU())\n","            in_dim = hidden_dim\n","        layers.append(nn.Linear(in_dim, 1))  # 输出层\n","        layers.append(nn.Tanh())\n","        self.model = nn.Sequential(*layers)\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","        self.validation_step_outputs = []\n","\n","    def forward(self, x):\n","        return 5 * self.model(x).squeeze(-1)  # 输出为一维张量\n","\n","    def training_step(self, batch):\n","        x, y, w = batch\n","        y_hat = self(x)\n","        loss = F.mse_loss(y_hat, y, reduction='none') * w  # 考虑样本权重\n","        loss = loss.mean()\n","        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x, y, w = batch\n","        y_hat = self(x)\n","        loss = F.mse_loss(y_hat, y, reduction='none') * w\n","        loss = loss.mean()\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n","        self.validation_step_outputs.append((y_hat, y, w))\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n","        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n","        if self.trainer.sanity_checking:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","        else:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n","            # r2_val\n","            val_r_square = r2_val(y, prob, weights)\n","            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.validation_step_outputs.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n","                                                               verbose=True)\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'monitor': 'val_loss',\n","            }\n","        }\n","\n","    def on_train_epoch_end(self):\n","        if self.trainer.sanity_checking:\n","            return\n","        epoch = self.trainer.current_epoch\n","        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n","        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n","        print(f\"Epoch {epoch}: {formatted_metrics}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.011681,"end_time":"2025-01-12T12:27:48.795359","exception":false,"start_time":"2025-01-12T12:27:48.783678","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def calculate_r2(y_true, y_pred, weights):\n","    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n","    denominator = np.sum(weights * (y_true ** 2))\n","    r2_score = 1 - (numerator / denominator)\n","    return r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.011829,"end_time":"2025-01-12T12:27:48.813976","exception":false,"start_time":"2025-01-12T12:27:48.802147","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["TARGET = 'responder_6'\n","FEAT_COLS_CAT = [f\"feature_{i:02d}\" for i in range(79)]\n","FEAT_COLS_LGB = [f\"feature_{i:02d}\" for i in range(79)]+ ['responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1',\n","       'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1',\n","       'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 이거는 catboost 일때\n","\n","model_path = '/kaggle/input/jsmodel-chan-lgbupdate'\n","cat_file_name = 'catboost_models'\n","lgb_file_name = 'lgb_models'\n","\n","lgb_models = joblib.load(f'{model_path}/{lgb_file_name}.pkl')\n","catboost_models = joblib.load(f'{model_path}/{cat_file_name}.pkl')\n","\n","catboost_holdout_model = joblib.load(f'/kaggle/input/jsmodel-chan-catholdout/catboost_holdout_model.pkl')\n","\n","print(f\"Loaded model from the saved file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.130356,"end_time":"2025-01-12T12:28:14.295966","exception":false,"start_time":"2025-01-12T12:28:14.165610","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["with open( CONFIG.model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    \n","model = result[\"model\"]\n","features = result[\"features\"]\n","print(len(features))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_list = [f\"feature_{idx:02d}\" for idx in range(79) if idx != 61]\n","\n","target_col = \"responder_6\" \n","\n","feature_test = feature_list \\\n","                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n","\n","feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n","feature_cont = [item for item in feature_test if item not in feature_cat]\n","\n","batch_size = 8192\n","\n","std_feature = [i for i in feature_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","\n","data_stats = joblib.load(\"/kaggle/input/my-own-js/data_stats.pkl\")\n","means = data_stats['mean']\n","stds = data_stats['std']\n","\n","def standardize(df, feature_cols, means, stds):\n","    return df.with_columns([\n","        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n"," 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n"," 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n","  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n"," 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n","  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n"," 'time_id' : {i : i for i in range(968)}}\n","\n","def encode_column(df, column, mapping):\n","    max_value = max(mapping.values())  \n","\n","    def encode_category(category):\n","        return mapping.get(category, max_value + 1)  \n","    \n","    return df.with_columns(\n","        pl.col(column).map_elements(encode_category).alias(column)\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.898667,"end_time":"2025-01-12T12:28:15.270293","exception":false,"start_time":"2025-01-12T12:28:14.371626","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class R2Loss(nn.Module):\n","    def __init__(self):\n","        super(R2Loss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        mse_loss = torch.sum((y_pred - y_true) ** 2)\n","        var_y = torch.sum(y_true ** 2)\n","        loss = mse_loss / (var_y + 1e-38)\n","        return loss\n","\n","class TAB(LightningModule):\n","    def __init__(self, n_cont_features, cat_cardinalities, n_classes, lr, weight_decay):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.k = 16\n","        self.model = Model(\n","                n_num_features=n_cont_features,\n","                cat_cardinalities=cat_cardinalities,\n","                n_classes=n_classes,\n","                backbone={\n","                    'type': 'MLP',\n","                    'n_blocks': 3 ,\n","                    'd_block': 512,\n","                    'dropout': 0.25,\n","                },\n","                bins=None,\n","                num_embeddings= None,\n","                arch_type='tabm',\n","                k=self.k,\n","            )\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","        self.training_step_outputs = []\n","        self.validation_step_outputs = []\n","        self.loss_fn = R2Loss()\n","        # self.loss_fn = weighted_mse_loss\n","\n","    def forward(self, x_cont, x_cat):\n","        return self.model(x_cont, x_cat).squeeze(-1)\n","\n","    def training_step(self, batch):\n","        x_cont,x_cat, y, w , w_y= batch\n","        x_cont = x_cont + torch.randn_like(x_cont) * 0.02\n","        y_hat = self(x_cont, x_cat)\n","        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n","        loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n","        self.training_step_outputs.append((y_hat.mean(1), y, w))\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x_cont,x_cat, y, w, w_y = batch\n","        x_cont = x_cont + torch.randn_like(x_cont) * 0.02\n","        y_hat = self(x_cont, x_cat)\n","        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n","        loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n","        self.validation_step_outputs.append((y_hat.mean(1), y, w))\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n","        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n","        if self.trainer.sanity_checking:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","        else:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n","            # r2_val\n","            val_r_square = r2_val(y, prob, weights)\n","            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.validation_step_outputs.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(make_parameter_groups(self.model), lr=self.lr, weight_decay=self.weight_decay)\n","        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5,\n","        #                                                        verbose=True)\n","        return {\n","            'optimizer': optimizer,\n","            # 'lr_scheduler': {\n","            #     'scheduler': scheduler,\n","            #     'monitor': 'val_r_square',\n","            # }\n","        }\n","\n","    def on_train_epoch_end(self):\n","        if self.trainer.sanity_checking:\n","            return\n","\n","        y = torch.cat([x[1] for x in self.training_step_outputs]).cpu().numpy()\n","        prob = torch.cat([x[0] for x in self.training_step_outputs]).detach().cpu().numpy()\n","        weights = torch.cat([x[2] for x in self.training_step_outputs]).cpu().numpy()\n","        # r2_training\n","        train_r_square = r2_val(y, prob, weights)\n","        self.log(\"train_r_square\", train_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.training_step_outputs.clear()\n","\n","        epoch = self.trainer.current_epoch\n","        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n","        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n","        print(f\"Epoch {epoch}: {formatted_metrics}\")\n","        \n","class custom_args():\n","    def __init__(self):\n","        self.usegpu = True\n","        self.gpuid = 0\n","        self.seed = 42\n","        self.model = 'nn'\n","        self.use_wandb = False\n","        self.project = 'js-tabm-with-lags'\n","        self.dname = \"./input_df/\"\n","        self.loader_workers = 10   \n","        self.bs = 8192\n","        self.lr = 1e-3\n","        self.weight_decay = 8e-4\n","        self.n_cont_features = 84\n","        self.n_cat_features = 5\n","        self.n_classes = None\n","        self.cat_cardinalities = [23, 10, 32, 40, 969]\n","        self.patience = 7\n","        self.max_epochs = 10\n","        self.N_fold = 5\n","\n","\n","my_args = custom_args()\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","tab_model = TAB.load_from_checkpoint('/kaggle/input/my-own-js/tabm_epochepoch03.ckpt').to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007231,"end_time":"2025-01-12T12:28:15.285330","exception":false,"start_time":"2025-01-12T12:28:15.278099","status":"completed"},"tags":[]},"source":["# NN"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.560464,"end_time":"2025-01-12T12:28:15.852926","exception":false,"start_time":"2025-01-12T12:28:15.292462","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["N_folds = 5\n","# 加载最佳模型\n","nn_models = []\n","for fold in range(N_folds):\n","    checkpoint_path = f\"{CONFIG.model_paths[0]}/nn_{fold}.model\"\n","    nn_model = NN.load_from_checkpoint(checkpoint_path)\n","    nn_models.append(nn_model.to(\"cuda:0\"))\n","nn_models[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.654128,"end_time":"2025-01-12T12:28:16.514466","exception":false,"start_time":"2025-01-12T12:28:15.860338","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["history = pl.scan_parquet(\n","    \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\n",").select(['date_id','time_id','symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).filter(\n","    (pl.col(\"date_id\")>=(1698 - CONFIG.lag_ndays))&(pl.col(\"date_id\")<1698)\n",")\n","\n","# 这里将历史date_id变为从-N到-1, 假设test的date_id=0紧随train的date_id=1698,\n","# 在第一个batch给出的lags应该是date_id=1698的responser(但date_id给的0),\n","# 这样history中最后一个date_id=1697变为-1, 正好可以和推理时给的lags衔接上\n","history = history.with_columns(\n","    date_id = (pl.col(\"date_id\") - pl.lit(1698)).cast(pl.Int16)\n",")\n","history = history.collect()\n","\n","# 这里是为了统一特征的dtypes(polars在concat时如果dtype对不上会报错)\n","history_column_types = {\n","    'date_id': pl.Int16,\n","    'time_id': pl.Int16,\n","    'symbol_id': pl.Int16\n","}\n","feature_column_types = {}\n","for f in [f\"feature_{idx:02d}\" for idx in range(79)]:\n","    feature_column_types[f] = pl.Float32\n","\n","responder_column_types = {}\n","for f in [f\"responder_{idx}\" for idx in range(9)]:\n","    responder_column_types[f] = pl.Float32\n","\n","history = history.cast(history_column_types)\n","history = history.cast(responder_column_types)\n","history.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.018903,"end_time":"2025-01-12T12:28:16.541833","exception":false,"start_time":"2025-01-12T12:28:16.522930","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["with open(\"/kaggle/input/jsridgev01011635/Ridge.dill\", \"rb\") as file_handle:\n","    rdg = dill.load(file_handle)\n","\n","def predict_ridge(test, lags):\n","    cols = [f'feature_{i:02}' for i in range(79)]\n","    predictions = test.select(\n","        'row_id',\n","        pl.lit(0.0).alias('responder_6'),\n","    )\n","    test_preds = rdg.predict(test[cols].to_pandas().fillna(3).values)\n","    return test_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.138458,"end_time":"2025-01-12T12:28:16.687541","exception":false,"start_time":"2025-01-12T12:28:16.549083","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["xgb_feature_cols = [\"symbol_id\", \"time_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","\n","xgb_model = None\n","model_path = \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result0.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model = result[\"model\"] # モデルオブジェクトを指定\n","    xgb_model.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )\n","\n","xgb_model1 = None\n","model_path = \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result1.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model1 = result[\"model\"]\n","    xgb_model1.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )\n","\n","xgb_model2 = None\n","model_path = \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result2.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model2 = result[\"model\"]\n","    xgb_model2.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )\n","    \n","xgb_model3 = None\n","model_path = \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result3.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model3 = result[\"model\"]\n","    xgb_model3.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )\n","    \n","xgb_model4 = None\n","model_path = \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result4.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model4 = result[\"model\"]\n","    xgb_model4.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )\n","    \n","xgb_model5 = None\n","model_path = \"/kaggle/input/js-with-lags-trained-xgb/result.pkl\"\n","with open( model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    xgb_model5 = result[\"model\"]\n","    xgb_model5.set_params(\n","        early_stopping_rounds=50,\n","        gamma=0.4,\n","        tree_method=\"hist\",\n","        max_depth=5,\n","        eval_metric='rmse',\n","        learning_rate=0.05\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.019945,"end_time":"2025-01-12T12:28:16.715006","exception":false,"start_time":"2025-01-12T12:28:16.695061","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Show model\n","display(xgb_model5)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.012956,"end_time":"2025-01-12T12:28:16.735248","exception":false,"start_time":"2025-01-12T12:28:16.722292","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class GaussianNoise(nn.Module):\n","    def __init__(self, std=0.1):\n","        super().__init__()\n","        self.std = std\n","\n","    def forward(self, x):\n","        if self.training:  # Only add noise during training\n","            noise = torch.randn_like(x) * self.std\n","            return x + noise\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.014563,"end_time":"2025-01-12T12:28:16.756983","exception":false,"start_time":"2025-01-12T12:28:16.742420","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_dim, output_size, num_layers):\n","        super(LSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.noise = GaussianNoise(std = .1)\n","        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x = self.noise(x)\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LSTM_RES(nn.Module):\n","    def __init__(self,\n","                 num_features: int,\n","                 hidden_size: int = 128,\n","                 num_layers: int = 1,\n","                 output_size: int = 1,\n","                 dropout_rate: float = 0.5):\n","        super(LSTM_RES, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.expand = nn.Sequential(\n","            nn.LayerNorm(num_features),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(num_features, hidden_size),\n","            nn.ReLU()\n","        )\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(in_channels=self.hidden_size // 16, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","\n","        # Residual connection for convolutional layers\n","        self.residual_conv = nn.Sequential(\n","            nn.Conv1d(in_channels=self.hidden_size // 16, out_channels=128, kernel_size=1),\n","            nn.MaxPool1d(kernel_size=4)\n","        )\n","\n","        # LSTM layer\n","        self.lstm = nn.LSTM(\n","            input_size=128,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout_rate if num_layers > 1 else 0\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Sequential(\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_size, output_size),\n","            nn.Tanh()\n","        )\n","    \n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for name, param in self.lstm.named_parameters():\n","            if 'weight_ih' in name:\n","                nn.init.xavier_uniform_(param.data)\n","            elif 'weight_hh' in name:\n","                nn.init.orthogonal_(param.data)\n","            elif 'bias' in name:\n","                param.data.fill_(0)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, num_features)\n","        x = self.expand(x)\n","        batch_size = x.size(0)\n","        seq_length = x.size(1) // (self.hidden_size // 16)\n","        x = x.view(batch_size, self.hidden_size // 16, seq_length)\n","\n","        # Save input for residual connection\n","        residual = x.clone()\n","\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        \n","        # Apply residual connection\n","        residual = self.residual_conv(residual)\n","        x += residual\n","\n","        # Prepare for LSTM\n","        x = x.permute(0, 2, 1)  # (batch_size, seq_length, features)\n","\n","        # Initialize hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","        self.lstm.flatten_parameters()\n","\n","        # LSTM layer\n","        out, _ = self.lstm(x, (h0, c0))\n","\n","        # Output layer\n","        out = self.fc(out[:, -1, :])\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.193067,"end_time":"2025-01-12T12:28:16.957634","exception":false,"start_time":"2025-01-12T12:28:16.764567","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["lstm_model_org = LSTM(input_size=79, hidden_dim=512, output_size = 1, num_layers = 1).to(device)\n","lstm_model_org.load_state_dict(torch.load('/kaggle/input/jsmodel-chan-lstm/torchlstm.pth', map_location=device,weights_only= True))\n","lstm_model_org.eval()\n","sel_cols  = [f\"feature_{i:02d}\" for i in range(79)]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lstm_model_new = LSTM_RES(num_features=79, hidden_size=512, output_size = 1, num_layers = 1).to(device)\n","lstm_model_new.load_state_dict(torch.load('/kaggle/input/js-cnn-lstm-adamw-res/cnn-lstm-adamw-res.pth', map_location=device,weights_only= True))\n","lstm_model_new.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.021293,"end_time":"2025-01-12T12:28:16.986751","exception":false,"start_time":"2025-01-12T12:28:16.965458","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["means_ = {'feature_00': 0.640198826789856, 'feature_01': 0.03755598142743111, 'feature_02': 0.6368075609207153, 'feature_03': 0.6365063786506653, 'feature_04': 0.013741530478000641, 'feature_05': -0.02173694409430027, 'feature_06': -0.006415014620870352, 'feature_07': -0.010971736162900925, 'feature_08': -0.04653771221637726, 'feature_09': 32.596106194690265, 'feature_10': 4.95929203539823, 'feature_11': 167.6541592920354, 'feature_12': -0.13415881991386414, 'feature_13': -0.07573335617780685, 'feature_14': -0.12015637010335922, 'feature_15': -0.7470195889472961, 'feature_16': -0.6257441639900208, 'feature_17': -0.7294047474861145, 'feature_18': -0.042215555906295776, 'feature_19': -0.08798160403966904, 'feature_20': -0.15741558372974396, 'feature_21': 0.10528526455163956, 'feature_22': 0.018054703250527382, 'feature_23': 0.03165541961789131, 'feature_24': 2.733017921447754, 'feature_25': 0.39958420395851135, 'feature_26': -0.11045943945646286, 'feature_27': -0.5332594513893127, 'feature_28': -0.4522790312767029, 'feature_29': -0.5739678144454956, 'feature_30': -0.7905704975128174, 'feature_31': 0.10600688308477402, 'feature_32': 0.40044134855270386, 'feature_33': -0.021725023165345192, 'feature_34': 0.4226262867450714, 'feature_35': 0.42143046855926514, 'feature_36': -0.00023802756913937628, 'feature_37': 0.027961043640971184, 'feature_38': 0.010258913040161133, 'feature_39': 0.005768273025751114, 'feature_40': 0.017485467717051506, 'feature_41': 0.038347117602825165, 'feature_42': -0.06123563274741173, 'feature_43': -0.11644423753023148, 'feature_44': -0.12342483550310135, 'feature_45': -0.028769943863153458, 'feature_46': -0.015200662426650524, 'feature_47': 0.015717582777142525, 'feature_48': -0.0033910537604242563, 'feature_49': -0.0052393232472240925, 'feature_50': -0.2285808026790619, 'feature_51': -0.3548349440097809, 'feature_52': -0.358092725276947, 'feature_53': 0.2607136368751526, 'feature_54': 0.18796788156032562, 'feature_55': 0.3154229521751404, 'feature_56': -0.1471923440694809, 'feature_57': 0.15730056166648865, 'feature_58': -0.021774644032120705, 'feature_59': -0.0037768862675875425, 'feature_60': -0.010220836848020554, 'feature_61': -0.03178725391626358, 'feature_62': -0.3769100308418274, 'feature_63': -0.3229374587535858, 'feature_64': -0.3718394339084625, 'feature_65': -0.10233989357948303, 'feature_66': -0.13688170909881592, 'feature_67': -0.14402112364768982, 'feature_68': -0.06875362992286682, 'feature_69': -0.11862917989492416, 'feature_70': -0.11789549142122269, 'feature_71': -0.06013699993491173, 'feature_72': -0.10766122490167618, 'feature_73': -0.09921672940254211, 'feature_74': -0.10233042389154434, 'feature_75': -0.05991339311003685, 'feature_76': -0.06349952518939972, 'feature_77': -0.07424316555261612, 'feature_78': -0.07759837061166763}\n","stds_ = {'feature_00': 1.027751088142395, 'feature_01': 1.0967519283294678, 'feature_02': 1.0156300067901611, 'feature_03': 1.0170334577560425, 'feature_04': 1.0726385116577148, 'feature_05': 0.9639211297035217, 'feature_06': 1.0963259935379028, 'feature_07': 1.0789952278137207, 'feature_08': 0.7962697148323059, 'feature_09': 23.72976726545254, 'feature_10': 3.1867162933797224, 'feature_11': 163.44513161352285, 'feature_12': 0.6700984835624695, 'feature_13': 0.5805172920227051, 'feature_14': 0.664044201374054, 'feature_15': 0.37517768144607544, 'feature_16': 0.3393096327781677, 'feature_17': 0.3603287935256958, 'feature_18': 0.9911752939224243, 'feature_19': 1.0550744533538818, 'feature_20': 0.6643751263618469, 'feature_21': 0.38239365816116333, 'feature_22': 0.950261116027832, 'feature_23': 0.8119344711303711, 'feature_24': 1.4362775087356567, 'feature_25': 1.0947270393371582, 'feature_26': 1.077124834060669, 'feature_27': 1.0645726919174194, 'feature_28': 1.0676648616790771, 'feature_29': 0.2640742361545563, 'feature_30': 0.19689509272575378, 'feature_31': 0.3815343976020813, 'feature_32': 1.2996565103530884, 'feature_33': 0.9989405870437622, 'feature_34': 1.3409572839736938, 'feature_35': 1.3365675210952759, 'feature_36': 0.8695492148399353, 'feature_37': 0.7334080934524536, 'feature_38': 0.698810338973999, 'feature_39': 0.7965824604034424, 'feature_40': 0.518515944480896, 'feature_41': 0.6384949088096619, 'feature_42': 0.8168442249298096, 'feature_43': 0.5228385925292969, 'feature_44': 0.6521403193473816, 'feature_45': 0.8666537404060364, 'feature_46': 0.9039222002029419, 'feature_47': 3.2711963653564453, 'feature_48': 0.6570901274681091, 'feature_49': 0.7083076238632202, 'feature_50': 1.0132617950439453, 'feature_51': 0.6081287860870361, 'feature_52': 0.9250587224960327, 'feature_53': 1.0421689748764038, 'feature_54': 0.5859629511833191, 'feature_55': 0.9191848039627075, 'feature_56': 0.9549097418785095, 'feature_57': 1.0204777717590332, 'feature_58': 0.8327276110649109, 'feature_59': 0.8309783339500427, 'feature_60': 0.8389413356781006, 'feature_61': 1.192766547203064, 'feature_62': 1.388945460319519, 'feature_63': 0.09957146644592285, 'feature_64': 0.3396177291870117, 'feature_65': 1.01683509349823, 'feature_66': 1.0824761390686035, 'feature_67': 0.642227828502655, 'feature_68': 0.5312599539756775, 'feature_69': 0.6208390593528748, 'feature_70': 0.6724499464035034, 'feature_71': 0.5356909036636353, 'feature_72': 0.6534596681594849, 'feature_73': 1.0855497121810913, 'feature_74': 1.0880277156829834, 'feature_75': 1.2321789264678955, 'feature_76': 1.2345560789108276, 'feature_77': 1.0921478271484375, 'feature_78': 1.0924347639083862}\n","def normalize_dataframe(df: pl.DataFrame, means: dict, stds: dict) -> pl.DataFrame:\n","    \"\"\"\n","    Normalize a Polars DataFrame using the provided means and standard deviations.\n","\n","    Args:\n","    df (pl.DataFrame): The input DataFrame to normalize\n","    means (dict): A dictionary of column means\n","    stds (dict): A dictionary of column standard deviations\n","\n","    Returns:\n","    pl.DataFrame: The normalized DataFrame\n","    \"\"\"\n","\n","    # Create a list to store our normalization expressions\n","    normalize_exprs = []\n","\n","    for col in df.columns:\n","        if col in means and col in stds:\n","            # Ensure we don't divide by zero\n","            if stds[col] != 0:\n","                normalize_exprs.append(\n","                    ((pl.col(col) - means[col]) / stds[col]).alias(col)\n","                )\n","            else:\n","                # If std is 0, just subtract the mean\n","                normalize_exprs.append(\n","                    (pl.col(col) - means[col]).alias(col)\n","                )\n","        else:\n","            # If we don't have mean/std for this column, leave it as is\n","            normalize_exprs.append(pl.col(col))\n","\n","    # Apply the normalization to the dataframe\n","    normalized_df = df.select(normalize_exprs)\n","\n","    return normalized_df"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.048409,"end_time":"2025-01-12T12:28:17.042277","exception":false,"start_time":"2025-01-12T12:28:16.993868","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["lgbm_original = lgb.Booster(model_file=CONFIG.data_paths[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.012659,"end_time":"2025-01-12T12:28:17.062429","exception":false,"start_time":"2025-01-12T12:28:17.049770","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Params used to retrain\n","input_params = {\"num_leaves\": 31, \"feature_fraction\": 0.9, \"n_estimators\": 100, \"learning_rate\": 0.1}\n","\n","# Define Parameters\n","params = {\n","    'objective': 'regression',\n","    'metric': 'rmse',                                      # Root Mean Squared Error\n","    'boosting_type': 'gbdt',                               # Gradient Boosted Decision Trees\n","    'num_leaves': input_params['num_leaves'],\n","    'learning_rate': input_params['learning_rate'],\n","    'feature_fraction': input_params['feature_fraction'],\n","    'n_estimators': input_params['n_estimators']      \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.035184,"end_time":"2025-01-12T12:28:17.105049","exception":false,"start_time":"2025-01-12T12:28:17.069865","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["lags_ : pl.DataFrame | None = None\n","\n","lags_history = None\n","\n","# lgb online retrain global variables start\n","\n","# Initialize global vars\n","cache = None\n","cache_list = []\n","# tot nb of days counter\n","day_count = 0\n","# training counter to be reset after each train\n","train_counter = 0\n","lgbm_retrained = None\n","\n","labels : pl.DataFrame | None = None\n","# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\n","\n","# end\n","\n","def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n","    \"\"\"Make a prediction.\"\"\"\n","    \n","    global cache          # Declare the global cache\n","    global day_count\n","    global lgbm_retrained\n","    global lags_\n","    global history\n","    global lags_infer\n","    global lags_history\n","    global cache_list\n","    global labels\n","    global train_counter\n","    # global gt\n","\n","    if lags is not None:\n","        lags_ = lags\n","\n","        # lgb online retrain global variable update start\n","        day_count += 1\n","        train_counter += 1\n","        # store ground truth from previous day\n","        update_labels = lags_[\"date_id\", \"symbol_id\", \"time_id\",\"responder_6_lag_1\"]\n","        lag_cols_rename = {\"responder_6_lag_1\": \"responder_6\"}\n","        update_labels = update_labels.rename(lag_cols_rename)\n","        if labels is not None:\n","            labels = pl.concat([labels, update_labels], rechunk=True)\n","        else:\n","            labels = update_labels\n","        # end\n","    \n","    # lstm start\n","    test_lstm = test.clone()\n","    missing_cols = set(sel_cols) - set(test_lstm.columns)\n","    if missing_cols:\n","        raise ValueError(f\"Missing columns in test data: {missing_cols}\")\n","        \n","    # Select the features\n","    test_features_lstm = test_lstm.select(sel_cols)\n","    # **Apply forward fill and then fill remaining missing values with zero**\n","    test_features_lstm = test_features_lstm.fill_null(strategy='forward').fill_null(0)\n","    test_features_lstm = normalize_dataframe(test_features_lstm, means_,stds_)\n","    # Convert Polars DataFrame to NumPy array\n","    X_test_lstm = test_features_lstm.to_numpy()\n","    # Convert to Torch tensor\n","    X_test_tensor_lstm = torch.tensor(X_test_lstm, dtype=torch.float32).to(device)\n","    \n","    # Make predictions\n","    with torch.no_grad():        \n","        outputs_lstm_org = lstm_model_org(X_test_tensor_lstm)\n","        outputs_lstm_new = lstm_model_new(X_test_tensor_lstm)\n","        # Assuming the model outputs a tensor of shape (batch_size, 1)\n","        pred_lstm_org = outputs_lstm_org.squeeze().cpu().numpy()\n","        pred_lstm_new = outputs_lstm_new.squeeze().cpu().numpy()\n","    # lstm end\n","\n","    # quadra xgb start\n","    lagsxgb = lags_.clone().group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n","    testxgb = test.clone().join(lagsxgb, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n","\n","    preds_qxgb = np.zeros((testxgb.shape[0],))\n","    preds_qxgb += xgb_model.predict(testxgb[xgb_feature_cols].to_pandas()) * 0.25\n","    preds_qxgb += xgb_model2.predict(testxgb[xgb_feature_cols].to_pandas()) * 0.25\n","    preds_qxgb += xgb_model4.predict(testxgb[xgb_feature_cols].to_pandas()) * 0.25\n","    preds_qxgb += xgb_model5.predict(testxgb[xgb_feature_cols].to_pandas()) * 0.25\n","    \n","    test_tab = test.clone()\n","    for col in feature_cat + ['symbol_id', 'time_id']:\n","        test_tab = encode_column(test_tab, col, category_mappings[col])\n","        \n","    # Initialize predictions with `row_id`\n","    predictions = test.select('row_id').with_columns(\n","        pl.lit(0.0).alias('responder_6')\n","    )\n","\n","    # lgb online retrain code\n","    test_lgb_re = test.clone()\n","    lags__lgb_re = lags_.clone()\n","    if not lags__lgb_re is None:\n","        lags_lgb_re = lags__lgb_re.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last() # pick up last record of previous date\n","        lags_lgb_re = lags_lgb_re.drop([\"time_id\"])\n","        test_lgb_re = test_lgb_re.join(lags_lgb_re, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n","    else:\n","        test_lgb_re = test_lgb_re.with_columns(\n","            ( pl.lit(0.0).alias(f'responder_{idx}_lag_1') for idx in range(9) )\n","        )\n","    if CONFIG.retrain:\n","        # store data for each batch\n","        cache_list.append(test_lgb_re)\n","        \n","    # initialize preds\n","    preds_lgb_re = np.zeros((test_lgb_re.shape[0],))\n","    \n","    # lightgbm model\n","    X_lgb_re = test_lgb_re[CONFIG.feature_colss].to_numpy()\n","\n","    # re-train a model on the fly every N days\n","    if CONFIG.retrain and train_counter % 4 == 0 and day_count>=60:\n","        # print(\"Start retraining\")\n","        if cache is not None:\n","            cache_update = pl.concat(cache_list, rechunk=True)\n","            cache = pl.concat([cache, cache_update], rechunk=True)\n","        else:\n","            cache = pl.concat(cache_list, rechunk=True)\n","        # filter labels\n","        # move data back to the previous day (we receive the lags at the same day but they are the ground truth of the previous day)\n","        df = labels.with_columns(\n","            (pl.col(\"date_id\") -1).alias(\"date_id\")\n","        )\n","        df = df.filter(pl.col(\"date_id\") >= np.min(cache[\"date_id\"].to_numpy()))\n","        # prepare data for training\n","        train = cache.join(df, on=[\"date_id\", \"symbol_id\", \"time_id\"],  how=\"left\")\n","        \n","        # drop columns where labels are none (normally last day)\n","        train_cleaned = train.filter(pl.col(CONFIG.target_col).is_not_nan())\n","        \n","        X_train = train_cleaned[CONFIG.feature_colss].to_numpy()\n","        y_train = train_cleaned[CONFIG.target_col].to_numpy().flatten()\n","\n","        train_data = lgb.Dataset(X_train, label=y_train)\n","        \n","        # Re-train the model\n","        lgbm_retrained = lgb.train(\n","            params,\n","            train_data,\n","            num_boost_round=40\n","        )\n","        # reset counter otherwise we will retrain for each time_id of the same day\n","        train_counter = 1\n","        # empty cache list\n","        cache_list = []\n","\n","        # store only last 50 days\n","        days = np.unique(cache[\"date_id\"].to_numpy())\n","        days = days[-40:]\n","        min_day = np.min(days)\n","        cache = cache.filter(pl.col(\"date_id\") >= min_day)\n","        \n","    # average original model with new retrained model\n","    if lgbm_retrained:\n","        # lightgbm models\n","        pred_lgbm_retrained = lgbm_retrained.predict(X_lgb_re, num_iteration=lgbm_original.best_iteration)\n","        pred_lgbm_original = lgbm_original.predict(X_lgb_re, num_iteration=lgbm_original.best_iteration)\n","        # simple average\n","        # weight more the prediction from the new model retrained on the new data\n","        pred_online_retrain = (0.6 * pred_lgbm_retrained + 0.4 * pred_lgbm_original)\n","    else:\n","        # lightgbm model\n","        pred_online_retrain = lgbm_original.predict(X_lgb_re, num_iteration=lgbm_original.best_iteration)\n","\n","    # end\n","\n","    # Prepare test_nn for NN processing\n","    test_nn = test.clone()\n","    symbol_ids = test_nn.select('symbol_id').to_numpy()[:, 0]\n","    current_date = test.select(\"date_id\").to_numpy()[:, 0][0]\n","    time_id = test.select(\"time_id\").to_numpy()[0]\n","    timie_id_array = test.select(\"time_id\").to_numpy()[:, 0]\n","\n","    # for tabm\n","    if time_id == 0:\n","        lagsss = lags.with_columns(pl.col('time_id').cast(pl.Int64))\n","        lagsss = lagsss.with_columns(pl.col('symbol_id').cast(pl.Int64))\n","    \n","        lags_history = lagsss\n","        lagsss = lagsss.filter(pl.col(\"time_id\") == 0)\n","        \n","        \n","        test_tab = test_tab.join(lagsss, on=[\"time_id\", \"symbol_id\"],  how=\"left\")\n","    else:\n","        lagsss = lags_history.filter(pl.col(\"time_id\") == time_id)\n","        test_tab = test_tab.join(lagsss, on=[\"time_id\", \"symbol_id\"],  how=\"left\")\n","        \n","    test_tab = test_tab.with_columns([\n","        pl.col(col).fill_null(0) for col in feature_list + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n","    ])\n","\n","    test_tab = standardize(test_tab, std_feature, means, stds)\n","\n","\n","    X_test_ = test_tab[feature_test].to_numpy()\n","    X_test_tensor = torch.tensor(X_test_, dtype=torch.float32).to(device)\n","\n","    symbol_tensor = torch.tensor(symbol_ids, dtype=torch.float32).to(device)\n","    time_tensor = torch.tensor(timie_id_array, dtype=torch.float32).to(device)\n","    X_cat = X_test_tensor[:, [9, 10, 11]]\n","    X_cont = X_test_tensor[:, [i for i in range(X_test_tensor.shape[1]) if i not in [9, 10, 11]]]\n","    # X_cont = X_cont + torch.randn_like(X_cont) * 0.02\n","\n","    X_cat = (torch.concat([X_cat, symbol_tensor.unsqueeze(-1), time_tensor.unsqueeze(-1)], axis=1)).to(torch.int64)\n","    \n","\n","    tab_model.eval()\n","    with torch.no_grad():\n","        \n","        outputs = tab_model(X_cont, X_cat)\n","        # Assuming the model outputs a tensor of shape (batch_size, 1)\n","        preds_tab = outputs.squeeze(-1).cpu().numpy()\n","        preds_tab = preds_tab.mean(1)\n","    # end\n","\n","    # rdg start\n","    lagsrdg = lags_.clone().group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last() # pick up \n","    testrdg = test.clone().join(lagsrdg, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n","    predsrdg = np.zeros((testrdg.shape[0],))\n","\n","    predsrdg = predict_ridge(testrdg,lagsrdg)\n","    # rdg end\n","    \n","    if lags is not None:\n","        lagss = lags.rename(CONFIG.lag_cols_rename)\n","        lagss = lagss.cast(history_column_types)\n","        lagss = lagss.cast(responder_column_types)\n","\n","        history = pl.concat([history, lagss])\n","        \n","        # 只储存最近N天的历史数据\n","        history = history.filter(pl.col(\"date_id\") > (current_date - CONFIG.lag_ndays))\n","\n","        # 这里用的XGB模型只使用了shift 1天的统计值\n","        agg_list = create_agg_list(1, CONFIG.lag_target_cols_name)\n","        shift_n_data = history.filter(pl.col(\"date_id\") == current_date)\n","        lags_infer = shift_n_data.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).agg(agg_list)\n","\n","    test_ = test.cast(history_column_types)\n","    test_ = test_.cast(feature_column_types)\n","    # 在一个date_id下的所有batch用到的lags_infer是相同的\n","    # 像lags_infer这样的统计特征在每个date_id的time_id=0时构造完成\n","    X_test = test_.join(lags_infer, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n","    \n","    preds_xgb = np.zeros((X_test.shape[0],))\n","    preds_xgb += model.predict(X_test[features].to_pandas().values)\n","    \n","    if lags is not None:\n","        lags = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n","        test_nn = test_nn.join(lags, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n","    else:\n","        test_nn = test_nn.with_columns(\n","            (pl.lit(0.0).alias(f'responder_{idx}_lag_1') for idx in range(9))\n","        )\n","\n","    # CatBoost predictions\n","    feat_cat = test[FEAT_COLS_CAT + ['symbol_id', 'weight', 'time_id']].to_pandas()\n","    feat_cat = feat_cat.fillna('NaN').astype(str)\n","    pred_cat = [model.predict(feat_cat) for model in catboost_models]\n","    pred_cat = np.mean(pred_cat, axis=0)\n","    \n","    pred_cat2 = catboost_holdout_model.predict(feat_cat)\n","\n","\n","    # LightGBM predictions\n","    feat_lgb = test_nn[FEAT_COLS_LGB + ['weight', 'symbol_id', 'time_id']].to_pandas()\n","    # feat_lgb['pred_cat'] = pred_cat\n","    pred_lgb = [model.predict(feat_lgb) for model in lgb_models]\n","    pred_lgb = np.mean(pred_lgb, axis=0)\n","    \n","    # Neural network predictions\n","    preds_nn = np.zeros((test_nn.shape[0],))\n","    test_input = test_nn[CONFIG.feature_cols].to_pandas()\n","    test_input = test_input.fillna(method='ffill').fillna(0)\n","    test_input = torch.FloatTensor(test_input.values).to(\"cuda:0\")\n","    with torch.no_grad():\n","        for i, nn_model in enumerate(tqdm(nn_models)):\n","            nn_model.eval()\n","            preds_nn += nn_model(test_input).cpu().numpy() / len(nn_models)\n","    print(f\"predict> nn_preds.shape =\", preds_nn.shape)\n","\n","    # Final prediction\n","    pred = pred_cat * 0.075 + \\\n","            pred_lgb * 0.1 + \\\n","            preds_nn * 0.3 + \\ \n","            preds_tab * 0.2 + \\\n","            pred_cat2 * 0.075 + \\\n","            preds_xgb * 0.1 + \\\n","            predsrdg * 0.1 + \\\n","            preds_qxgb * 0.2 + \\\n","            pred_lstm_org * 0.05 + \\\n","            pred_lstm_new * 0.05 + \\\n","            pred_online_retrain * 0.1\n","\n","    # Clip predictions to the range [-5, 5]\n","    predictions = test.select('row_id').with_columns(\n","        pl.Series(\n","            name='responder_6',\n","            values=np.clip(pred, a_min=-5, a_max=5),\n","            dtype=pl.Float64\n","        )\n","    )\n","\n","    print(predictions)\n","    \n","    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n","    assert list(predictions.columns) == ['row_id', 'responder_6']\n","    assert len(predictions) == len(test)\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.463193,"end_time":"2025-01-12T12:28:18.575896","exception":false,"start_time":"2025-01-12T12:28:17.112703","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n","\n","if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n","    inference_server.serve()\n","else:\n","    inference_server.run_local_gateway(\n","        (\n","            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n","            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.007576,"end_time":"2025-01-12T12:28:18.591621","exception":false,"start_time":"2025-01-12T12:28:18.584045","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# import time"]},{"cell_type":"markdown","metadata":{},"source":["# 1. make valid data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# valid_from = 1577 # for private you should change to 1455 (1 year)\n","# alltraindata = pl.scan_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\")\n","# valid_df = alltraindata.filter(pl.col(\"date_id\")>=valid_from).collect()\n","# valid_df = valid_df.with_columns(pl.Series(range(len(valid_df))).alias(\"row_id\"),\n","#                                 pl.lit(True).alias(\"is_scored\"))\n","# valid_df.write_parquet(\"valid_df.parquet\")\n","# test_sample = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\")\n","# valid_df = valid_df.select(test_sample.columns)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. make lag function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lag_sample = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\")\n","# train_sample = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\",n_rows=1)\n","# responder_cols = [s for s in train_sample.columns if \"responder\" in s]\n","\n","# def makelag(date_id):\n","#     \"\"\"\n","#     Making lag at the previout day\n","\n","#     Args:\n","#     date_id (int): date_id at the previout day\n","    \n","#     Returns:\n","#     pl.dataframe\n","#     \"\"\"\n","    \n","#     lag = alltraindata.filter(pl.col(\"date_id\")==date_id).select([\"date_id\",\"time_id\",\"symbol_id\"] + responder_cols).collect()\n","#     lag.columns = lag_sample.columns\n","    \n","#     return lag"]},{"cell_type":"markdown","metadata":{},"source":["# 3. make the test and lag data for debug"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# os.makedirs(\"./debug/test.parquet\",exist_ok=True)\n","# os.makedirs(\"./debug/lags.parquet\",exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# total_iterations = len(valid_df[\"date_id\"].unique())\n","# total_iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for num_days, df_per_day in tqdm(valid_df.group_by(\"date_id\",maintain_order=True),total=total_iterations,desc=\"Processing\"):\n","    \n","       \n","#     day = num_days[0] - valid_from # date_id must start from 0.\n","    \n","#     os.makedirs(f\"./debug/test.parquet/date_id={day}\",exist_ok=True)\n","#     os.makedirs(f\"./debug/lags.parquet/date_id={day}\",exist_ok=True)\n","    \n","#     lag = makelag(num_days[0] - 1)\n","    \n","#     df_per_day.write_parquet(f\"./debug/test.parquet/date_id={day}/part-0.parquet\")\n","#     lag.write_parquet(f\"./debug/lags.parquet/date_id={day}/part-0.parquet\")"]},{"cell_type":"markdown","metadata":{},"source":["# 5. check submission using the evalution API"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# %%time\n","\n","# inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n","\n","# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n","#     inference_server.serve()\n","# else:\n","#     inference_server.run_local_gateway(\n","#         (\n","#             './debug/test.parquet',\n","#             './debug/lags.parquet',\n","#         )\n","#     )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# all_submission_dataframe = []\n","# all_inference_times = []\n","# timeout = 60\n","# total_iterations = len(valid_df[\"date_id\"].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# ## Step 1 The data is split by day using group_by.\n","\n","# for num_days, df_per_day in tqdm(valid_df.group_by(\"date_id\",maintain_order=True),total=total_iterations,desc=\"Processing\"):\n","    \n","#     ## Step 2 The data is split by time_id using group_by, and the lag is generated (for time_id == 0).\n","    \n","#     for time_id, test in df_per_day.group_by(\"time_id\",maintain_order=True):\n","        \n","#         ## when time_id == 0, makelags\n","        \n","#         start_time = time.time()\n","        \n","#         if time_id[0] == 0:\n","#             lag = makelag(num_days[0] - 1)\n","#         else:\n","#             lag = None\n","        \n","#         submission_dataframe = predict(test, lag)\n","        \n","#         all_submission_dataframe.append(submission_dataframe)\n","        \n","#         end_time = time.time()\n","        \n","#         diff = end_time - start_time\n","        \n","#         all_inference_times.append(diff)\n","        \n","#      #   print(f\"{num_days[0]=},{time_id[0]=}{diff=}\")\n","        \n","#         if diff > timeout:\n","#             print(f\"{num_days[0]=},{time_id[0]=}{diff=}\")\n","#             assert elapsed_time < timeout, f\"process over {timeout/60} mins. cancelled\"\n","        \n","# all_submission_dataframe = pl.concat(all_submission_dataframe)\n","# all_submission_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# def weighted_zero_mean_r2(y_true, y_pred, weights):\n","#     \"\"\"\n","#     Calculate the sample weighted zero-mean R-squared score.\n","\n","#     Parameters:\n","#     y_true (numpy.ndarray): Ground-truth values for responder_6.\n","#     y_pred (numpy.ndarray): Predicted values for responder_6.\n","#     weights (numpy.ndarray): Sample weight vector.\n","\n","#     Returns:\n","#     float: The weighted zero-mean R-squared score.\n","#     \"\"\"\n","#     numerator = np.sum(weights * (y_true - y_pred)**2)\n","#     denominator = np.sum(weights * y_true**2)\n","    \n","#     r2_score = 1 - numerator / denominator\n","#     return r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# valid_df = pl.read_parquet(\"valid_df.parquet\")\n","# y_true = valid_df.select(\"responder_6\").to_numpy().reshape(-1)\n","# y_pred = all_submission_dataframe.select(\"responder_6\").to_numpy().reshape(-1)\n","# weights = valid_df.select(\"weight\").to_numpy().reshape(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-01-12T15:58:51.275Z"},"trusted":true},"outputs":[],"source":["# weighted_zero_mean_r2(y_true, y_pred, weights)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9871156,"sourceId":84493,"sourceType":"competition"},{"datasetId":6006872,"sourceId":9801075,"sourceType":"datasetVersion"},{"datasetId":6010899,"sourceId":9806342,"sourceType":"datasetVersion"},{"datasetId":6035149,"sourceId":9838282,"sourceType":"datasetVersion"},{"datasetId":6293027,"sourceId":10186479,"sourceType":"datasetVersion"},{"datasetId":6297065,"sourceId":10253875,"sourceType":"datasetVersion"},{"datasetId":6378806,"sourceId":10304887,"sourceType":"datasetVersion"},{"datasetId":6410107,"sourceId":10351700,"sourceType":"datasetVersion"},{"datasetId":6433793,"sourceId":10386263,"sourceType":"datasetVersion"},{"datasetId":6434411,"sourceId":10386352,"sourceType":"datasetVersion"},{"datasetId":6467169,"sourceId":10448107,"sourceType":"datasetVersion"},{"datasetId":6469806,"sourceId":10451770,"sourceType":"datasetVersion"},{"sourceId":211920287,"sourceType":"kernelVersion"},{"sourceId":214286693,"sourceType":"kernelVersion"},{"sourceId":215616115,"sourceType":"kernelVersion"},{"sourceId":216017958,"sourceType":"kernelVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":62.307192,"end_time":"2025-01-12T12:28:21.331643","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-12T12:27:19.024451","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"23e431e12dd04a579f40530ce389b04e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8152cc94a667417a978e9fab8e9b27fa","IPY_MODEL_2935bcf1bd734054a911a305bae485b4","IPY_MODEL_c5ae412c1f6e4b75b01a37869ecff7e0"],"layout":"IPY_MODEL_bb0388f2301f42fe8cbe2513e6750291","tabbable":null,"tooltip":null}},"2935bcf1bd734054a911a305bae485b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_e0f61210260c4558a149b469c09dbca2","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db2a21d704ba4aeba274e09d3f363d67","tabbable":null,"tooltip":null,"value":5}},"4ae42f6387444c97b44736890129045d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8152cc94a667417a978e9fab8e9b27fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f0e47892826042ffa168f6111b80a169","placeholder":"​","style":"IPY_MODEL_e5ce8447abf0480d825c715521ed18fa","tabbable":null,"tooltip":null,"value":"100%"}},"bb0388f2301f42fe8cbe2513e6750291":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5ae412c1f6e4b75b01a37869ecff7e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4ae42f6387444c97b44736890129045d","placeholder":"​","style":"IPY_MODEL_d88fd771c0f649d3abaaf496eb1ed71d","tabbable":null,"tooltip":null,"value":" 5/5 [00:00&lt;00:00,  6.22it/s]"}},"d88fd771c0f649d3abaaf496eb1ed71d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"db2a21d704ba4aeba274e09d3f363d67":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0f61210260c4558a149b469c09dbca2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5ce8447abf0480d825c715521ed18fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f0e47892826042ffa168f6111b80a169":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
