{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/kaggle/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "#import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "# 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = 'responder_6'\n",
    "FEAT_COLS_CAT = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "FEAT_COLS_LGB = [f\"feature_{i:02d}\" for i in range(79)]+ ['responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1',\n",
    "       'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1',\n",
    "       'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true, y_pred, weights):\n",
    "    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "    denominator = np.sum(weights * (y_true ** 2))\n",
    "    r2_score = 1 - (numerator / denominator)\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def load_data(date_id_range=None, time_id_range=None, columns=None, return_type='pl'):\n",
    "    data = pl.scan_parquet(f'/kaggle/input/js24-preprocessing-create-lags/training.parquet').collect() # 내꺼로 바꾸기 lag 없는거\n",
    "    \n",
    "    if date_id_range is not None:\n",
    "        start_date, end_date = date_id_range\n",
    "        data = data.filter((pl.col(\"date_id\") >= start_date) & (pl.col(\"date_id\") <= end_date))\n",
    "    \n",
    "    if time_id_range is not None:\n",
    "        start_time, end_time = time_id_range\n",
    "        data = data.filter((pl.col(\"time_id\") >= start_time) & (pl.col(\"time_id\") <= end_time))\n",
    "    \n",
    "    if columns is not None:\n",
    "        data = data.select(columns)\n",
    "\n",
    "    if return_type == 'pd':\n",
    "        return data.to_pandas()\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_kfold_single(total_days=1699, n_splits=5, save_model=True, save_path='modellgb/'):\n",
    "    if save_model and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    max_valid_days = 1200\n",
    "    valid_days = min(total_days, max_valid_days)\n",
    "    valid_start = 1699 - valid_days\n",
    "    \n",
    "    fold_size = valid_days // n_splits\n",
    "    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n",
    "    \n",
    "    lgb_models = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        valid_range = folds[fold_idx]\n",
    "        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n",
    "        print(f'Fold {fold_idx}: Training LGB')\n",
    "        valid_data = load_data(\n",
    "            date_id_range=valid_range,  # noqa: F821\n",
    "            columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "            return_type='pl')\n",
    "        \n",
    "        # 학습 데이터 로드\n",
    "        train_data = None\n",
    "        for train_range in train_ranges:\n",
    "            partial_train_data = load_data(\n",
    "                date_id_range=train_range,  # noqa: F821\n",
    "                columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "                return_type='pl')\n",
    "\n",
    "            if train_data is None:\n",
    "                train_data = partial_train_data\n",
    "            else:\n",
    "                train_data = train_data.vstack(partial_train_data)\n",
    "                \n",
    "        print(f\"Train Data (before making ds) shape: {train_data.shape}\")\n",
    "        print(f\"Valid Data (before making ds) shape: {valid_data.shape}\")\n",
    "        \n",
    "        # LightGBM 데이터셋 생성\n",
    "        train_ds = lgb.Dataset(train_data[FEAT_COLS_LGB+['weight']].to_pandas(),\n",
    "                             label=train_data[TARGET].to_pandas(),\n",
    "                             weight=train_data['weight'].to_pandas())\n",
    "        valid_ds = lgb.Dataset(valid_data[FEAT_COLS_LGB+['weight']].to_pandas(),\n",
    "                             label=valid_data[TARGET].to_pandas(),\n",
    "                             weight=valid_data['weight'].to_pandas(),\n",
    "                             reference=train_ds)\n",
    "        \n",
    "        # LightGBM 파라미터\n",
    "        LGB_PARAMS = {\n",
    "            'objective': 'regression_l2',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'random_state': 42,\n",
    "            'device': 'gpu',\n",
    "        }\n",
    "        \n",
    "        # 콜백 함수\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(100),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "        \n",
    "        # 모델 학습\n",
    "        model = lgb.train(\n",
    "            LGB_PARAMS,\n",
    "            train_ds,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[train_ds, valid_ds],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        lgb_models.append(model)\n",
    "        \n",
    "        # R2 점수 계산\n",
    "        y_valid_pred = model.predict(valid_data[FEAT_COLS_LGB+['weight']].to_pandas())\n",
    "        r2_score = calculate_r2(valid_data[TARGET].to_pandas(), y_valid_pred, valid_data['weight'].to_pandas())\n",
    "        print(f\"LGB Fold {fold_idx} validation R2 score: {r2_score}\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    if save_model:\n",
    "        joblib.dump(lgb_models, os.path.join(save_path, \"lgb_models.pkl\"))\n",
    "        print(\"Saved all models to lgb_models.pkl\")\n",
    "    \n",
    "    return lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Training LGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data (before making ds) shape: (14254768, 92)\n",
      "Valid Data (before making ds) shape: (3713248, 92)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 21987\n",
      "[LightGBM] [Info] Number of data points in the train set: 14254768, number of used features: 89\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 89 dense feature groups (1250.69 MB) transferred to GPU in 1.043705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.002701\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 0.810927\tvalid's rmse: 0.771196\n",
      "[100]\ttrain's rmse: 0.80772\tvalid's rmse: 0.770536\n",
      "[150]\ttrain's rmse: 0.804678\tvalid's rmse: 0.770276\n",
      "[200]\ttrain's rmse: 0.802237\tvalid's rmse: 0.770103\n",
      "[250]\ttrain's rmse: 0.800376\tvalid's rmse: 0.769996\n",
      "[300]\ttrain's rmse: 0.798676\tvalid's rmse: 0.769934\n",
      "[350]\ttrain's rmse: 0.797323\tvalid's rmse: 0.769904\n",
      "[400]\ttrain's rmse: 0.796026\tvalid's rmse: 0.769868\n",
      "[450]\ttrain's rmse: 0.7947\tvalid's rmse: 0.769851\n",
      "[500]\ttrain's rmse: 0.793477\tvalid's rmse: 0.769853\n",
      "Early stopping, best iteration is:\n",
      "[442]\ttrain's rmse: 0.794971\tvalid's rmse: 0.769827\n",
      "LGB Fold 0 validation R2 score: 0.01194556415139314\n",
      "Fold 1: Training LGB\n",
      "Train Data (before making ds) shape: (14289616, 92)\n",
      "Valid Data (before making ds) shape: (3678400, 92)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 21988\n",
      "[LightGBM] [Info] Number of data points in the train set: 14289616, number of used features: 89\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 89 dense feature groups (1253.74 MB) transferred to GPU in 1.036947 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.002106\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 0.78039\tvalid's rmse: 0.901717\n",
      "[100]\ttrain's rmse: 0.776275\tvalid's rmse: 0.901148\n",
      "[150]\ttrain's rmse: 0.773407\tvalid's rmse: 0.901082\n",
      "[200]\ttrain's rmse: 0.770884\tvalid's rmse: 0.900899\n",
      "[250]\ttrain's rmse: 0.768882\tvalid's rmse: 0.900877\n",
      "[300]\ttrain's rmse: 0.767127\tvalid's rmse: 0.900788\n",
      "[350]\ttrain's rmse: 0.765547\tvalid's rmse: 0.900819\n",
      "[400]\ttrain's rmse: 0.764234\tvalid's rmse: 0.900843\n",
      "[450]\ttrain's rmse: 0.762968\tvalid's rmse: 0.900891\n",
      "Early stopping, best iteration is:\n",
      "[371]\ttrain's rmse: 0.765053\tvalid's rmse: 0.900762\n",
      "LGB Fold 1 validation R2 score: 0.012351194177027236\n",
      "Fold 2: Training LGB\n",
      "Train Data (before making ds) shape: (14374800, 92)\n",
      "Valid Data (before making ds) shape: (3593216, 92)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 21989\n",
      "[LightGBM] [Info] Number of data points in the train set: 14374800, number of used features: 89\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 89 dense feature groups (1261.22 MB) transferred to GPU in 1.069514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.002457\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 0.803808\tvalid's rmse: 0.806015\n",
      "[100]\ttrain's rmse: 0.799628\tvalid's rmse: 0.805516\n",
      "[150]\ttrain's rmse: 0.796789\tvalid's rmse: 0.805222\n",
      "[200]\ttrain's rmse: 0.794542\tvalid's rmse: 0.805225\n",
      "[250]\ttrain's rmse: 0.792673\tvalid's rmse: 0.805109\n",
      "[300]\ttrain's rmse: 0.791066\tvalid's rmse: 0.805136\n",
      "[350]\ttrain's rmse: 0.789546\tvalid's rmse: 0.80525\n",
      "Early stopping, best iteration is:\n",
      "[291]\ttrain's rmse: 0.791419\tvalid's rmse: 0.805043\n",
      "LGB Fold 2 validation R2 score: 0.016064068760600514\n",
      "Fold 3: Training LGB\n",
      "Train Data (before making ds) shape: (14248960, 92)\n",
      "Valid Data (before making ds) shape: (3719056, 92)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 21989\n",
      "[LightGBM] [Info] Number of data points in the train set: 14248960, number of used features: 89\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 89 dense feature groups (1250.18 MB) transferred to GPU in 1.098362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.000953\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 0.818121\tvalid's rmse: 0.750665\n",
      "[100]\ttrain's rmse: 0.814733\tvalid's rmse: 0.75001\n",
      "[150]\ttrain's rmse: 0.81221\tvalid's rmse: 0.749777\n",
      "[200]\ttrain's rmse: 0.809986\tvalid's rmse: 0.749545\n",
      "[250]\ttrain's rmse: 0.808147\tvalid's rmse: 0.749415\n",
      "[300]\ttrain's rmse: 0.806529\tvalid's rmse: 0.749383\n",
      "[350]\ttrain's rmse: 0.804949\tvalid's rmse: 0.749373\n",
      "[400]\ttrain's rmse: 0.803568\tvalid's rmse: 0.749339\n",
      "[450]\ttrain's rmse: 0.802325\tvalid's rmse: 0.749282\n",
      "[500]\ttrain's rmse: 0.801119\tvalid's rmse: 0.749301\n",
      "[550]\ttrain's rmse: 0.79999\tvalid's rmse: 0.749308\n",
      "Early stopping, best iteration is:\n",
      "[475]\ttrain's rmse: 0.801719\tvalid's rmse: 0.749267\n",
      "LGB Fold 3 validation R2 score: 0.011677283213323841\n",
      "Fold 4: Training LGB\n",
      "Train Data (before making ds) shape: (14703920, 92)\n",
      "Valid Data (before making ds) shape: (3264096, 92)\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 21985\n",
      "[LightGBM] [Info] Number of data points in the train set: 14703920, number of used features: 89\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 89 dense feature groups (1290.09 MB) transferred to GPU in 1.106697 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.000785\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 0.804336\tvalid's rmse: 0.801307\n",
      "[100]\ttrain's rmse: 0.800011\tvalid's rmse: 0.801079\n",
      "[150]\ttrain's rmse: 0.797105\tvalid's rmse: 0.800881\n",
      "[200]\ttrain's rmse: 0.794818\tvalid's rmse: 0.800858\n",
      "[250]\ttrain's rmse: 0.79286\tvalid's rmse: 0.800835\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttrain's rmse: 0.795646\tvalid's rmse: 0.800801\n",
      "LGB Fold 4 validation R2 score: 0.006687485267608317\n",
      "Saved all models to lgb_models.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lgb_models = train_lgb_kfold_single(total_days=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
