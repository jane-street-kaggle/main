{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "class CONFIG:\n",
    "    seed = 42\n",
    "    target_col = \"responder_6\"\n",
    "    feature_cols = [\"symbol_id\", \"time_id\"] \\\n",
    "        + [f\"feature_{idx:02d}\" for idx in range(79)] \\\n",
    "        + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "    weight_col = \"weight\"\n",
    "    \n",
    "train = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/training.parquet\").collect().to_pandas()\n",
    "valid = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet\").collect().to_pandas()\n",
    "train = pd.concat([train, valid]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export POLARS_ALLOW_FORKING_THREAD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[ CONFIG.feature_cols ]\n",
    "y_train = train[ CONFIG.target_col ]\n",
    "w_train = train[ \"weight\" ]\n",
    "X_valid = valid[ CONFIG.feature_cols ]\n",
    "y_valid = valid[ CONFIG.target_col ]\n",
    "w_valid = valid[ \"weight\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.usegpu = True\n",
    "        self.gpuid = 0\n",
    "        self.seed = 42\n",
    "        self.model = 'nn'\n",
    "        self.use_wandb = False\n",
    "        self.project = 'js-xs-nn-with-lags'\n",
    "        self.dname = \"./input_df/\"\n",
    "        self.loader_workers = 0\n",
    "        self.bs = 8192\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dropouts = [0.1, 0.1]\n",
    "        self.n_hidden = [512, 512, 256]\n",
    "        self.patience = 25\n",
    "        self.max_epochs = 2000\n",
    "        self.N_fold = 5\n",
    "\n",
    "\n",
    "my_args = custom_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, accelerator):\n",
    "        self.features = torch.FloatTensor(df[CONFIG.feature_cols].values).to(accelerator)\n",
    "        self.labels = torch.FloatTensor(df[CONFIG.target_col].values).to(accelerator)\n",
    "        self.weights = torch.FloatTensor(df[CONFIG.weight_col].values).to(accelerator)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_df, batch_size, valid_df=None, accelerator='gpu'):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.batch_size = batch_size\n",
    "        self.accelerator = accelerator\n",
    "\n",
    "        self.dates = self.train_df['date_id'].unique()\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "    \n",
    "    def setup(self, fold=0, n_fold=5, stage=None):\n",
    "        selected_dates = [date for ii, date in enumerate(self.dates) if ii % n_fold != fold]\n",
    "        df_train = self.train_df.loc[self.train_df['date_id'].isin(selected_dates)]\n",
    "        self.train_dataset = CustomDataset(df_train, self.accelerator)\n",
    "        if self.valid_df is not None:\n",
    "            df_valid = self.valid_df\n",
    "            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n",
    "    \n",
    "    def train_dataloader(self, num_workers=4):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self, num_workers=4):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1)) \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            val_r_square = r2_score(y, prob, sample_weight=weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.lr, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = my_args\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "loader_device = 'cpu'\n",
    "\n",
    "train = train.ffill().fillna(0)\n",
    "valid = valid.ffill().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_df=train, \n",
    "    valid_df=valid, \n",
    "    batch_size=args.bs, \n",
    "    accelerator=loader_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2159/2159 [14:58<00:00,  2.40it/s, v_num=3]\n",
      "Epoch 0: 100%|██████████| 2159/2159 [09:48<00:00,  3.67it/s, v_num=4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/micromamba/envs/kaggle/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 443 K  | train\n",
      "---------------------------------------------\n",
      "443 K     Trainable params\n",
      "0         Non-trainable params\n",
      "443 K     Total params\n",
      "1.772     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2159/2159 [01:57<00:00, 18.38it/s, v_num=6, val_loss=1.070, val_r_square=0.00521, train_loss=1.510]Epoch 0: {'val_loss': '1.06751', 'val_r_square': '0.00521', 'train_loss': '1.50968'}\n",
      "Epoch 1: 100%|██████████| 2159/2159 [02:00<00:00, 17.90it/s, v_num=6, val_loss=1.070, val_r_square=0.00522, train_loss=1.510]Epoch 1: {'val_loss': '1.06750', 'val_r_square': '0.00522', 'train_loss': '1.50734'}\n",
      "Epoch 2: 100%|██████████| 2159/2159 [02:03<00:00, 17.53it/s, v_num=6, val_loss=1.070, val_r_square=0.0063, train_loss=1.510] Epoch 2: {'val_loss': '1.06635', 'val_r_square': '0.00630', 'train_loss': '1.50589'}\n",
      "Epoch 3: 100%|██████████| 2159/2159 [01:59<00:00, 18.08it/s, v_num=6, val_loss=1.070, val_r_square=0.00633, train_loss=1.500]Epoch 3: {'val_loss': '1.06631', 'val_r_square': '0.00633', 'train_loss': '1.50482'}\n",
      "Epoch 4: 100%|██████████| 2159/2159 [01:57<00:00, 18.30it/s, v_num=6, val_loss=1.070, val_r_square=0.00692, train_loss=1.500]Epoch 4: {'val_loss': '1.06568', 'val_r_square': '0.00692', 'train_loss': '1.50406'}\n",
      "Epoch 5: 100%|██████████| 2159/2159 [01:58<00:00, 18.23it/s, v_num=6, val_loss=1.070, val_r_square=0.0072, train_loss=1.500] Epoch 5: {'val_loss': '1.06538', 'val_r_square': '0.00720', 'train_loss': '1.50330'}\n",
      "Epoch 6: 100%|██████████| 2159/2159 [02:00<00:00, 17.84it/s, v_num=6, val_loss=1.070, val_r_square=0.00713, train_loss=1.500]Epoch 6: {'val_loss': '1.06546', 'val_r_square': '0.00713', 'train_loss': '1.50280'}\n",
      "Epoch 7: 100%|██████████| 2159/2159 [02:03<00:00, 17.54it/s, v_num=6, val_loss=1.060, val_r_square=0.00809, train_loss=1.500]Epoch 7: {'val_loss': '1.06443', 'val_r_square': '0.00809', 'train_loss': '1.50264'}\n",
      "Epoch 8: 100%|██████████| 2159/2159 [02:09<00:00, 16.66it/s, v_num=6, val_loss=1.070, val_r_square=0.00703, train_loss=1.500]Epoch 8: {'val_loss': '1.06556', 'val_r_square': '0.00703', 'train_loss': '1.50222'}\n",
      "Epoch 9: 100%|██████████| 2159/2159 [01:59<00:00, 18.07it/s, v_num=6, val_loss=1.070, val_r_square=0.00572, train_loss=1.500]Epoch 9: {'val_loss': '1.06697', 'val_r_square': '0.00572', 'train_loss': '1.50200'}\n",
      "Epoch 10: 100%|██████████| 2159/2159 [01:57<00:00, 18.30it/s, v_num=6, val_loss=1.070, val_r_square=0.00732, train_loss=1.500]Epoch 10: {'val_loss': '1.06525', 'val_r_square': '0.00732', 'train_loss': '1.50172'}\n",
      "Epoch 11: 100%|██████████| 2159/2159 [01:57<00:00, 18.37it/s, v_num=6, val_loss=1.070, val_r_square=0.00675, train_loss=1.500]Epoch 11: {'val_loss': '1.06586', 'val_r_square': '0.00675', 'train_loss': '1.50172'}\n",
      "Epoch 12: 100%|██████████| 2159/2159 [01:58<00:00, 18.16it/s, v_num=6, val_loss=1.070, val_r_square=0.00626, train_loss=1.500]Epoch 12: {'val_loss': '1.06639', 'val_r_square': '0.00626', 'train_loss': '1.50170'}\n",
      "Epoch 13: 100%|██████████| 2159/2159 [02:10<00:00, 16.49it/s, v_num=6, val_loss=1.070, val_r_square=0.00691, train_loss=1.500]Epoch 13: {'val_loss': '1.06569', 'val_r_square': '0.00691', 'train_loss': '1.50135'}\n",
      "Epoch 14: 100%|██████████| 2159/2159 [02:04<00:00, 17.38it/s, v_num=6, val_loss=1.070, val_r_square=0.00703, train_loss=1.500]Epoch 14: {'val_loss': '1.06556', 'val_r_square': '0.00703', 'train_loss': '1.50007'}\n",
      "Epoch 15: 100%|██████████| 2159/2159 [02:00<00:00, 17.89it/s, v_num=6, val_loss=1.060, val_r_square=0.00755, train_loss=1.500]Epoch 15: {'val_loss': '1.06500', 'val_r_square': '0.00755', 'train_loss': '1.49991'}\n",
      "Epoch 16: 100%|██████████| 2159/2159 [02:00<00:00, 17.96it/s, v_num=6, val_loss=1.060, val_r_square=0.00773, train_loss=1.500]Epoch 16: {'val_loss': '1.06481', 'val_r_square': '0.00773', 'train_loss': '1.49964'}\n",
      "Epoch 17: 100%|██████████| 2159/2159 [01:58<00:00, 18.29it/s, v_num=6, val_loss=1.060, val_r_square=0.00783, train_loss=1.500]Epoch 17: {'val_loss': '1.06470', 'val_r_square': '0.00783', 'train_loss': '1.49968'}\n",
      "Epoch 18: 100%|██████████| 2159/2159 [01:59<00:00, 18.02it/s, v_num=6, val_loss=1.060, val_r_square=0.00768, train_loss=1.500]Epoch 18: {'val_loss': '1.06486', 'val_r_square': '0.00768', 'train_loss': '1.49963'}\n",
      "Epoch 19: 100%|██████████| 2159/2159 [02:01<00:00, 17.75it/s, v_num=6, val_loss=1.060, val_r_square=0.00805, train_loss=1.500]Epoch 19: {'val_loss': '1.06446', 'val_r_square': '0.00805', 'train_loss': '1.49970'}\n",
      "Epoch 20: 100%|██████████| 2159/2159 [01:58<00:00, 18.21it/s, v_num=6, val_loss=1.060, val_r_square=0.00859, train_loss=1.500]Epoch 20: {'val_loss': '1.06389', 'val_r_square': '0.00859', 'train_loss': '1.49888'}\n",
      "Epoch 21: 100%|██████████| 2159/2159 [02:00<00:00, 17.95it/s, v_num=6, val_loss=1.060, val_r_square=0.00804, train_loss=1.500]Epoch 21: {'val_loss': '1.06448', 'val_r_square': '0.00804', 'train_loss': '1.49866'}\n",
      "Epoch 22: 100%|██████████| 2159/2159 [02:00<00:00, 17.92it/s, v_num=6, val_loss=1.060, val_r_square=0.00857, train_loss=1.500]Epoch 22: {'val_loss': '1.06391', 'val_r_square': '0.00857', 'train_loss': '1.49848'}\n",
      "Epoch 23: 100%|██████████| 2159/2159 [01:57<00:00, 18.30it/s, v_num=6, val_loss=1.060, val_r_square=0.00823, train_loss=1.500]Epoch 23: {'val_loss': '1.06427', 'val_r_square': '0.00823', 'train_loss': '1.49843'}\n",
      "Epoch 24: 100%|██████████| 2159/2159 [01:59<00:00, 18.08it/s, v_num=6, val_loss=1.060, val_r_square=0.00799, train_loss=1.500]Epoch 24: {'val_loss': '1.06453', 'val_r_square': '0.00799', 'train_loss': '1.49859'}\n",
      "Epoch 25: 100%|██████████| 2159/2159 [02:09<00:00, 16.67it/s, v_num=6, val_loss=1.060, val_r_square=0.00784, train_loss=1.500]Epoch 25: {'val_loss': '1.06469', 'val_r_square': '0.00784', 'train_loss': '1.49840'}\n",
      "Epoch 26: 100%|██████████| 2159/2159 [02:10<00:00, 16.57it/s, v_num=6, val_loss=1.060, val_r_square=0.00861, train_loss=1.500]Epoch 26: {'val_loss': '1.06386', 'val_r_square': '0.00861', 'train_loss': '1.49861'}\n",
      "Epoch 27: 100%|██████████| 2159/2159 [01:59<00:00, 18.03it/s, v_num=6, val_loss=1.060, val_r_square=0.00813, train_loss=1.500]Epoch 27: {'val_loss': '1.06438', 'val_r_square': '0.00813', 'train_loss': '1.49798'}\n",
      "Epoch 28: 100%|██████████| 2159/2159 [02:01<00:00, 17.79it/s, v_num=6, val_loss=1.060, val_r_square=0.00901, train_loss=1.500]Epoch 28: {'val_loss': '1.06344', 'val_r_square': '0.00901', 'train_loss': '1.49771'}\n",
      "Epoch 29: 100%|██████████| 2159/2159 [01:56<00:00, 18.50it/s, v_num=6, val_loss=1.060, val_r_square=0.0083, train_loss=1.500] Epoch 29: {'val_loss': '1.06420', 'val_r_square': '0.00830', 'train_loss': '1.49781'}\n",
      "Epoch 30: 100%|██████████| 2159/2159 [02:02<00:00, 17.57it/s, v_num=6, val_loss=1.060, val_r_square=0.00817, train_loss=1.500]Epoch 30: {'val_loss': '1.06434', 'val_r_square': '0.00817', 'train_loss': '1.49775'}\n",
      "Epoch 31: 100%|██████████| 2159/2159 [02:09<00:00, 16.70it/s, v_num=6, val_loss=1.060, val_r_square=0.00815, train_loss=1.500]Epoch 31: {'val_loss': '1.06436', 'val_r_square': '0.00815', 'train_loss': '1.49775'}\n",
      "Epoch 32: 100%|██████████| 2159/2159 [02:01<00:00, 17.74it/s, v_num=6, val_loss=1.060, val_r_square=0.00851, train_loss=1.500]Epoch 32: {'val_loss': '1.06397', 'val_r_square': '0.00851', 'train_loss': '1.49769'}\n",
      "Epoch 33: 100%|██████████| 2159/2159 [02:06<00:00, 17.00it/s, v_num=6, val_loss=1.060, val_r_square=0.00816, train_loss=1.500]Epoch 33: {'val_loss': '1.06434', 'val_r_square': '0.00816', 'train_loss': '1.49768'}\n",
      "Epoch 34: 100%|██████████| 2159/2159 [01:59<00:00, 18.13it/s, v_num=6, val_loss=1.060, val_r_square=0.0086, train_loss=1.500] Epoch 34: {'val_loss': '1.06388', 'val_r_square': '0.00860', 'train_loss': '1.49767'}\n",
      "Epoch 35: 100%|██████████| 2159/2159 [02:02<00:00, 17.63it/s, v_num=6, val_loss=1.060, val_r_square=0.00897, train_loss=1.500]Epoch 35: {'val_loss': '1.06348', 'val_r_square': '0.00897', 'train_loss': '1.49737'}\n",
      "Epoch 36: 100%|██████████| 2159/2159 [02:00<00:00, 17.97it/s, v_num=6, val_loss=1.060, val_r_square=0.00867, train_loss=1.500]Epoch 36: {'val_loss': '1.06380', 'val_r_square': '0.00867', 'train_loss': '1.49731'}\n",
      "Epoch 37: 100%|██████████| 2159/2159 [02:08<00:00, 16.85it/s, v_num=6, val_loss=1.060, val_r_square=0.00846, train_loss=1.500]Epoch 37: {'val_loss': '1.06403', 'val_r_square': '0.00846', 'train_loss': '1.49735'}\n",
      "Epoch 38: 100%|██████████| 2159/2159 [02:02<00:00, 17.68it/s, v_num=6, val_loss=1.060, val_r_square=0.0082, train_loss=1.500] Epoch 38: {'val_loss': '1.06430', 'val_r_square': '0.00820', 'train_loss': '1.49735'}\n",
      "Epoch 39: 100%|██████████| 2159/2159 [01:58<00:00, 18.26it/s, v_num=6, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 39: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.49719'}\n",
      "Epoch 40: 100%|██████████| 2159/2159 [01:59<00:00, 18.04it/s, v_num=6, val_loss=1.060, val_r_square=0.00891, train_loss=1.500]Epoch 40: {'val_loss': '1.06354', 'val_r_square': '0.00891', 'train_loss': '1.49710'}\n",
      "Epoch 41: 100%|██████████| 2159/2159 [02:01<00:00, 17.82it/s, v_num=6, val_loss=1.060, val_r_square=0.00866, train_loss=1.500]Epoch 41: {'val_loss': '1.06381', 'val_r_square': '0.00866', 'train_loss': '1.49710'}\n",
      "Epoch 42: 100%|██████████| 2159/2159 [01:59<00:00, 18.06it/s, v_num=6, val_loss=1.060, val_r_square=0.00907, train_loss=1.500]Epoch 42: {'val_loss': '1.06338', 'val_r_square': '0.00907', 'train_loss': '1.49703'}\n",
      "Epoch 43: 100%|██████████| 2159/2159 [02:04<00:00, 17.31it/s, v_num=6, val_loss=1.060, val_r_square=0.00862, train_loss=1.500]Epoch 43: {'val_loss': '1.06386', 'val_r_square': '0.00862', 'train_loss': '1.49691'}\n",
      "Epoch 44: 100%|██████████| 2159/2159 [02:02<00:00, 17.68it/s, v_num=6, val_loss=1.060, val_r_square=0.00879, train_loss=1.500]Epoch 44: {'val_loss': '1.06368', 'val_r_square': '0.00879', 'train_loss': '1.49687'}\n",
      "Epoch 45: 100%|██████████| 2159/2159 [02:09<00:00, 16.67it/s, v_num=6, val_loss=1.060, val_r_square=0.0088, train_loss=1.500] Epoch 45: {'val_loss': '1.06366', 'val_r_square': '0.00880', 'train_loss': '1.49698'}\n",
      "Epoch 46: 100%|██████████| 2159/2159 [02:02<00:00, 17.61it/s, v_num=6, val_loss=1.060, val_r_square=0.0089, train_loss=1.500]Epoch 46: {'val_loss': '1.06356', 'val_r_square': '0.00890', 'train_loss': '1.49684'}\n",
      "Epoch 47: 100%|██████████| 2159/2159 [01:56<00:00, 18.59it/s, v_num=6, val_loss=1.060, val_r_square=0.00878, train_loss=1.500]Epoch 47: {'val_loss': '1.06369', 'val_r_square': '0.00878', 'train_loss': '1.49676'}\n",
      "Epoch 48: 100%|██████████| 2159/2159 [02:01<00:00, 17.76it/s, v_num=6, val_loss=1.060, val_r_square=0.00896, train_loss=1.500]Epoch 48: {'val_loss': '1.06349', 'val_r_square': '0.00896', 'train_loss': '1.49693'}\n",
      "Epoch 49: 100%|██████████| 2159/2159 [02:10<00:00, 16.51it/s, v_num=6, val_loss=1.060, val_r_square=0.00901, train_loss=1.500]Epoch 49: {'val_loss': '1.06344', 'val_r_square': '0.00901', 'train_loss': '1.49682'}\n",
      "Epoch 50: 100%|██████████| 2159/2159 [02:11<00:00, 16.43it/s, v_num=6, val_loss=1.060, val_r_square=0.00887, train_loss=1.500]Epoch 50: {'val_loss': '1.06359', 'val_r_square': '0.00887', 'train_loss': '1.49675'}\n",
      "Epoch 51: 100%|██████████| 2159/2159 [01:58<00:00, 18.18it/s, v_num=6, val_loss=1.060, val_r_square=0.00876, train_loss=1.500]Epoch 51: {'val_loss': '1.06370', 'val_r_square': '0.00876', 'train_loss': '1.49686'}\n",
      "Epoch 52: 100%|██████████| 2159/2159 [01:59<00:00, 18.00it/s, v_num=6, val_loss=1.060, val_r_square=0.00881, train_loss=1.500]Epoch 52: {'val_loss': '1.06365', 'val_r_square': '0.00881', 'train_loss': '1.49684'}\n",
      "Epoch 53: 100%|██████████| 2159/2159 [02:00<00:00, 17.95it/s, v_num=6, val_loss=1.060, val_r_square=0.00887, train_loss=1.500]Epoch 53: {'val_loss': '1.06358', 'val_r_square': '0.00887', 'train_loss': '1.49671'}\n",
      "Epoch 54: 100%|██████████| 2159/2159 [01:57<00:00, 18.34it/s, v_num=6, val_loss=1.060, val_r_square=0.00905, train_loss=1.500]Epoch 54: {'val_loss': '1.06339', 'val_r_square': '0.00905', 'train_loss': '1.49678'}\n",
      "Epoch 55: 100%|██████████| 2159/2159 [02:00<00:00, 17.86it/s, v_num=6, val_loss=1.060, val_r_square=0.00884, train_loss=1.500]Epoch 55: {'val_loss': '1.06362', 'val_r_square': '0.00884', 'train_loss': '1.49680'}\n",
      "Epoch 56: 100%|██████████| 2159/2159 [02:02<00:00, 17.59it/s, v_num=6, val_loss=1.060, val_r_square=0.00894, train_loss=1.500]Epoch 56: {'val_loss': '1.06351', 'val_r_square': '0.00894', 'train_loss': '1.49669'}\n",
      "Epoch 57: 100%|██████████| 2159/2159 [02:09<00:00, 16.69it/s, v_num=6, val_loss=1.060, val_r_square=0.00868, train_loss=1.500]Epoch 57: {'val_loss': '1.06380', 'val_r_square': '0.00868', 'train_loss': '1.49675'}\n",
      "Epoch 58: 100%|██████████| 2159/2159 [02:02<00:00, 17.57it/s, v_num=6, val_loss=1.060, val_r_square=0.00908, train_loss=1.500]Epoch 58: {'val_loss': '1.06336', 'val_r_square': '0.00908', 'train_loss': '1.49673'}\n",
      "Epoch 59: 100%|██████████| 2159/2159 [02:00<00:00, 17.86it/s, v_num=6, val_loss=1.060, val_r_square=0.00882, train_loss=1.500]Epoch 59: {'val_loss': '1.06365', 'val_r_square': '0.00882', 'train_loss': '1.49675'}\n",
      "Epoch 60: 100%|██████████| 2159/2159 [02:01<00:00, 17.81it/s, v_num=6, val_loss=1.060, val_r_square=0.00924, train_loss=1.500]Epoch 60: {'val_loss': '1.06319', 'val_r_square': '0.00924', 'train_loss': '1.49679'}\n",
      "Epoch 61: 100%|██████████| 2159/2159 [02:12<00:00, 16.24it/s, v_num=6, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 61: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.49659'}\n",
      "Epoch 62: 100%|██████████| 2159/2159 [02:01<00:00, 17.71it/s, v_num=6, val_loss=1.060, val_r_square=0.00882, train_loss=1.500]Epoch 62: {'val_loss': '1.06364', 'val_r_square': '0.00882', 'train_loss': '1.49676'}\n",
      "Epoch 63: 100%|██████████| 2159/2159 [02:03<00:00, 17.42it/s, v_num=6, val_loss=1.060, val_r_square=0.00893, train_loss=1.500]Epoch 63: {'val_loss': '1.06352', 'val_r_square': '0.00893', 'train_loss': '1.49677'}\n",
      "Epoch 64: 100%|██████████| 2159/2159 [02:04<00:00, 17.31it/s, v_num=6, val_loss=1.060, val_r_square=0.00886, train_loss=1.500]Epoch 64: {'val_loss': '1.06359', 'val_r_square': '0.00886', 'train_loss': '1.49669'}\n",
      "Epoch 65: 100%|██████████| 2159/2159 [01:58<00:00, 18.17it/s, v_num=6, val_loss=1.060, val_r_square=0.00907, train_loss=1.500]Epoch 65: {'val_loss': '1.06338', 'val_r_square': '0.00907', 'train_loss': '1.49652'}\n",
      "Epoch 66: 100%|██████████| 2159/2159 [02:02<00:00, 17.63it/s, v_num=6, val_loss=1.060, val_r_square=0.00872, train_loss=1.500]Epoch 66: {'val_loss': '1.06375', 'val_r_square': '0.00872', 'train_loss': '1.49666'}\n",
      "Epoch 67: 100%|██████████| 2159/2159 [02:02<00:00, 17.67it/s, v_num=6, val_loss=1.060, val_r_square=0.0088, train_loss=1.500] Epoch 67: {'val_loss': '1.06367', 'val_r_square': '0.00880', 'train_loss': '1.49660'}\n",
      "Epoch 68: 100%|██████████| 2159/2159 [01:57<00:00, 18.35it/s, v_num=6, val_loss=1.060, val_r_square=0.00904, train_loss=1.500]Epoch 68: {'val_loss': '1.06340', 'val_r_square': '0.00904', 'train_loss': '1.49672'}\n",
      "Epoch 69: 100%|██████████| 2159/2159 [02:03<00:00, 17.46it/s, v_num=6, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 69: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.49675'}\n",
      "Epoch 70: 100%|██████████| 2159/2159 [01:57<00:00, 18.30it/s, v_num=6, val_loss=1.060, val_r_square=0.0087, train_loss=1.500] Epoch 70: {'val_loss': '1.06377', 'val_r_square': '0.00870', 'train_loss': '1.49661'}\n",
      "Epoch 71: 100%|██████████| 2159/2159 [01:59<00:00, 18.00it/s, v_num=6, val_loss=1.060, val_r_square=0.00882, train_loss=1.500]Epoch 71: {'val_loss': '1.06364', 'val_r_square': '0.00882', 'train_loss': '1.49675'}\n",
      "Epoch 72: 100%|██████████| 2159/2159 [02:01<00:00, 17.79it/s, v_num=6, val_loss=1.060, val_r_square=0.00895, train_loss=1.500]Epoch 72: {'val_loss': '1.06350', 'val_r_square': '0.00895', 'train_loss': '1.49662'}\n",
      "Epoch 73: 100%|██████████| 2159/2159 [02:01<00:00, 17.73it/s, v_num=6, val_loss=1.060, val_r_square=0.00885, train_loss=1.500]Epoch 73: {'val_loss': '1.06361', 'val_r_square': '0.00885', 'train_loss': '1.49681'}\n",
      "Epoch 74: 100%|██████████| 2159/2159 [01:59<00:00, 18.04it/s, v_num=6, val_loss=1.060, val_r_square=0.0088, train_loss=1.500] Epoch 74: {'val_loss': '1.06366', 'val_r_square': '0.00880', 'train_loss': '1.49669'}\n",
      "Epoch 75: 100%|██████████| 2159/2159 [02:02<00:00, 17.62it/s, v_num=6, val_loss=1.060, val_r_square=0.00878, train_loss=1.500]Epoch 75: {'val_loss': '1.06368', 'val_r_square': '0.00878', 'train_loss': '1.49673'}\n",
      "Epoch 76: 100%|██████████| 2159/2159 [02:01<00:00, 17.83it/s, v_num=6, val_loss=1.060, val_r_square=0.00905, train_loss=1.500]Epoch 76: {'val_loss': '1.06339', 'val_r_square': '0.00905', 'train_loss': '1.49668'}\n",
      "Epoch 77: 100%|██████████| 2159/2159 [01:59<00:00, 18.11it/s, v_num=6, val_loss=1.060, val_r_square=0.00878, train_loss=1.500]Epoch 77: {'val_loss': '1.06368', 'val_r_square': '0.00878', 'train_loss': '1.49658'}\n",
      "Epoch 78: 100%|██████████| 2159/2159 [02:01<00:00, 17.72it/s, v_num=6, val_loss=1.060, val_r_square=0.00882, train_loss=1.500]Epoch 78: {'val_loss': '1.06364', 'val_r_square': '0.00882', 'train_loss': '1.49670'}\n",
      "Epoch 79: 100%|██████████| 2159/2159 [02:10<00:00, 16.52it/s, v_num=6, val_loss=1.060, val_r_square=0.00887, train_loss=1.500]Epoch 79: {'val_loss': '1.06359', 'val_r_square': '0.00887', 'train_loss': '1.49661'}\n",
      "Epoch 80: 100%|██████████| 2159/2159 [02:01<00:00, 17.77it/s, v_num=6, val_loss=1.060, val_r_square=0.00883, train_loss=1.500]Epoch 80: {'val_loss': '1.06363', 'val_r_square': '0.00883', 'train_loss': '1.49667'}\n",
      "Epoch 81: 100%|██████████| 2159/2159 [02:07<00:00, 16.89it/s, v_num=6, val_loss=1.060, val_r_square=0.00904, train_loss=1.500]Epoch 81: {'val_loss': '1.06340', 'val_r_square': '0.00904', 'train_loss': '1.49665'}\n",
      "Epoch 82: 100%|██████████| 2159/2159 [02:02<00:00, 17.67it/s, v_num=6, val_loss=1.060, val_r_square=0.00898, train_loss=1.500]Epoch 82: {'val_loss': '1.06347', 'val_r_square': '0.00898', 'train_loss': '1.49669'}\n",
      "Epoch 83: 100%|██████████| 2159/2159 [02:10<00:00, 16.51it/s, v_num=6, val_loss=1.060, val_r_square=0.009, train_loss=1.500]  Epoch 83: {'val_loss': '1.06344', 'val_r_square': '0.00900', 'train_loss': '1.49668'}\n",
      "Epoch 84: 100%|██████████| 2159/2159 [02:03<00:00, 17.52it/s, v_num=6, val_loss=1.060, val_r_square=0.00911, train_loss=1.500]Epoch 84: {'val_loss': '1.06333', 'val_r_square': '0.00911', 'train_loss': '1.49670'}\n",
      "Epoch 85: 100%|██████████| 2159/2159 [02:01<00:00, 17.72it/s, v_num=6, val_loss=1.060, val_r_square=0.00873, train_loss=1.500]Epoch 85: {'val_loss': '1.06374', 'val_r_square': '0.00873', 'train_loss': '1.49655'}\n",
      "Epoch 85: 100%|██████████| 2159/2159 [02:01<00:00, 17.72it/s, v_num=6, val_loss=1.060, val_r_square=0.00873, train_loss=1.500]\n",
      "Fold-0 Training completed in 10512.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/micromamba/envs/kaggle/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 443 K  | train\n",
      "---------------------------------------------\n",
      "443 K     Trainable params\n",
      "0         Non-trainable params\n",
      "443 K     Total params\n",
      "1.772     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2158/2158 [02:10<00:00, 16.49it/s, v_num=7, val_loss=1.070, val_r_square=0.00617, train_loss=1.510]Epoch 0: {'val_loss': '1.06649', 'val_r_square': '0.00617', 'train_loss': '1.51029'}\n",
      "Epoch 1: 100%|██████████| 2158/2158 [02:16<00:00, 15.76it/s, v_num=7, val_loss=1.070, val_r_square=0.00647, train_loss=1.510]Epoch 1: {'val_loss': '1.06616', 'val_r_square': '0.00647', 'train_loss': '1.50784'}\n",
      "Epoch 2: 100%|██████████| 2158/2158 [02:16<00:00, 15.85it/s, v_num=7, val_loss=1.070, val_r_square=0.00679, train_loss=1.510]Epoch 2: {'val_loss': '1.06581', 'val_r_square': '0.00679', 'train_loss': '1.50643'}\n",
      "Epoch 3: 100%|██████████| 2158/2158 [02:22<00:00, 15.20it/s, v_num=7, val_loss=1.070, val_r_square=0.00685, train_loss=1.510]Epoch 3: {'val_loss': '1.06575', 'val_r_square': '0.00685', 'train_loss': '1.50576'}\n",
      "Epoch 4: 100%|██████████| 2158/2158 [02:12<00:00, 16.28it/s, v_num=7, val_loss=1.070, val_r_square=0.00716, train_loss=1.510]Epoch 4: {'val_loss': '1.06542', 'val_r_square': '0.00716', 'train_loss': '1.50510'}\n",
      "Epoch 5: 100%|██████████| 2158/2158 [02:13<00:00, 16.15it/s, v_num=7, val_loss=1.070, val_r_square=0.00689, train_loss=1.500]Epoch 5: {'val_loss': '1.06571', 'val_r_square': '0.00689', 'train_loss': '1.50473'}\n",
      "Epoch 6: 100%|██████████| 2158/2158 [02:14<00:00, 16.07it/s, v_num=7, val_loss=1.070, val_r_square=0.00725, train_loss=1.500]Epoch 6: {'val_loss': '1.06533', 'val_r_square': '0.00725', 'train_loss': '1.50442'}\n",
      "Epoch 7: 100%|██████████| 2158/2158 [02:15<00:00, 15.92it/s, v_num=7, val_loss=1.070, val_r_square=0.00689, train_loss=1.500]Epoch 7: {'val_loss': '1.06571', 'val_r_square': '0.00689', 'train_loss': '1.50407'}\n",
      "Epoch 8: 100%|██████████| 2158/2158 [02:14<00:00, 16.09it/s, v_num=7, val_loss=1.070, val_r_square=0.00714, train_loss=1.500]Epoch 8: {'val_loss': '1.06544', 'val_r_square': '0.00714', 'train_loss': '1.50392'}\n",
      "Epoch 9: 100%|██████████| 2158/2158 [02:10<00:00, 16.49it/s, v_num=7, val_loss=1.060, val_r_square=0.00821, train_loss=1.500]Epoch 9: {'val_loss': '1.06429', 'val_r_square': '0.00821', 'train_loss': '1.50358'}\n",
      "Epoch 10: 100%|██████████| 2158/2158 [02:14<00:00, 16.06it/s, v_num=7, val_loss=1.060, val_r_square=0.00763, train_loss=1.500]Epoch 10: {'val_loss': '1.06492', 'val_r_square': '0.00763', 'train_loss': '1.50347'}\n",
      "Epoch 11: 100%|██████████| 2158/2158 [02:16<00:00, 15.80it/s, v_num=7, val_loss=1.070, val_r_square=0.00602, train_loss=1.500]Epoch 11: {'val_loss': '1.06665', 'val_r_square': '0.00602', 'train_loss': '1.50335'}\n",
      "Epoch 12: 100%|██████████| 2158/2158 [02:14<00:00, 16.05it/s, v_num=7, val_loss=1.070, val_r_square=0.00691, train_loss=1.500]Epoch 12: {'val_loss': '1.06570', 'val_r_square': '0.00691', 'train_loss': '1.50313'}\n",
      "Epoch 13: 100%|██████████| 2158/2158 [02:15<00:00, 15.94it/s, v_num=7, val_loss=1.070, val_r_square=0.00687, train_loss=1.500]Epoch 13: {'val_loss': '1.06573', 'val_r_square': '0.00687', 'train_loss': '1.50305'}\n",
      "Epoch 14: 100%|██████████| 2158/2158 [02:13<00:00, 16.19it/s, v_num=7, val_loss=1.060, val_r_square=0.0078, train_loss=1.500] Epoch 14: {'val_loss': '1.06473', 'val_r_square': '0.00780', 'train_loss': '1.50292'}\n",
      "Epoch 15: 100%|██████████| 2158/2158 [02:18<00:00, 15.53it/s, v_num=7, val_loss=1.060, val_r_square=0.0077, train_loss=1.500]Epoch 15: {'val_loss': '1.06484', 'val_r_square': '0.00770', 'train_loss': '1.50272'}\n",
      "Epoch 16: 100%|██████████| 2158/2158 [02:13<00:00, 16.12it/s, v_num=7, val_loss=1.070, val_r_square=0.00693, train_loss=1.500]Epoch 16: {'val_loss': '1.06567', 'val_r_square': '0.00693', 'train_loss': '1.50162'}\n",
      "Epoch 17: 100%|██████████| 2158/2158 [02:14<00:00, 16.04it/s, v_num=7, val_loss=1.070, val_r_square=0.00677, train_loss=1.500]Epoch 17: {'val_loss': '1.06584', 'val_r_square': '0.00677', 'train_loss': '1.50140'}\n",
      "Epoch 18: 100%|██████████| 2158/2158 [02:17<00:00, 15.75it/s, v_num=7, val_loss=1.060, val_r_square=0.00781, train_loss=1.500]Epoch 18: {'val_loss': '1.06472', 'val_r_square': '0.00781', 'train_loss': '1.50119'}\n",
      "Epoch 19: 100%|██████████| 2158/2158 [02:16<00:00, 15.83it/s, v_num=7, val_loss=1.070, val_r_square=0.00667, train_loss=1.500]Epoch 19: {'val_loss': '1.06594', 'val_r_square': '0.00667', 'train_loss': '1.50128'}\n",
      "Epoch 20: 100%|██████████| 2158/2158 [02:13<00:00, 16.17it/s, v_num=7, val_loss=1.060, val_r_square=0.00784, train_loss=1.500]Epoch 20: {'val_loss': '1.06470', 'val_r_square': '0.00784', 'train_loss': '1.50130'}\n",
      "Epoch 21: 100%|██████████| 2158/2158 [02:09<00:00, 16.63it/s, v_num=7, val_loss=1.060, val_r_square=0.00758, train_loss=1.500]Epoch 21: {'val_loss': '1.06497', 'val_r_square': '0.00758', 'train_loss': '1.50120'}\n",
      "Epoch 22: 100%|██████████| 2158/2158 [02:13<00:00, 16.20it/s, v_num=7, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 22: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.50033'}\n",
      "Epoch 23: 100%|██████████| 2158/2158 [02:14<00:00, 15.99it/s, v_num=7, val_loss=1.060, val_r_square=0.00808, train_loss=1.500]Epoch 23: {'val_loss': '1.06443', 'val_r_square': '0.00808', 'train_loss': '1.50029'}\n",
      "Epoch 24: 100%|██████████| 2158/2158 [02:16<00:00, 15.84it/s, v_num=7, val_loss=1.060, val_r_square=0.00795, train_loss=1.500]Epoch 24: {'val_loss': '1.06457', 'val_r_square': '0.00795', 'train_loss': '1.50020'}\n",
      "Epoch 25: 100%|██████████| 2158/2158 [02:20<00:00, 15.32it/s, v_num=7, val_loss=1.060, val_r_square=0.00837, train_loss=1.500]Epoch 25: {'val_loss': '1.06412', 'val_r_square': '0.00837', 'train_loss': '1.50017'}\n",
      "Epoch 26: 100%|██████████| 2158/2158 [02:09<00:00, 16.61it/s, v_num=7, val_loss=1.060, val_r_square=0.0085, train_loss=1.500] Epoch 26: {'val_loss': '1.06398', 'val_r_square': '0.00850', 'train_loss': '1.50028'}\n",
      "Epoch 27: 100%|██████████| 2158/2158 [02:10<00:00, 16.56it/s, v_num=7, val_loss=1.060, val_r_square=0.00842, train_loss=1.500]Epoch 27: {'val_loss': '1.06407', 'val_r_square': '0.00842', 'train_loss': '1.50028'}\n",
      "Epoch 28: 100%|██████████| 2158/2158 [02:14<00:00, 16.08it/s, v_num=7, val_loss=1.060, val_r_square=0.00827, train_loss=1.500]Epoch 28: {'val_loss': '1.06423', 'val_r_square': '0.00827', 'train_loss': '1.50017'}\n",
      "Epoch 29: 100%|██████████| 2158/2158 [02:16<00:00, 15.81it/s, v_num=7, val_loss=1.060, val_r_square=0.0085, train_loss=1.500] Epoch 29: {'val_loss': '1.06398', 'val_r_square': '0.00850', 'train_loss': '1.49969'}\n",
      "Epoch 30: 100%|██████████| 2158/2158 [02:14<00:00, 16.10it/s, v_num=7, val_loss=1.060, val_r_square=0.00775, train_loss=1.500]Epoch 30: {'val_loss': '1.06479', 'val_r_square': '0.00775', 'train_loss': '1.49962'}\n",
      "Epoch 31: 100%|██████████| 2158/2158 [02:26<00:00, 14.76it/s, v_num=7, val_loss=1.060, val_r_square=0.00834, train_loss=1.500]Epoch 31: {'val_loss': '1.06416', 'val_r_square': '0.00834', 'train_loss': '1.49948'}\n",
      "Epoch 32: 100%|██████████| 2158/2158 [02:12<00:00, 16.34it/s, v_num=7, val_loss=1.060, val_r_square=0.00827, train_loss=1.500]Epoch 32: {'val_loss': '1.06423', 'val_r_square': '0.00827', 'train_loss': '1.49949'}\n",
      "Epoch 33: 100%|██████████| 2158/2158 [02:11<00:00, 16.45it/s, v_num=7, val_loss=1.060, val_r_square=0.00862, train_loss=1.500]Epoch 33: {'val_loss': '1.06385', 'val_r_square': '0.00862', 'train_loss': '1.49961'}\n",
      "Epoch 34: 100%|██████████| 2158/2158 [02:16<00:00, 15.83it/s, v_num=7, val_loss=1.060, val_r_square=0.00809, train_loss=1.500]Epoch 34: {'val_loss': '1.06443', 'val_r_square': '0.00809', 'train_loss': '1.49958'}\n",
      "Epoch 35: 100%|██████████| 2158/2158 [02:13<00:00, 16.20it/s, v_num=7, val_loss=1.060, val_r_square=0.00835, train_loss=1.500]Epoch 35: {'val_loss': '1.06415', 'val_r_square': '0.00835', 'train_loss': '1.49928'}\n",
      "Epoch 36: 100%|██████████| 2158/2158 [02:13<00:00, 16.21it/s, v_num=7, val_loss=1.060, val_r_square=0.0086, train_loss=1.500] Epoch 36: {'val_loss': '1.06387', 'val_r_square': '0.00860', 'train_loss': '1.49907'}\n",
      "Epoch 37: 100%|██████████| 2158/2158 [02:22<00:00, 15.18it/s, v_num=7, val_loss=1.060, val_r_square=0.00879, train_loss=1.500]Epoch 37: {'val_loss': '1.06368', 'val_r_square': '0.00879', 'train_loss': '1.49918'}\n",
      "Epoch 38: 100%|██████████| 2158/2158 [02:11<00:00, 16.36it/s, v_num=7, val_loss=1.060, val_r_square=0.00834, train_loss=1.500]Epoch 38: {'val_loss': '1.06415', 'val_r_square': '0.00834', 'train_loss': '1.49914'}\n",
      "Epoch 39: 100%|██████████| 2158/2158 [02:20<00:00, 15.34it/s, v_num=7, val_loss=1.060, val_r_square=0.00821, train_loss=1.500]Epoch 39: {'val_loss': '1.06429', 'val_r_square': '0.00821', 'train_loss': '1.49922'}\n",
      "Epoch 40: 100%|██████████| 2158/2158 [02:13<00:00, 16.13it/s, v_num=7, val_loss=1.060, val_r_square=0.00806, train_loss=1.500]Epoch 40: {'val_loss': '1.06446', 'val_r_square': '0.00806', 'train_loss': '1.49913'}\n",
      "Epoch 41: 100%|██████████| 2158/2158 [02:16<00:00, 15.84it/s, v_num=7, val_loss=1.060, val_r_square=0.00843, train_loss=1.500]Epoch 41: {'val_loss': '1.06406', 'val_r_square': '0.00843', 'train_loss': '1.49889'}\n",
      "Epoch 42: 100%|██████████| 2158/2158 [02:11<00:00, 16.39it/s, v_num=7, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 42: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.49876'}\n",
      "Epoch 43: 100%|██████████| 2158/2158 [02:14<00:00, 16.05it/s, v_num=7, val_loss=1.060, val_r_square=0.00896, train_loss=1.500]Epoch 43: {'val_loss': '1.06349', 'val_r_square': '0.00896', 'train_loss': '1.49885'}\n",
      "Epoch 44: 100%|██████████| 2158/2158 [02:12<00:00, 16.29it/s, v_num=7, val_loss=1.060, val_r_square=0.00843, train_loss=1.500]Epoch 44: {'val_loss': '1.06406', 'val_r_square': '0.00843', 'train_loss': '1.49878'}\n",
      "Epoch 45: 100%|██████████| 2158/2158 [02:13<00:00, 16.17it/s, v_num=7, val_loss=1.060, val_r_square=0.00846, train_loss=1.500]Epoch 45: {'val_loss': '1.06402', 'val_r_square': '0.00846', 'train_loss': '1.49878'}\n",
      "Epoch 46: 100%|██████████| 2158/2158 [02:12<00:00, 16.23it/s, v_num=7, val_loss=1.060, val_r_square=0.00834, train_loss=1.500]Epoch 46: {'val_loss': '1.06416', 'val_r_square': '0.00834', 'train_loss': '1.49880'}\n",
      "Epoch 47: 100%|██████████| 2158/2158 [02:10<00:00, 16.51it/s, v_num=7, val_loss=1.060, val_r_square=0.00852, train_loss=1.500]Epoch 47: {'val_loss': '1.06396', 'val_r_square': '0.00852', 'train_loss': '1.49888'}\n",
      "Epoch 48: 100%|██████████| 2158/2158 [02:09<00:00, 16.63it/s, v_num=7, val_loss=1.060, val_r_square=0.00845, train_loss=1.500]Epoch 48: {'val_loss': '1.06404', 'val_r_square': '0.00845', 'train_loss': '1.49893'}\n",
      "Epoch 49: 100%|██████████| 2158/2158 [02:22<00:00, 15.14it/s, v_num=7, val_loss=1.060, val_r_square=0.00874, train_loss=1.500]Epoch 49: {'val_loss': '1.06372', 'val_r_square': '0.00874', 'train_loss': '1.49883'}\n",
      "Epoch 50: 100%|██████████| 2158/2158 [02:13<00:00, 16.13it/s, v_num=7, val_loss=1.060, val_r_square=0.00876, train_loss=1.500]Epoch 50: {'val_loss': '1.06371', 'val_r_square': '0.00876', 'train_loss': '1.49864'}\n",
      "Epoch 51: 100%|██████████| 2158/2158 [02:23<00:00, 15.07it/s, v_num=7, val_loss=1.060, val_r_square=0.00871, train_loss=1.500]Epoch 51: {'val_loss': '1.06376', 'val_r_square': '0.00871', 'train_loss': '1.49874'}\n",
      "Epoch 52: 100%|██████████| 2158/2158 [02:12<00:00, 16.23it/s, v_num=7, val_loss=1.060, val_r_square=0.00813, train_loss=1.500]Epoch 52: {'val_loss': '1.06438', 'val_r_square': '0.00813', 'train_loss': '1.49876'}\n",
      "Epoch 53: 100%|██████████| 2158/2158 [02:13<00:00, 16.11it/s, v_num=7, val_loss=1.060, val_r_square=0.00887, train_loss=1.500]Epoch 53: {'val_loss': '1.06359', 'val_r_square': '0.00887', 'train_loss': '1.49871'}\n",
      "Epoch 54: 100%|██████████| 2158/2158 [02:10<00:00, 16.49it/s, v_num=7, val_loss=1.060, val_r_square=0.00863, train_loss=1.500]Epoch 54: {'val_loss': '1.06384', 'val_r_square': '0.00863', 'train_loss': '1.49862'}\n",
      "Epoch 55: 100%|██████████| 2158/2158 [02:14<00:00, 16.01it/s, v_num=7, val_loss=1.060, val_r_square=0.00872, train_loss=1.500]Epoch 55: {'val_loss': '1.06375', 'val_r_square': '0.00872', 'train_loss': '1.49872'}\n",
      "Epoch 56: 100%|██████████| 2158/2158 [02:19<00:00, 15.42it/s, v_num=7, val_loss=1.060, val_r_square=0.00873, train_loss=1.500]Epoch 56: {'val_loss': '1.06374', 'val_r_square': '0.00873', 'train_loss': '1.49879'}\n",
      "Epoch 57: 100%|██████████| 2158/2158 [02:11<00:00, 16.44it/s, v_num=7, val_loss=1.060, val_r_square=0.00864, train_loss=1.500]Epoch 57: {'val_loss': '1.06383', 'val_r_square': '0.00864', 'train_loss': '1.49868'}\n",
      "Epoch 58: 100%|██████████| 2158/2158 [02:14<00:00, 16.06it/s, v_num=7, val_loss=1.060, val_r_square=0.00832, train_loss=1.500]Epoch 58: {'val_loss': '1.06418', 'val_r_square': '0.00832', 'train_loss': '1.49872'}\n",
      "Epoch 59: 100%|██████████| 2158/2158 [02:16<00:00, 15.81it/s, v_num=7, val_loss=1.060, val_r_square=0.00874, train_loss=1.500]Epoch 59: {'val_loss': '1.06372', 'val_r_square': '0.00874', 'train_loss': '1.49868'}\n",
      "Epoch 60: 100%|██████████| 2158/2158 [02:12<00:00, 16.28it/s, v_num=7, val_loss=1.060, val_r_square=0.00852, train_loss=1.500]Epoch 60: {'val_loss': '1.06396', 'val_r_square': '0.00852', 'train_loss': '1.49863'}\n",
      "Epoch 61: 100%|██████████| 2158/2158 [02:23<00:00, 15.03it/s, v_num=7, val_loss=1.060, val_r_square=0.00863, train_loss=1.500]Epoch 61: {'val_loss': '1.06385', 'val_r_square': '0.00863', 'train_loss': '1.49855'}\n",
      "Epoch 62: 100%|██████████| 2158/2158 [02:12<00:00, 16.30it/s, v_num=7, val_loss=1.060, val_r_square=0.00887, train_loss=1.500]Epoch 62: {'val_loss': '1.06359', 'val_r_square': '0.00887', 'train_loss': '1.49865'}\n",
      "Epoch 63: 100%|██████████| 2158/2158 [02:17<00:00, 15.65it/s, v_num=7, val_loss=1.060, val_r_square=0.00857, train_loss=1.500]Epoch 63: {'val_loss': '1.06391', 'val_r_square': '0.00857', 'train_loss': '1.49869'}\n",
      "Epoch 64: 100%|██████████| 2158/2158 [02:15<00:00, 15.97it/s, v_num=7, val_loss=1.060, val_r_square=0.00884, train_loss=1.500]Epoch 64: {'val_loss': '1.06362', 'val_r_square': '0.00884', 'train_loss': '1.49868'}\n",
      "Epoch 65: 100%|██████████| 2158/2158 [02:14<00:00, 15.99it/s, v_num=7, val_loss=1.060, val_r_square=0.00842, train_loss=1.500]Epoch 65: {'val_loss': '1.06407', 'val_r_square': '0.00842', 'train_loss': '1.49850'}\n",
      "Epoch 66: 100%|██████████| 2158/2158 [02:15<00:00, 15.98it/s, v_num=7, val_loss=1.060, val_r_square=0.00864, train_loss=1.500]Epoch 66: {'val_loss': '1.06384', 'val_r_square': '0.00864', 'train_loss': '1.49868'}\n",
      "Epoch 67: 100%|██████████| 2158/2158 [02:12<00:00, 16.30it/s, v_num=7, val_loss=1.060, val_r_square=0.00856, train_loss=1.500]Epoch 67: {'val_loss': '1.06392', 'val_r_square': '0.00856', 'train_loss': '1.49856'}\n",
      "Epoch 68: 100%|██████████| 2158/2158 [02:10<00:00, 16.60it/s, v_num=7, val_loss=1.060, val_r_square=0.00832, train_loss=1.500]Epoch 68: {'val_loss': '1.06418', 'val_r_square': '0.00832', 'train_loss': '1.49867'}\n",
      "Epoch 68: 100%|██████████| 2158/2158 [02:10<00:00, 16.59it/s, v_num=7, val_loss=1.060, val_r_square=0.00832, train_loss=1.500]\n",
      "Fold-1 Training completed in 9306.60s\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del train, valid\n",
    "gc.collect()\n",
    "\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "\n",
    "for fold in range(args.N_fold):\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    data_module.setup(fold, args.N_fold)\n",
    "    input_dim = data_module.train_dataset.features.shape[1]\n",
    "\n",
    "    model = NN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=args.n_hidden,\n",
    "        dropouts=args.dropouts,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    if args.use_wandb:\n",
    "        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "        logger = WandbLogger(experiment=wandb_run)\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n",
    "    timer = Timer()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=[args.gpuid] if args.usegpu else None,\n",
    "        logger=logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=model, \n",
    "        train_dataloaders=data_module.train_dataloader(args.loader_workers), \n",
    "        val_dataloaders=data_module.val_dataloader(args.loader_workers)\n",
    "    )\n",
    "    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
