{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e958012cc8606e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:21.373114Z",
     "start_time": "2025-01-10T07:08:20.626469Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6414bbb8-d7f0-4ba7-9f99-e349a4ae48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_numerical_features: int,\n",
    "                 hidden_size: int = 1024,\n",
    "                 n_target: int = 1,\n",
    "                 channel_1: int = 64,\n",
    "                 channel_2: int = 128,\n",
    "                 kernel_size: int = 5,\n",
    "                 dropout_rate: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size  # Store hidden_size as an instance variable\n",
    "\n",
    "        # 1. Expand 단계: Dense 레이어\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.LayerNorm(num_numerical_features),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_numerical_features, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=hidden_size // 16, \n",
    "                out_channels=channel_1, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=1, \n",
    "                padding=kernel_size // 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # 3. Conv 블록 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=channel_1, \n",
    "                out_channels=channel_2, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=1, \n",
    "                padding=kernel_size // 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.AdaptiveAvgPool1d(output_size=16)\n",
    "        )\n",
    "\n",
    "        # 4. Flatten and Dense\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(channel_2 * 16, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(640, n_target)\n",
    "        )\n",
    "\n",
    "        # 추가된 Tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Expand 단계\n",
    "        x = self.expand(x)\n",
    "\n",
    "        # 2. Reshape to match Conv input\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1) // (self.hidden_size // 16)  # Dynamically compute seq_length\n",
    "        x = x.view(batch_size, self.hidden_size // 16, seq_length)\n",
    "\n",
    "        # 3. Conv 블록 1\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # 4. Conv 블록 2\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Flatten and Dense\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = 5 * self.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee7e46a66e4c424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:21.502341Z",
     "start_time": "2025-01-10T07:08:21.499862Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mse_loss(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Multi-target weighted MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: target values (batch_size, n_targets)\n",
    "        y_pred: predicted values (batch_size, n_targets)\n",
    "        weights: weights for each target (batch_size, n_targets)\n",
    "    \"\"\"\n",
    "    return torch.mean(weights * (y_true - y_pred)**2)\n",
    "\n",
    "def weighted_r2_score(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Multi-target weighted R2 score\n",
    "\n",
    "    Args:\n",
    "        y_true: target values (batch_size, n_targets)\n",
    "        y_pred: predicted values (batch_size, n_targets)\n",
    "        weights: weights for each target (batch_size, n_targets)\n",
    "\n",
    "    Returns:\n",
    "        weighted R2 score (scalar)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on CPU and converted to numpy\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    weights = weights.detach().cpu().numpy()\n",
    "\n",
    "    weights = np.repeat(weights, y_true.shape[1], axis=1)\n",
    "\n",
    "    # print(y_true.shape, y_pred.shape, weights.shape)\n",
    "    # Calculate weighted means for each target\n",
    "    weighted_mean = np.average(y_true, weights=weights, axis=0)\n",
    "\n",
    "    # Calculate total sum of squares\n",
    "    total_ss = np.sum(weights * (y_true - weighted_mean) ** 2, axis=0)\n",
    "\n",
    "    # Calculate residual sum of squares\n",
    "    residual_ss = np.sum(weights * (y_true - y_pred) ** 2, axis=0)\n",
    "\n",
    "    # Calculate R2 score for each target\n",
    "    r2_scores = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    # Return mean R2 score across all targets\n",
    "    return np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70674ae941a64fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:21.433171Z",
     "start_time": "2025-01-10T07:08:21.429338Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, numerical_columns, target_columns, weight_columns=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: pandas DataFrame containing all features\n",
    "            numerical_columns: list of column names for numerical features\n",
    "            target_columns: list of target column names\n",
    "            weight_columns: list of weight column names (optional)\n",
    "        \"\"\"\n",
    "        self.numerical_features = torch.FloatTensor(data[numerical_columns].values)\n",
    "        self.symbol = torch.LongTensor(data['symbol_id'].values)\n",
    "        self.feature_09 = torch.LongTensor(data['feature_09'].values)\n",
    "        self.feature_10 = torch.LongTensor(data['feature_10'].values)\n",
    "        self.feature_11 = torch.LongTensor(data['feature_11'].values)\n",
    "        self.time = torch.LongTensor(data['time_id'].values)\n",
    "\n",
    "        # Multi-target 처리\n",
    "        self.targets = torch.FloatTensor(data[target_columns].values)\n",
    "\n",
    "        # 가중치 처리 (옵션)\n",
    "        if weight_columns:\n",
    "            self.weights = torch.FloatTensor(data[weight_columns].values)\n",
    "        else:\n",
    "            self.weights = torch.ones_like(self.targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'numerical_features': self.numerical_features[idx],\n",
    "            'symbol_id': self.symbol[idx],\n",
    "            'feature_09': self.feature_09[idx],\n",
    "            'feature_10': self.feature_10[idx],\n",
    "            'feature_11': self.feature_11[idx],\n",
    "            'time_id': self.time[idx],\n",
    "            'targets': self.targets[idx],\n",
    "            'weights': self.weights[idx]\n",
    "        }\n",
    "\n",
    "def create_data_loaders(train_data, valid_data, numerical_columns,\n",
    "                        target_columns, weight_columns=None,\n",
    "                        batch_size=256, num_workers=4):\n",
    "    \"\"\"\n",
    "    데이터로더를 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        train_data: 학습 데이터가 담긴 DataFrame\n",
    "        valid_data: 검증 데이터가 담긴 DataFrame\n",
    "        numerical_columns: 수치형 특성들의 컬럼명 리스트\n",
    "        target_columns: 타겟 변수들의 컬럼명 리스트\n",
    "        weight_columns: 가중치 컬럼명 리스트 (옵션)\n",
    "        batch_size: 배치 크기\n",
    "        num_workers: 데이터 로딩에 사용할 워커 수\n",
    "    \"\"\"\n",
    "\n",
    "    # Dataset 객체 생성\n",
    "    train_dataset = CustomDataset(train_data, numerical_columns, target_columns, weight_columns)\n",
    "    valid_dataset = CustomDataset(valid_data, numerical_columns, target_columns, weight_columns)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_r2 = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # 데이터를 디바이스로 이동\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        symbol = batch['symbol_id'].to(device)\n",
    "        feature_09 = batch['feature_09'].to(device)\n",
    "        feature_10 = batch['feature_10'].to(device)\n",
    "        feature_11 = batch['feature_11'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        weights = batch['weights'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(numerical_features)\n",
    "\n",
    "        # 손실과 R2 score 계산\n",
    "        loss = weighted_mse_loss(targets, outputs, weights)\n",
    "        r2 = weighted_r2_score(targets, outputs, weights)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_r2 += r2\n",
    "\n",
    "        # 배치별 진행상황 출력 (10배치마다)\n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_r2 = total_r2 / (batch_idx + 1)\n",
    "            print(f'Batch [{batch_idx+1}/{num_batches}] Loss: {avg_loss:.4f}, R2: {avg_r2:.4f}')\n",
    "\n",
    "    return total_loss / num_batches, total_r2 / num_batches\n",
    "\n",
    "def validate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_r2 = 0\n",
    "    num_batches = len(valid_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_loader):\n",
    "            numerical_features = batch['numerical_features'].to(device)\n",
    "            feature_09 = batch['feature_09'].to(device)\n",
    "            feature_10 = batch['feature_10'].to(device)\n",
    "            feature_11 = batch['feature_11'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            weights = batch['weights'].to(device)\n",
    "\n",
    "            outputs = model(numerical_features)\n",
    "\n",
    "            loss = weighted_mse_loss(targets, outputs, weights)\n",
    "            r2 = weighted_r2_score(targets, outputs, weights)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_r2 += r2\n",
    "\n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_r2 = total_r2 / (batch_idx + 1)\n",
    "            print(f'Batch [{batch_idx+1}/{num_batches}] Loss: {avg_loss:.4f}, R2: {avg_r2:.4f}')\n",
    "        \n",
    "    return total_loss / num_batches, total_r2 / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef27647920443fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:30.651969Z",
     "start_time": "2025-01-10T07:08:21.557543Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "train = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/training.parquet\").collect().to_pandas()\n",
    "valid = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet\").collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ed06be8025f3cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:37.243949Z",
     "start_time": "2025-01-10T07:08:30.667657Z"
    }
   },
   "outputs": [],
   "source": [
    "data = train\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38009203f70d09a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:37.258361Z",
     "start_time": "2025-01-10T07:08:37.256584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df Length: 31052392\n",
      "valid_df Length: 3450266\n"
     ]
    }
   ],
   "source": [
    "data_length = data.shape[0]\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_point = int(data_length * train_ratio)\n",
    "\n",
    "train_df = data[:split_point]\n",
    "valid_df = data[split_point:]\n",
    "\n",
    "print(f\"train_df Length: {train_df.shape[0]}\")\n",
    "print(f\"valid_df Length: {valid_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe46e5ef201b2593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:37.278794Z",
     "start_time": "2025-01-10T07:08:37.276792Z"
    }
   },
   "outputs": [],
   "source": [
    "category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    "                     'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    "                     'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "                                    76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    "                     'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "                                   20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    "                     'time_id' : {i : i for i in range(968)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58df5ea3b9046ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:44.287575Z",
     "start_time": "2025-01-10T07:08:37.313637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_09 range after mapping:\n",
      "Train: 0 to 21\n",
      "Valid: 0 to 20\n",
      "\n",
      "feature_10 range after mapping:\n",
      "Train: 0 to 8\n",
      "Valid: 0 to 8\n",
      "\n",
      "feature_11 range after mapping:\n",
      "Train: 0 to 30\n",
      "Valid: 0 to 29\n",
      "\n",
      "symbol_id range after mapping:\n",
      "Train: 0 to 38\n",
      "Valid: 0 to 38\n",
      "\n",
      "time_id range after mapping:\n",
      "Train: 68 to 967\n",
      "Valid: 68 to 967\n"
     ]
    }
   ],
   "source": [
    "def apply_category_mappings(df, category_mappings):\n",
    "    \"\"\"\n",
    "    주어진 매핑 딕셔너리를 사용하여 범주형 변수들을 변환하는 함수\n",
    "\n",
    "    Args:\n",
    "        df: 변환할 데이터프레임\n",
    "        category_mappings: 각 컬럼별 매핑 딕셔너리\n",
    "    \"\"\"\n",
    "    df = df.copy()  # 원본 데이터 보존\n",
    "\n",
    "    for column, mapping in category_mappings.items():\n",
    "        # 매핑되지 않은 값이 있는지 체크\n",
    "        unmapped_values = set(df[column].unique()) - set(mapping.keys())\n",
    "        if unmapped_values:\n",
    "            print(f\"Warning: {column}에서 매핑되지 않은 값 발견: {unmapped_values}\")\n",
    "\n",
    "        # 매핑 적용\n",
    "        df[column] = df[column].map(mapping)\n",
    "\n",
    "        # NA 값 체크\n",
    "        na_count = df[column].isna().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"Warning: {column}에서 {na_count}개의 NA 값 발견\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 데이터로더 생성 전에 매핑 적용\n",
    "train_df = apply_category_mappings(train_df, category_mappings)\n",
    "valid_df = apply_category_mappings(valid_df, category_mappings)\n",
    "\n",
    "# 매핑 후 값 범위 확인\n",
    "for column in category_mappings.keys():\n",
    "    print(f\"\\n{column} range after mapping:\")\n",
    "    print(f\"Train: {train_df[column].min()} to {train_df[column].max()}\")\n",
    "    print(f\"Valid: {valid_df[column].min()} to {valid_df[column].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9052491a271b379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:08:44.314990Z",
     "start_time": "2025-01-10T07:08:44.302666Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/kaggle/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7c465a6c15ed71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:24:31.912968Z",
     "start_time": "2025-01-10T07:24:25.714920Z"
    }
   },
   "outputs": [],
   "source": [
    "# 컬럼 정의\n",
    "numerical_columns = data.columns[data.columns.str.contains('feature')].tolist() + ['time_id', 'symbol_id']\n",
    "target_columns = ['responder_6', 'responder_7', 'responder_8'] # 예측할 타겟들\n",
    "weight_columns = ['weight']  # 각 타겟에 대한 가중치 (옵션)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader, valid_loader = create_data_loaders(\n",
    "    train_data=train_df,\n",
    "    valid_data=valid_df,\n",
    "    numerical_columns=numerical_columns,\n",
    "    target_columns=target_columns,\n",
    "    weight_columns=weight_columns,\n",
    "    batch_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4582e666-fc10-41b0-979d-ff9662387c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3c0aa9c23428d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T07:24:32.122034Z",
     "start_time": "2025-01-10T07:24:32.112746Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CNN1DModel(\n",
    "    num_numerical_features=len(numerical_columns),\n",
    "    n_target=len(target_columns),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e82deb40bf6cbb45",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-10T07:24:39.966268Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/kaggle/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "If you really know what your doing, you can silence this warning with the warning module\n",
      "or by setting POLARS_ALLOW_FORKING_THREAD=1.\n",
      "\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1000/15163] Loss: 1.2563, R2: -0.0032\n"
     ]
    }
   ],
   "source": [
    " # 학습 루프 수정\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay = 5e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_r2 = train_epoch(model, train_loader, optimizer, device)\n",
    "    valid_loss, valid_r2 = validate(model, valid_loader, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train R2: {train_r2:.4f}')\n",
    "    print(f'Valid Loss: {valid_loss:.4f}, Valid R2: {valid_r2:.4f}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018060782ec9cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
