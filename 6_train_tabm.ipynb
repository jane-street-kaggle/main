{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TabNet-Training: Attentive Interpretable Tabular Learning\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-training/notebook\n",
    "\n",
    "# TabNet-Inference\n",
    "https://www.kaggle.com/code/i2nfinit3y/jane-street-tabm-ft-transformer-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import rtdl_num_embeddings\n",
    "from rtdl_num_embeddings import compute_bins\n",
    "import rtdl_revisiting_models\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import delu\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "import joblib\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "feature_train_list = [f\"feature_{idx:02d}\" for idx in range(79)] \n",
    "target_col = \"responder_6\"\n",
    "feature_train = feature_train_list \\\n",
    "                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n",
    "\n",
    "start_dt = 110\n",
    "end_dt = 1577\n",
    "\n",
    "feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "feature_cont = [item for item in feature_train if item not in feature_cat]\n",
    "std_feature = [i for i in feature_train_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "\n",
    "batch_size = 8192\n",
    "num_epochs = 100\n",
    "\n",
    "data_stats = joblib.load(\"/kaggle/input/jane-street-data-preprocessing/data_stats.pkl\")\n",
    "means = data_stats['mean']\n",
    "stds = data_stats['std']\n",
    "\n",
    "def standardize(df, feature_cols, means, stds):\n",
    "    return df.with_columns([\n",
    "        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found in feature_09:\n",
      "category_mappings[feature_09]: {9: 0, 12: 1, 15: 2, 30: 3, 42: 4, 57: 5, 81: 6, 4: 7, 25: 8, 34: 9, 49: 10, 46: 11, 64: 12, 70: 13, 82: 14, 2: 15, 11: 16, 26: 17, 14: 18, 44: 19, 50: 20, 68: 21}\n",
      "_category_mappings[feature_09]: {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21}\n",
      "Mismatch found in feature_10:\n",
      "category_mappings[feature_10]: {3: 0, 6: 1, 12: 2, 4: 3, 1: 4, 7: 5, 10: 6, 5: 7, 2: 8}\n",
      "_category_mappings[feature_10]: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8}\n",
      "Mismatch found in feature_11:\n",
      "category_mappings[feature_11]: {9: 0, 539: 1, 24: 2, 158: 3, 48: 4, 63: 5, 66: 6, 230: 7, 376: 8, 388: 9, 522: 10, 13: 11, 534: 12, 150: 13, 16: 14, 25: 15, 159: 16, 34: 17, 40: 18, 171: 19, 195: 20, 76: 21, 261: 22, 11: 23, 410: 24, 297: 25, 50: 26, 62: 27, 59: 28, 336: 29, 214: 30}\n",
      "_category_mappings[feature_11]: {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13, 76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30}\n",
      "Mismatch found in symbol_id:\n",
      "category_mappings[symbol_id]: {0: 0, 6: 1, 12: 2, 3: 3, 9: 4, 18: 5, 15: 6, 24: 7, 21: 8, 36: 9, 33: 10, 27: 11, 30: 12, 7: 13, 1: 14, 13: 15, 10: 16, 4: 17, 22: 18, 25: 19, 16: 20, 19: 21, 28: 22, 34: 23, 37: 24, 31: 25, 5: 26, 8: 27, 11: 28, 2: 29, 26: 30, 20: 31, 17: 32, 23: 33, 14: 34, 29: 35, 38: 36, 32: 37, 35: 38}\n",
      "_category_mappings[symbol_id]: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38}\n",
      "Mismatch found in time_id:\n",
      "category_mappings[time_id]: {920: 0, 265: 1, 396: 2, 661: 3, 3: 4, 923: 5, 6: 6, 12: 7, 667: 8, 926: 9, 134: 10, 274: 11, 399: 12, 792: 13, 536: 14, 271: 15, 789: 16, 140: 17, 405: 18, 795: 19, 137: 20, 664: 21, 9: 22, 143: 23, 402: 24, 530: 25, 527: 26, 798: 27, 929: 28, 658: 29, 0: 30, 533: 31, 268: 32, 24: 33, 801: 34, 15: 35, 545: 36, 155: 37, 414: 38, 810: 39, 804: 40, 152: 41, 149: 42, 542: 43, 676: 44, 935: 45, 670: 46, 932: 47, 283: 48, 673: 49, 277: 50, 280: 51, 539: 52, 18: 53, 411: 54, 417: 55, 21: 56, 941: 57, 548: 58, 146: 59, 938: 60, 408: 61, 679: 62, 286: 63, 807: 64, 158: 65, 39: 66, 36: 67, 563: 68, 950: 69, 816: 70, 298: 71, 292: 72, 426: 73, 560: 74, 30: 75, 822: 76, 423: 77, 289: 78, 420: 79, 301: 80, 170: 81, 33: 82, 947: 83, 813: 84, 694: 85, 295: 86, 688: 87, 551: 88, 432: 89, 944: 90, 956: 91, 161: 92, 557: 93, 164: 94, 429: 95, 685: 96, 554: 97, 819: 98, 825: 99, 691: 100, 953: 101, 682: 102, 167: 103, 27: 104, 959: 105, 307: 106, 310: 107, 179: 108, 831: 109, 965: 110, 444: 111, 697: 112, 575: 113, 837: 114, 48: 115, 45: 116, 441: 117, 42: 118, 700: 119, 176: 120, 313: 121, 706: 122, 51: 123, 182: 124, 962: 125, 435: 126, 173: 127, 828: 128, 569: 129, 566: 130, 438: 131, 834: 132, 572: 133, 304: 134, 703: 135, 584: 136, 447: 137, 846: 138, 852: 139, 63: 140, 843: 141, 450: 142, 60: 143, 453: 144, 325: 145, 587: 146, 456: 147, 581: 148, 185: 149, 57: 150, 191: 151, 316: 152, 322: 153, 715: 154, 712: 155, 194: 156, 849: 157, 188: 158, 319: 159, 578: 160, 840: 161, 718: 162, 54: 163, 709: 164, 468: 165, 465: 166, 200: 167, 203: 168, 724: 169, 462: 170, 721: 171, 69: 172, 861: 173, 334: 174, 596: 175, 209: 176, 471: 177, 855: 178, 602: 179, 337: 180, 733: 181, 864: 182, 331: 183, 727: 184, 197: 185, 599: 186, 66: 187, 78: 188, 75: 189, 590: 190, 730: 191, 593: 192, 328: 193, 206: 194, 340: 195, 858: 196, 459: 197, 72: 198, 90: 199, 221: 200, 608: 201, 745: 202, 605: 203, 215: 204, 611: 205, 474: 206, 343: 207, 483: 208, 739: 209, 346: 210, 87: 211, 742: 212, 876: 213, 84: 214, 349: 215, 870: 216, 867: 217, 81: 218, 352: 219, 477: 220, 212: 221, 480: 222, 873: 223, 736: 224, 614: 225, 218: 226, 760: 227, 617: 228, 361: 229, 233: 230, 358: 231, 754: 232, 364: 233, 230: 234, 629: 235, 102: 236, 882: 237, 620: 238, 757: 239, 227: 240, 489: 241, 492: 242, 93: 243, 96: 244, 885: 245, 751: 246, 355: 247, 879: 248, 626: 249, 495: 250, 891: 251, 888: 252, 99: 253, 623: 254, 486: 255, 748: 256, 498: 257, 224: 258, 367: 259, 242: 260, 376: 261, 248: 262, 236: 263, 105: 264, 111: 265, 769: 266, 507: 267, 239: 268, 641: 269, 772: 270, 635: 271, 117: 272, 766: 273, 897: 274, 373: 275, 379: 276, 903: 277, 638: 278, 510: 279, 501: 280, 108: 281, 504: 282, 763: 283, 900: 284, 370: 285, 894: 286, 632: 287, 245: 288, 114: 289, 260: 290, 123: 291, 778: 292, 912: 293, 391: 294, 385: 295, 781: 296, 644: 297, 513: 298, 519: 299, 909: 300, 382: 301, 522: 302, 784: 303, 915: 304, 653: 305, 906: 306, 775: 307, 126: 308, 516: 309, 254: 310, 120: 311, 251: 312, 650: 313, 129: 314, 257: 315, 647: 316, 388: 317, 406: 318, 132: 319, 796: 320, 528: 321, 138: 322, 269: 323, 263: 324, 10: 325, 790: 326, 668: 327, 659: 328, 525: 329, 13: 330, 4: 331, 921: 332, 799: 333, 403: 334, 656: 335, 135: 336, 531: 337, 662: 338, 537: 339, 927: 340, 930: 341, 400: 342, 141: 343, 397: 344, 793: 345, 266: 346, 394: 347, 918: 348, 665: 349, 924: 350, 534: 351, 1: 352, 272: 353, 7: 354, 144: 355, 787: 356, 275: 357, 278: 358, 287: 359, 16: 360, 156: 361, 153: 362, 939: 363, 674: 364, 546: 365, 677: 366, 933: 367, 808: 368, 19: 369, 147: 370, 671: 371, 811: 372, 284: 373, 802: 374, 412: 375, 805: 376, 25: 377, 409: 378, 936: 379, 150: 380, 942: 381, 549: 382, 418: 383, 415: 384, 22: 385, 281: 386, 540: 387, 543: 388, 680: 389, 299: 390, 820: 391, 954: 392, 168: 393, 817: 394, 165: 395, 558: 396, 424: 397, 37: 398, 692: 399, 28: 400, 951: 401, 552: 402, 427: 403, 683: 404, 34: 405, 823: 406, 689: 407, 31: 408, 421: 409, 945: 410, 290: 411, 430: 412, 293: 413, 948: 414, 555: 415, 159: 416, 296: 417, 686: 418, 814: 419, 561: 420, 162: 421, 695: 422, 838: 423, 174: 424, 570: 425, 564: 426, 305: 427, 707: 428, 963: 429, 698: 430, 576: 431, 46: 432, 832: 433, 445: 434, 826: 435, 180: 436, 52: 437, 701: 438, 177: 439, 308: 440, 171: 441, 436: 442, 829: 443, 43: 444, 966: 445, 835: 446, 433: 447, 957: 448, 183: 449, 704: 450, 311: 451, 960: 452, 439: 453, 49: 454, 314: 455, 40: 456, 567: 457, 573: 458, 442: 459, 302: 460, 719: 461, 716: 462, 55: 463, 585: 464, 58: 465, 189: 466, 582: 467, 847: 468, 320: 469, 186: 470, 195: 471, 192: 472, 64: 473, 454: 474, 710: 475, 841: 476, 448: 477, 61: 478, 457: 479, 844: 480, 713: 481, 850: 482, 579: 483, 323: 484, 588: 485, 326: 486, 317: 487, 451: 488, 466: 489, 70: 490, 728: 491, 338: 492, 859: 493, 73: 494, 198: 495, 207: 496, 201: 497, 76: 498, 469: 499, 731: 500, 463: 501, 734: 502, 862: 503, 600: 504, 856: 505, 865: 506, 67: 507, 332: 508, 460: 509, 329: 510, 335: 511, 591: 512, 594: 513, 725: 514, 597: 515, 204: 516, 722: 517, 853: 518, 737: 519, 484: 520, 353: 521, 91: 522, 606: 523, 222: 524, 615: 525, 612: 526, 213: 527, 609: 528, 481: 529, 740: 530, 88: 531, 85: 532, 219: 533, 871: 534, 877: 535, 341: 536, 79: 537, 868: 538, 350: 539, 216: 540, 475: 541, 210: 542, 743: 543, 347: 544, 472: 545, 82: 546, 603: 547, 344: 548, 746: 549, 874: 550, 478: 551, 880: 552, 94: 553, 758: 554, 496: 555, 231: 556, 97: 557, 225: 558, 621: 559, 234: 560, 493: 561, 359: 562, 356: 563, 103: 564, 362: 565, 365: 566, 624: 567, 752: 568, 889: 569, 627: 570, 490: 571, 228: 572, 618: 573, 886: 574, 883: 575, 487: 576, 755: 577, 749: 578, 100: 579, 380: 580, 246: 581, 109: 582, 895: 583, 249: 584, 371: 585, 773: 586, 502: 587, 633: 588, 898: 589, 240: 590, 243: 591, 499: 592, 377: 593, 112: 594, 764: 595, 511: 596, 508: 597, 636: 598, 892: 599, 770: 600, 115: 601, 106: 602, 374: 603, 904: 604, 761: 605, 505: 606, 767: 607, 642: 608, 630: 609, 639: 610, 237: 611, 901: 612, 368: 613, 389: 614, 118: 615, 776: 616, 523: 617, 261: 618, 258: 619, 913: 620, 255: 621, 910: 622, 121: 623, 517: 624, 785: 625, 127: 626, 907: 627, 520: 628, 782: 629, 916: 630, 124: 631, 130: 632, 645: 633, 651: 634, 648: 635, 383: 636, 654: 637, 779: 638, 252: 639, 392: 640, 386: 641, 514: 642, 133: 643, 5: 644, 535: 645, 788: 646, 666: 647, 660: 648, 273: 649, 922: 650, 532: 651, 529: 652, 791: 653, 657: 654, 267: 655, 2: 656, 270: 657, 794: 658, 11: 659, 526: 660, 928: 661, 264: 662, 136: 663, 142: 664, 395: 665, 797: 666, 919: 667, 8: 668, 925: 669, 398: 670, 139: 671, 404: 672, 663: 673, 401: 674, 812: 675, 943: 676, 157: 677, 279: 678, 20: 679, 675: 680, 809: 681, 681: 682, 407: 683, 26: 684, 413: 685, 410: 686, 17: 687, 419: 688, 14: 689, 416: 690, 934: 691, 276: 692, 550: 693, 145: 694, 541: 695, 800: 696, 803: 697, 547: 698, 544: 699, 931: 700, 288: 701, 669: 702, 23: 703, 282: 704, 940: 705, 285: 706, 148: 707, 151: 708, 678: 709, 806: 710, 672: 711, 154: 712, 937: 713, 538: 714, 553: 715, 693: 716, 818: 717, 169: 718, 422: 719, 821: 720, 29: 721, 428: 722, 952: 723, 687: 724, 946: 725, 684: 726, 160: 727, 556: 728, 38: 729, 690: 730, 431: 731, 949: 732, 562: 733, 294: 734, 955: 735, 291: 736, 824: 737, 300: 738, 35: 739, 32: 740, 815: 741, 559: 742, 163: 743, 425: 744, 297: 745, 166: 746, 827: 747, 702: 748, 312: 749, 309: 750, 574: 751, 565: 752, 443: 753, 696: 754, 50: 755, 961: 756, 303: 757, 175: 758, 41: 759, 181: 760, 440: 761, 964: 762, 437: 763, 830: 764, 178: 765, 172: 766, 836: 767, 833: 768, 571: 769, 44: 770, 705: 771, 967: 772, 306: 773, 568: 774, 434: 775, 958: 776, 47: 777, 699: 778, 62: 779, 446: 780, 318: 781, 711: 782, 845: 783, 452: 784, 458: 785, 586: 786, 59: 787, 449: 788, 65: 789, 708: 790, 580: 791, 324: 792, 321: 793, 193: 794, 714: 795, 53: 796, 839: 797, 56: 798, 184: 799, 589: 800, 577: 801, 327: 802, 196: 803, 583: 804, 315: 805, 455: 806, 190: 807, 717: 808, 851: 809, 848: 810, 842: 811, 187: 812, 720: 813, 202: 814, 470: 815, 854: 816, 598: 817, 595: 818, 464: 819, 863: 820, 592: 821, 205: 822, 723: 823, 336: 824, 74: 825, 71: 826, 601: 827, 726: 828, 857: 829, 860: 830, 208: 831, 68: 832, 729: 833, 199: 834, 467: 835, 461: 836, 732: 837, 339: 838, 333: 839, 77: 840, 330: 841, 473: 842, 878: 843, 872: 844, 735: 845, 351: 846, 875: 847, 479: 848, 345: 849, 217: 850, 741: 851, 220: 852, 747: 853, 610: 854, 83: 855, 869: 856, 744: 857, 607: 858, 482: 859, 89: 860, 80: 861, 214: 862, 348: 863, 476: 864, 616: 865, 604: 866, 342: 867, 866: 868, 613: 869, 86: 870, 738: 871, 211: 872, 628: 873, 488: 874, 363: 875, 226: 876, 881: 877, 494: 878, 235: 879, 491: 880, 229: 881, 750: 882, 887: 883, 95: 884, 101: 885, 223: 886, 98: 887, 357: 888, 485: 889, 360: 890, 92: 891, 497: 892, 354: 893, 625: 894, 890: 895, 622: 896, 884: 897, 759: 898, 753: 899, 104: 900, 756: 901, 619: 902, 232: 903, 366: 904, 500: 905, 244: 906, 241: 907, 762: 908, 107: 909, 113: 910, 634: 911, 369: 912, 116: 913, 238: 914, 509: 915, 893: 916, 375: 917, 899: 918, 768: 919, 378: 920, 247: 921, 902: 922, 506: 923, 631: 924, 765: 925, 640: 926, 896: 927, 110: 928, 503: 929, 372: 930, 637: 931, 771: 932, 643: 933, 652: 934, 390: 935, 780: 936, 256: 937, 646: 938, 381: 939, 649: 940, 655: 941, 384: 942, 911: 943, 524: 944, 393: 945, 521: 946, 125: 947, 119: 948, 128: 949, 515: 950, 783: 951, 786: 952, 250: 953, 777: 954, 914: 955, 518: 956, 774: 957, 131: 958, 253: 959, 262: 960, 259: 961, 387: 962, 905: 963, 908: 964, 917: 965, 122: 966, 512: 967}\n",
      "_category_mappings[time_id]: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179, 180: 180, 181: 181, 182: 182, 183: 183, 184: 184, 185: 185, 186: 186, 187: 187, 188: 188, 189: 189, 190: 190, 191: 191, 192: 192, 193: 193, 194: 194, 195: 195, 196: 196, 197: 197, 198: 198, 199: 199, 200: 200, 201: 201, 202: 202, 203: 203, 204: 204, 205: 205, 206: 206, 207: 207, 208: 208, 209: 209, 210: 210, 211: 211, 212: 212, 213: 213, 214: 214, 215: 215, 216: 216, 217: 217, 218: 218, 219: 219, 220: 220, 221: 221, 222: 222, 223: 223, 224: 224, 225: 225, 226: 226, 227: 227, 228: 228, 229: 229, 230: 230, 231: 231, 232: 232, 233: 233, 234: 234, 235: 235, 236: 236, 237: 237, 238: 238, 239: 239, 240: 240, 241: 241, 242: 242, 243: 243, 244: 244, 245: 245, 246: 246, 247: 247, 248: 248, 249: 249, 250: 250, 251: 251, 252: 252, 253: 253, 254: 254, 255: 255, 256: 256, 257: 257, 258: 258, 259: 259, 260: 260, 261: 261, 262: 262, 263: 263, 264: 264, 265: 265, 266: 266, 267: 267, 268: 268, 269: 269, 270: 270, 271: 271, 272: 272, 273: 273, 274: 274, 275: 275, 276: 276, 277: 277, 278: 278, 279: 279, 280: 280, 281: 281, 282: 282, 283: 283, 284: 284, 285: 285, 286: 286, 287: 287, 288: 288, 289: 289, 290: 290, 291: 291, 292: 292, 293: 293, 294: 294, 295: 295, 296: 296, 297: 297, 298: 298, 299: 299, 300: 300, 301: 301, 302: 302, 303: 303, 304: 304, 305: 305, 306: 306, 307: 307, 308: 308, 309: 309, 310: 310, 311: 311, 312: 312, 313: 313, 314: 314, 315: 315, 316: 316, 317: 317, 318: 318, 319: 319, 320: 320, 321: 321, 322: 322, 323: 323, 324: 324, 325: 325, 326: 326, 327: 327, 328: 328, 329: 329, 330: 330, 331: 331, 332: 332, 333: 333, 334: 334, 335: 335, 336: 336, 337: 337, 338: 338, 339: 339, 340: 340, 341: 341, 342: 342, 343: 343, 344: 344, 345: 345, 346: 346, 347: 347, 348: 348, 349: 349, 350: 350, 351: 351, 352: 352, 353: 353, 354: 354, 355: 355, 356: 356, 357: 357, 358: 358, 359: 359, 360: 360, 361: 361, 362: 362, 363: 363, 364: 364, 365: 365, 366: 366, 367: 367, 368: 368, 369: 369, 370: 370, 371: 371, 372: 372, 373: 373, 374: 374, 375: 375, 376: 376, 377: 377, 378: 378, 379: 379, 380: 380, 381: 381, 382: 382, 383: 383, 384: 384, 385: 385, 386: 386, 387: 387, 388: 388, 389: 389, 390: 390, 391: 391, 392: 392, 393: 393, 394: 394, 395: 395, 396: 396, 397: 397, 398: 398, 399: 399, 400: 400, 401: 401, 402: 402, 403: 403, 404: 404, 405: 405, 406: 406, 407: 407, 408: 408, 409: 409, 410: 410, 411: 411, 412: 412, 413: 413, 414: 414, 415: 415, 416: 416, 417: 417, 418: 418, 419: 419, 420: 420, 421: 421, 422: 422, 423: 423, 424: 424, 425: 425, 426: 426, 427: 427, 428: 428, 429: 429, 430: 430, 431: 431, 432: 432, 433: 433, 434: 434, 435: 435, 436: 436, 437: 437, 438: 438, 439: 439, 440: 440, 441: 441, 442: 442, 443: 443, 444: 444, 445: 445, 446: 446, 447: 447, 448: 448, 449: 449, 450: 450, 451: 451, 452: 452, 453: 453, 454: 454, 455: 455, 456: 456, 457: 457, 458: 458, 459: 459, 460: 460, 461: 461, 462: 462, 463: 463, 464: 464, 465: 465, 466: 466, 467: 467, 468: 468, 469: 469, 470: 470, 471: 471, 472: 472, 473: 473, 474: 474, 475: 475, 476: 476, 477: 477, 478: 478, 479: 479, 480: 480, 481: 481, 482: 482, 483: 483, 484: 484, 485: 485, 486: 486, 487: 487, 488: 488, 489: 489, 490: 490, 491: 491, 492: 492, 493: 493, 494: 494, 495: 495, 496: 496, 497: 497, 498: 498, 499: 499, 500: 500, 501: 501, 502: 502, 503: 503, 504: 504, 505: 505, 506: 506, 507: 507, 508: 508, 509: 509, 510: 510, 511: 511, 512: 512, 513: 513, 514: 514, 515: 515, 516: 516, 517: 517, 518: 518, 519: 519, 520: 520, 521: 521, 522: 522, 523: 523, 524: 524, 525: 525, 526: 526, 527: 527, 528: 528, 529: 529, 530: 530, 531: 531, 532: 532, 533: 533, 534: 534, 535: 535, 536: 536, 537: 537, 538: 538, 539: 539, 540: 540, 541: 541, 542: 542, 543: 543, 544: 544, 545: 545, 546: 546, 547: 547, 548: 548, 549: 549, 550: 550, 551: 551, 552: 552, 553: 553, 554: 554, 555: 555, 556: 556, 557: 557, 558: 558, 559: 559, 560: 560, 561: 561, 562: 562, 563: 563, 564: 564, 565: 565, 566: 566, 567: 567, 568: 568, 569: 569, 570: 570, 571: 571, 572: 572, 573: 573, 574: 574, 575: 575, 576: 576, 577: 577, 578: 578, 579: 579, 580: 580, 581: 581, 582: 582, 583: 583, 584: 584, 585: 585, 586: 586, 587: 587, 588: 588, 589: 589, 590: 590, 591: 591, 592: 592, 593: 593, 594: 594, 595: 595, 596: 596, 597: 597, 598: 598, 599: 599, 600: 600, 601: 601, 602: 602, 603: 603, 604: 604, 605: 605, 606: 606, 607: 607, 608: 608, 609: 609, 610: 610, 611: 611, 612: 612, 613: 613, 614: 614, 615: 615, 616: 616, 617: 617, 618: 618, 619: 619, 620: 620, 621: 621, 622: 622, 623: 623, 624: 624, 625: 625, 626: 626, 627: 627, 628: 628, 629: 629, 630: 630, 631: 631, 632: 632, 633: 633, 634: 634, 635: 635, 636: 636, 637: 637, 638: 638, 639: 639, 640: 640, 641: 641, 642: 642, 643: 643, 644: 644, 645: 645, 646: 646, 647: 647, 648: 648, 649: 649, 650: 650, 651: 651, 652: 652, 653: 653, 654: 654, 655: 655, 656: 656, 657: 657, 658: 658, 659: 659, 660: 660, 661: 661, 662: 662, 663: 663, 664: 664, 665: 665, 666: 666, 667: 667, 668: 668, 669: 669, 670: 670, 671: 671, 672: 672, 673: 673, 674: 674, 675: 675, 676: 676, 677: 677, 678: 678, 679: 679, 680: 680, 681: 681, 682: 682, 683: 683, 684: 684, 685: 685, 686: 686, 687: 687, 688: 688, 689: 689, 690: 690, 691: 691, 692: 692, 693: 693, 694: 694, 695: 695, 696: 696, 697: 697, 698: 698, 699: 699, 700: 700, 701: 701, 702: 702, 703: 703, 704: 704, 705: 705, 706: 706, 707: 707, 708: 708, 709: 709, 710: 710, 711: 711, 712: 712, 713: 713, 714: 714, 715: 715, 716: 716, 717: 717, 718: 718, 719: 719, 720: 720, 721: 721, 722: 722, 723: 723, 724: 724, 725: 725, 726: 726, 727: 727, 728: 728, 729: 729, 730: 730, 731: 731, 732: 732, 733: 733, 734: 734, 735: 735, 736: 736, 737: 737, 738: 738, 739: 739, 740: 740, 741: 741, 742: 742, 743: 743, 744: 744, 745: 745, 746: 746, 747: 747, 748: 748, 749: 749, 750: 750, 751: 751, 752: 752, 753: 753, 754: 754, 755: 755, 756: 756, 757: 757, 758: 758, 759: 759, 760: 760, 761: 761, 762: 762, 763: 763, 764: 764, 765: 765, 766: 766, 767: 767, 768: 768, 769: 769, 770: 770, 771: 771, 772: 772, 773: 773, 774: 774, 775: 775, 776: 776, 777: 777, 778: 778, 779: 779, 780: 780, 781: 781, 782: 782, 783: 783, 784: 784, 785: 785, 786: 786, 787: 787, 788: 788, 789: 789, 790: 790, 791: 791, 792: 792, 793: 793, 794: 794, 795: 795, 796: 796, 797: 797, 798: 798, 799: 799, 800: 800, 801: 801, 802: 802, 803: 803, 804: 804, 805: 805, 806: 806, 807: 807, 808: 808, 809: 809, 810: 810, 811: 811, 812: 812, 813: 813, 814: 814, 815: 815, 816: 816, 817: 817, 818: 818, 819: 819, 820: 820, 821: 821, 822: 822, 823: 823, 824: 824, 825: 825, 826: 826, 827: 827, 828: 828, 829: 829, 830: 830, 831: 831, 832: 832, 833: 833, 834: 834, 835: 835, 836: 836, 837: 837, 838: 838, 839: 839, 840: 840, 841: 841, 842: 842, 843: 843, 844: 844, 845: 845, 846: 846, 847: 847, 848: 848, 849: 849, 850: 850, 851: 851, 852: 852, 853: 853, 854: 854, 855: 855, 856: 856, 857: 857, 858: 858, 859: 859, 860: 860, 861: 861, 862: 862, 863: 863, 864: 864, 865: 865, 866: 866, 867: 867, 868: 868, 869: 869, 870: 870, 871: 871, 872: 872, 873: 873, 874: 874, 875: 875, 876: 876, 877: 877, 878: 878, 879: 879, 880: 880, 881: 881, 882: 882, 883: 883, 884: 884, 885: 885, 886: 886, 887: 887, 888: 888, 889: 889, 890: 890, 891: 891, 892: 892, 893: 893, 894: 894, 895: 895, 896: 896, 897: 897, 898: 898, 899: 899, 900: 900, 901: 901, 902: 902, 903: 903, 904: 904, 905: 905, 906: 906, 907: 907, 908: 908, 909: 909, 910: 910, 911: 911, 912: 912, 913: 913, 914: 914, 915: 915, 916: 916, 917: 917, 918: 918, 919: 919, 920: 920, 921: 921, 922: 922, 923: 923, 924: 924, 925: 925, 926: 926, 927: 927, 928: 928, 929: 929, 930: 930, 931: 931, 932: 932, 933: 933, 934: 934, 935: 935, 936: 936, 937: 937, 938: 938, 939: 939, 940: 940, 941: 941, 942: 942, 943: 943, 944: 944, 945: 945, 946: 946, 947: 947, 948: 948, 949: 949, 950: 950, 951: 951, 952: 952, 953: 953, 954: 954, 955: 955, 956: 956, 957: 957, 958: 958, 959: 959, 960: 960, 961: 961, 962: 962, 963: 963, 964: 964, 965: 965, 966: 966, 967: 967}\n"
     ]
    }
   ],
   "source": [
    "train_original = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/training.parquet\")\n",
    "valid_original = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet\")\n",
    "all_original = pl.concat([train_original, valid_original])\n",
    "\n",
    "def get_category_mapping(df, column):\n",
    "    unique_values = df.select([column]).unique().collect().to_series()\n",
    "    return {cat: idx for idx, cat in enumerate(unique_values)}\n",
    "category_mappings = {col: get_category_mapping(all_original, col) for col in feature_cat + ['symbol_id', 'time_id']}\n",
    "\n",
    "_category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    " 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    " 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    " 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    " 'time_id' : {i : i for i in range(968)}}\n",
    "\n",
    "# Compare category_mappings and _category_mappings\n",
    "for key in category_mappings:\n",
    "    if category_mappings[key] != _category_mappings.get(key, {}):\n",
    "        print(f\"Mismatch found in {key}:\")\n",
    "        print(f\"category_mappings[{key}]: {category_mappings[key]}\")\n",
    "        print(f\"_category_mappings[{key}]: {_category_mappings.get(key, {})}\")\n",
    "\n",
    "def encode_column(df, column, mapping):\n",
    "    def encode_category(category):\n",
    "        return mapping.get(category, -1)  \n",
    "    \n",
    "    return df.with_columns(\n",
    "        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n",
    "    )\n",
    "\n",
    "for col in feature_cat + ['symbol_id', 'time_id']:\n",
    "    train_original = encode_column(train_original, col, category_mappings[col])\n",
    "    valid_original = encode_column(valid_original, col, category_mappings[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = train_original \\\n",
    "             .filter((pl.col(\"date_id\") >= start_dt) & (pl.col(\"date_id\") <= end_dt)) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data2 = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") <= end_dt) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data = pl.concat([train_data1, train_data2])\n",
    "valid_data = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") > end_dt)\\\n",
    "             .sort(['date_id', 'time_id'])\\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor = torch.tensor(train_data.collect().to_numpy(), dtype=torch.float32)\n",
    "valid_data_tensor = torch.tensor(valid_data.collect().to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=1, pin_memory=False, shuffle=True)\n",
    "valid_ds = TensorDataset(valid_data_tensor)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=1, pin_memory=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = False\n",
    "if all_data:\n",
    "    train_ds = ConcatDataset([train_ds, valid_ds])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont_features = 85\n",
    "n_cat_features = 5\n",
    "n_classes = None\n",
    "cat_cardinalities = [23, 10, 32, 40, 969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ...,  True, False,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins_input = train_data_tensor[:, :-4][:, [col for col in range(train_data_tensor[:, :-4].shape[1]) if col not in [9, 10, 11]]]\n",
    "nan_mask = torch.isnan(bins_input)\n",
    "inf_mask = torch.isinf(bins_input)\n",
    "valid_rows = ~(nan_mask.any(dim=1) | inf_mask.any(dim=1))\n",
    "valid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bins_input_clean = bins_input[valid_rows][:1_000_000]\n",
    "bins = compute_bins(bins_input_clean , n_bins=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parameter_groups(model):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # Gradientê°€ í•„ìš” ì—†ëŠ” ê²½ìš° ìƒëžµ\n",
    "        if 'cont_embeddings' in name or 'bias' in name:\n",
    "            # Embedding ë ˆì´ì–´ì™€ biasì—ëŠ” weight decayë¥¼ ì ìš©í•˜ì§€ ì•ŠìŒ\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ì—ëŠ” weight decay ì ìš©\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {'params': no_decay, 'weight_decay': 0.0},\n",
    "        {'params': decay, 'weight_decay': 5e-3}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features: int,\n",
    "        cat_cardinalities: list[int],\n",
    "        bins: Optional[list[Tensor]],\n",
    "        mlp_kwargs: dict,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.cat_cardinalities = cat_cardinalities\n",
    "        # The total representation size for categorical features\n",
    "        # == the sum of one-hot representation sizes\n",
    "        # == the sum of the numbers of distinct values of all features.\n",
    "        d_cat = sum(cat_cardinalities)\n",
    "\n",
    "        # Choose any of the embeddings below.\n",
    "\n",
    "        # d_embedding = 24\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "        #     n_cont_features, d_embedding, lite=False\n",
    "        # )\n",
    "        # d_num = n_cont_features * d_embedding\n",
    "\n",
    "        # assert bins is not None\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEncoding(bins)\n",
    "        # d_num = sum(len(b) - 1 for b in bins)\n",
    "\n",
    "        assert bins is not None\n",
    "        d_embedding = 8\n",
    "        self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "            bins, d_embedding, activation=False, version='B'\n",
    "        )\n",
    "        d_num = n_cont_features * d_embedding\n",
    "\n",
    "        # d_embedding = 32\n",
    "        # self.cont_embeddings = rtdl_num_embeddings.LinearReLUEmbeddings(\n",
    "        #     n_cont_features, d_embedding\n",
    "        # )\n",
    "        # d_num = n_cont_features * d_embedding\n",
    "\n",
    "        self.backbone = rtdl_revisiting_models.MLP(d_in=d_num + d_cat, **mlp_kwargs)\n",
    "\n",
    "    def forward(self, x_cont: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        x = []\n",
    "\n",
    "        # Step 1. Embed the continuous features.\n",
    "        # Flattening is needed for MLP-like models.\n",
    "        x.append(self.cont_embeddings(x_cont).flatten(1))\n",
    "\n",
    "        # Step 2. Encode the categorical features using any strategy.\n",
    "        if x_cat is not None:\n",
    "            x.extend(\n",
    "                F.one_hot(column, cardinality)\n",
    "                for column, cardinality in zip(x_cat.T, self.cat_cardinalities)\n",
    "            )\n",
    "\n",
    "        # Step 3. Assemble the vector input for the backbone.\n",
    "        x = torch.column_stack(x)\n",
    "\n",
    "        # Step 4. Apply the backbone.\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = 'regression'\n",
    "model = Model(\n",
    "    n_cont_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    bins=bins,\n",
    "    mlp_kwargs={        \n",
    "        'n_blocks': 3,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.25,\n",
    "        'd_out': n_classes if task_type == 'multiclass' else 1,\n",
    "    },\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    # make_parameter_groups(model),\n",
    "    lr=1e-4,\n",
    "    weight_decay=5e-3 ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# loss_fn = nn.HuberLoss(delta=0.2)\n",
    "# loss_fn = R2Loss()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "timer = delu.tools.Timer()\n",
    "patience = 10\n",
    "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
    "best = {\n",
    "    \"val\": -math.inf,\n",
    "    \"epoch\": -1,\n",
    "}\n",
    "timer.run()\n",
    "\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "    weighted_residual_sum = np.sum(residuals)\n",
    "\n",
    "    # Calculate weighted sum of squared true values (denominator)\n",
    "    weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "\n",
    "    # Calculate weighted R2\n",
    "    r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export POLARS_ALLOW_FORKING_THREAD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"./epoch5_r2_-0.00616002082824707.pt\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5068/5068 [05:23<00:00, 15.66it/s, epoch=1/100, loss=0.786277, lr=1.000e-04]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:03<00:00, 15.54it/s, epoch=1/100, val_loss=0.590054, lr=1.000e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_r2 = 0.027868, val_loss_mean=0.536878, val_r2=0.005548, [time] 0:05:29.308264\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5068/5068 [05:17<00:00, 15.95it/s, epoch=2/100, loss=0.744501, lr=1.000e-04]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:03<00:00, 15.48it/s, epoch=2/100, val_loss=0.588350, lr=1.000e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_r2 = 0.044409, val_loss_mean=0.538477, val_r2=0.002747, [time] 0:10:50.716732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 199/5068 [00:14<06:02, 13.43it/s, epoch=3/100, loss=0.759624, lr=1.000e-04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_dl, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m phar:\n\u001b[1;32m      6\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mphar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(res)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:541\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 541\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:525\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m     \u001b[43manswer_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     deliver_challenge(c, authkey)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:769\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[0;32m--> 769\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m message[:\u001b[38;5;28mlen\u001b[39m(CHALLENGE)] \u001b[38;5;241m==\u001b[39m CHALLENGE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message\n\u001b[1;32m    771\u001b[0m message \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;28mlen\u001b[39m(CHALLENGE):]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_pred_list = []\n",
    "    with tqdm(train_dl, total=len(train_dl), leave=True) as phar:\n",
    "        i = 0\n",
    "        for train_tensor in phar:\n",
    "            optimizer.zero_grad()\n",
    "            X_input = train_tensor[0][:, :-4].to(device)\n",
    "            y_input = train_tensor[0][:, -4].to(device)\n",
    "            w_input = train_tensor[0][:, -3].to(device)\n",
    "            symbol_input    = train_tensor[0][:, -2].to(device)\n",
    "            time_input      = train_tensor[0][:, -1].to(device)            \n",
    "            x_cont_input = X_input[:, [col for col in range(X_input.shape[1]) if col not in [9, 10, 11]]]\n",
    "\n",
    "            x_cat_input = X_input[:, [9, 10, 11]]\n",
    "            x_cat_input = torch.concat(\n",
    "                [x_cat_input, symbol_input.unsqueeze(-1), time_input.unsqueeze(-1)], axis=1\n",
    "            ).to(torch.int64)\n",
    "\n",
    "            # Replace NaNs/Infs in x_cont_input with 0\n",
    "            x_cont_input = torch.where(torch.isnan(x_cont_input) | torch.isinf(x_cont_input), torch.tensor(0.0, device=x_cont_input.device), x_cont_input)\n",
    "            output = model(x_cont_input, x_cat_input).squeeze(-1)\n",
    "            loss = loss_fn(output, y_input)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                model.parameters(), \n",
    "                clip_value=1.0,\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                train_pred_list.append((output, y_input, w_input))\n",
    "                phar.set_postfix(\n",
    "                    OrderedDict(\n",
    "                        epoch=f'{epoch + 1}/{num_epochs}',\n",
    "                        loss=f'{loss.item():.6f}',\n",
    "                        lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                    )\n",
    "                )\n",
    "            phar.update(1)\n",
    "            i += 1\n",
    "\n",
    "    weights_train = torch.cat([x[2] for x in train_pred_list]).cpu().numpy()\n",
    "    y_train = torch.cat([x[1] for x in train_pred_list]).cpu().numpy()\n",
    "    prob_train = torch.cat([x[0] for x in train_pred_list]).detach().cpu().numpy()\n",
    "    train_r2 = r2_val(y_train, prob_train, weights_train)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_pred_list = []\n",
    "    with tqdm(valid_dl, total=len(valid_dl), leave=True) as phar:\n",
    "        for valid_tensor in phar:\n",
    "            X_valid = valid_tensor[0][:, :-4].to(device)\n",
    "            y_valid = valid_tensor[0][:, -4].to(device)\n",
    "            w_valid = valid_tensor[0][:, -3].to(device)\n",
    "            symbol_valid = valid_tensor[0][:, -2].to(device)\n",
    "            time_valid = valid_tensor[0][:, -1].to(device)\n",
    "            \n",
    "            x_cont_valid = X_valid[:, [col for col in range(X_valid.shape[1]) if col not in [9, 10, 11]]]\n",
    "            x_cont_valid = torch.where(\n",
    "                torch.isnan(x_cont_valid) | torch.isinf(x_cont_valid),\n",
    "                torch.tensor(0.0, device=x_cont_valid.device), \n",
    "            x_cont_valid)\n",
    "            \n",
    "            x_cat_valid = X_valid[:, [9, 10, 11]]\n",
    "            x_cat_valid = (torch.concat(\n",
    "                [x_cat_valid, symbol_valid.unsqueeze(-1),time_valid.unsqueeze(-1)], axis=1)\n",
    "            ).to(torch.int64)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_cont_valid, x_cat_valid).squeeze(-1)\n",
    "\n",
    "            val_loss = loss_fn(y_pred, y_valid)\n",
    "            valid_loss_list.append(val_loss)\n",
    "            valid_pred_list.append((y_pred, y_valid, w_valid))\n",
    "            phar.set_postfix(\n",
    "                OrderedDict(\n",
    "                    epoch=f'{epoch + 1}/{num_epochs}',\n",
    "                    val_loss=f'{val_loss.item():.6f}',\n",
    "                    lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                )\n",
    "            )\n",
    "            phar.update(1)\n",
    "            i += 1\n",
    "\n",
    "        valid_loss_mean = sum(valid_loss_list) / len(valid_loss_list)\n",
    "        weights_eval = torch.cat([x[2] for x in valid_pred_list]).cpu().numpy()\n",
    "        y_eval = torch.cat([x[1] for x in valid_pred_list]).cpu().numpy()\n",
    "        prob_eval = torch.cat([x[0] for x in valid_pred_list]).cpu().numpy()\n",
    "        val_r2 = r2_val(y_eval, prob_eval, weights_eval)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: train_r2 = {train_r2:.6f}, val_loss_mean={valid_loss_mean:.6f}, val_r2={val_r2:.6f}, [time] {timer}\")\n",
    "\n",
    "        if val_r2 > best[\"val\"]:\n",
    "            print(\"ðŸŒ¸ New best epoch! ðŸŒ¸\")\n",
    "            best = {\"val\": val_r2, \"epoch\": epoch}\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'r2': val_r2,\n",
    "        }\n",
    "        torch.save(checkpoint, f'epoch{epoch}_r2_{val_r2}.pt')\n",
    "    print()\n",
    "    early_stopping.update(val_r2)\n",
    "    if early_stopping.should_stop():\n",
    "        print(\"Early stop\")\n",
    "        break\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'r2': val_r2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
