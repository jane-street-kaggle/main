{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "#import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "# 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations (for nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seed = 42\n",
    "    target_col = \"responder_6\"\n",
    "    # feature_cols = [\"symbol_id\", \"time_id\"] + [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "    feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "    \n",
    "    model_paths = [\n",
    "        #\"/kaggle/input/js24-train-gbdt-model-with-lags-singlemodel/result.pkl\",\n",
    "        #\"/kaggle/input/js24-trained-gbdt-model/result.pkl\",\n",
    "        \"/kaggle/input/js-xs-nn-trained-model\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data (for nn training - calculate cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalid = pl.scan_parquet(\\n    f\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet/\"\\n).collect().to_pandas()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "valid = pl.scan_parquet(\n",
    "    f\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet/\"\n",
    ").collect().to_pandas()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model (for nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom R2 metric for validation\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return r2\n",
    "\n",
    "\n",
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1))  # 输出层\n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 5 * self.model(x).squeeze(-1)  # 输出为一维张量\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w  # 考虑样本权重\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            # r2_val\n",
    "            val_r_square = r2_val(y, prob, weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                                               verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# kaggle load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(date_id_range=None, time_id_range=None, columns=None, return_type='pl'):\n",
    "#     data_dir = '/kaggle/input/jane-street-real-time-market-data-forecasting'\n",
    "#     data = pl.scan_parquet(f'{data_dir}/train.parquet')\n",
    "    \n",
    "#     if date_id_range is not None:\n",
    "#         start_date, end_date = date_id_range\n",
    "#         data = data.filter((pl.col(\"date_id\") >= start_date) & (pl.col(\"date_id\") <= end_date))\n",
    "    \n",
    "#     if time_id_range is not None:\n",
    "#         start_time, end_time = time_id_range\n",
    "#         data = data.filter((pl.col(\"time_id\") >= start_time) & (pl.col(\"time_id\") <= end_time))\n",
    "    \n",
    "#     if columns is not None:\n",
    "#         data = data.select(columns)\n",
    "\n",
    "#     if return_type == 'pd':\n",
    "#         return data.collect().to_pandas()\n",
    "#     else:\n",
    "#         return data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "local load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_preprocessed = pl.scan_parquet(\n",
    "#     f\"/home/jupyter-chan/.cache/kagglehub/datasets/yuanweijun/js24-preprocessing-create-lags/versions/1/training.parquet/\"\n",
    "#     ).collect().to_pandas()\n",
    "# valid_preprocessed = pl.scan_parquet(\n",
    "#     f\"/home/jupyter-chan/.cache/kagglehub/datasets/yuanweijun/js24-preprocessing-create-lags/versions/1/validation.parquet/\"\n",
    "#     ).collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train_preprocessed, valid_preprocessed], ignore_index=True)\n",
    "# data = pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_date = data.select(pl.col(\"date_id\").min()).item()\n",
    "# max_date = data.select(pl.col(\"date_id\").max()).item()\n",
    "\n",
    "# print(f\"Min date_id: {min_date}\")\n",
    "# print(f\"Max date_id: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pl.scan_parquet(f'./data/train.parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_date = data.select(pl.col(\"date_id\").min()).item()\n",
    "# max_date = data.select(pl.col(\"date_id\").max()).item()\n",
    "\n",
    "# print(f\"Min date_id: {min_date}\")\n",
    "# print(f\"Max date_id: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(date_id_range=None, time_id_range=None, columns=None, return_type='pl'):\n",
    "    # data = pd.concat([train_preprocessed, valid_preprocessed], ignore_index=True)\n",
    "    # data = pl.DataFrame(data)\n",
    "    data = pl.scan_parquet(f'./data/train.parquet').collect() # 내꺼로 바꾸기 lag 없는거\n",
    "    \n",
    "    if date_id_range is not None:\n",
    "        start_date, end_date = date_id_range\n",
    "        data = data.filter((pl.col(\"date_id\") >= start_date) & (pl.col(\"date_id\") <= end_date))\n",
    "    \n",
    "    if time_id_range is not None:\n",
    "        start_time, end_time = time_id_range\n",
    "        data = data.filter((pl.col(\"time_id\") >= start_time) & (pl.col(\"time_id\") <= end_time))\n",
    "    \n",
    "    if columns is not None:\n",
    "        data = data.select(columns)\n",
    "\n",
    "    if return_type == 'pd':\n",
    "        return data.to_pandas()\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 보류\n",
    "# def load_data_stacking(data_with_cat=None,date_id_range=None, time_id_range=None, columns=None, return_type='pl'):\n",
    "#     data = pd.concat([train_preprocessed, valid_preprocessed], ignore_index=True)\n",
    "#     data = pl.DataFrame(data)\n",
    "#     data_with_cat = pl.DataFrame(data_with_cat)\n",
    "    \n",
    "#     if date_id_range is not None:\n",
    "#         start_date, end_date = date_id_range\n",
    "#         data = data.filter((pl.col(\"date_id\") >= start_date) & (pl.col(\"date_id\") <= end_date))\n",
    "#         print('data1 shape:',data.shape)\n",
    "#         data_with_cat = data_with_cat.with_columns(data[\"date_id\"])\n",
    "#         print('data2 shape:', data_with_cat.shape)\n",
    "#         data = pl.concat([data,data_with_cat.select(['catboost_pred'])], how=\"horizontal\")\n",
    "#         print('data3 shape:',data.shape)\n",
    "        \n",
    "#     if time_id_range is not None:\n",
    "#         start_time, end_time = time_id_range\n",
    "#         data = data.filter((pl.col(\"time_id\") >= start_time) & (pl.col(\"time_id\") <= end_time))\n",
    "    \n",
    "#     if columns is not None:\n",
    "#         data = data.select(columns)\n",
    "\n",
    "#     if return_type == 'pd':\n",
    "#         return data.to_pandas()\n",
    "#     else:\n",
    "#         return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true, y_pred, weights):\n",
    "    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "    denominator = np.sum(weights * (y_true ** 2))\n",
    "    r2_score = 1 - (numerator / denominator)\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'responder_6'\n",
    "FEAT_COLS_CAT = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "FEAT_COLS_LGB = [f\"feature_{i:02d}\" for i in range(79)]+ ['responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1',\n",
    "       'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1',\n",
    "       'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_lgb_kfold_single(total_days=1699, n_splits=5, save_model=True, save_path='modellgb/'):\n",
    "    if save_model and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # data_with_cat = data_with_cat\n",
    "    \n",
    "    max_valid_days = 1200\n",
    "    valid_days = min(total_days, max_valid_days)\n",
    "    valid_start = 1699 - valid_days\n",
    "    \n",
    "    fold_size = valid_days // n_splits\n",
    "    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n",
    "    \n",
    "    lgb_models = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        valid_range = folds[fold_idx]\n",
    "        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n",
    "        print(f'Fold {fold_idx}: Training LGB')\n",
    "        \n",
    "        # 검증 데이터 로드\n",
    "        # valid_data = load_data_stacking(data_with_cat = data_with_cat,\n",
    "        #                                 date_id_range=valid_range,\n",
    "        #                      #columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "        #                        columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "        #                      return_type='pl')\n",
    "        \n",
    "        valid_data = load_data(date_id_range=valid_range,\n",
    "                             #columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "                               columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "                             return_type='pl')\n",
    "        \n",
    "        # 학습 데이터 로드\n",
    "        train_data = None\n",
    "        for train_range in train_ranges:\n",
    "            # partial_train_data = load_data_stacking(data_with_cat = data_with_cat,\n",
    "            #                                         date_id_range=train_range,\n",
    "            #                              # columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "            #                                columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "            #                              return_type='pl')\n",
    "            partial_train_data = load_data(date_id_range=train_range,\n",
    "                                         # columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "                                           columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_LGB + [TARGET],\n",
    "                                         return_type='pl')\n",
    "\n",
    "            if train_data is None:\n",
    "                train_data = partial_train_data\n",
    "            else:\n",
    "                train_data = train_data.vstack(partial_train_data)\n",
    "                \n",
    "        print(f\"Train Data (before making ds) shape: {train_data.shape}\")\n",
    "        print(f\"Valid Data (before making ds) shape: {valid_data.shape}\")\n",
    "        \n",
    "        # LightGBM 데이터셋 생성\n",
    "        train_ds = lgb.Dataset(train_data[FEAT_COLS_LGB+['weight']].to_pandas(),\n",
    "                             label=train_data[TARGET].to_pandas(),\n",
    "                             weight=train_data['weight'].to_pandas())\n",
    "        valid_ds = lgb.Dataset(valid_data[FEAT_COLS_LGB+['weight']].to_pandas(),\n",
    "                             label=valid_data[TARGET].to_pandas(),\n",
    "                             weight=valid_data['weight'].to_pandas(),\n",
    "                             reference=train_ds)\n",
    "        \n",
    "        # LightGBM 파라미터\n",
    "        LGB_PARAMS = {\n",
    "            'objective': 'regression_l2',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'random_state': 42,\n",
    "            'device': 'gpu',\n",
    "        }\n",
    "        \n",
    "        # 콜백 함수\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(100),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "        \n",
    "        # 모델 학습\n",
    "        model = lgb.train(\n",
    "            LGB_PARAMS,\n",
    "            train_ds,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[train_ds, valid_ds],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        lgb_models.append(model)\n",
    "        \n",
    "        # R2 점수 계산\n",
    "        y_valid_pred = model.predict(valid_data[FEAT_COLS_LGB+['weight']].to_pandas())\n",
    "        r2_score = calculate_r2(valid_data[TARGET].to_pandas(), y_valid_pred, valid_data['weight'].to_pandas())\n",
    "        print(f\"LGB Fold {fold_idx} validation R2 score: {r2_score}\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    if save_model:\n",
    "        joblib.dump(lgb_models, os.path.join(save_path, \"lgb_models.pkl\"))\n",
    "        print(\"Saved all models to lgb_models.pkl\")\n",
    "    \n",
    "    return lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def train_catboost_kfold_single(total_days=1699, n_splits=5, cat_features=None, save_model=True, save_path='modelcat/'):\n",
    "    \n",
    "    if save_model and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    max_valid_days = 1200\n",
    "    valid_days = min(total_days, max_valid_days)\n",
    "    valid_start = 1699 - valid_days\n",
    "    \n",
    "    fold_size = valid_days // n_splits\n",
    "    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n",
    "    \n",
    "    catboost_models = []\n",
    "    data_fractions = []\n",
    "    \n",
    "    for fold_idx in range(n_splits):\n",
    "        valid_range = folds[fold_idx]\n",
    "        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n",
    "        print(f'Fold {fold_idx}: Training Catboost')\n",
    "        \n",
    "        # 검증 데이터 로드\n",
    "        valid_data = load_data(date_id_range=valid_range,\n",
    "                             #columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "                               columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_CAT + [TARGET],\n",
    "                             return_type='pl')\n",
    "        \n",
    "        # 학습 데이터 로드\n",
    "        train_data = None\n",
    "        for train_range in train_ranges:\n",
    "            partial_train_data = load_data(date_id_range=train_range,\n",
    "                                         # columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n",
    "                                           columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS_CAT + [TARGET],\n",
    "                                         return_type='pl')\n",
    "            if train_data is None:\n",
    "                train_data = partial_train_data\n",
    "            else:\n",
    "                train_data = train_data.vstack(partial_train_data)\n",
    "        \n",
    "        # CatBoost 모델 학습\n",
    "        catboost_model = CatBoostRegressor(\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            iterations=1000,\n",
    "            learning_rate=0.03,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100,\n",
    "            cat_features=cat_features,\n",
    "            task_type='GPU'\n",
    "        )\n",
    "        \n",
    "        # Polars to pandas conversion for CatBoost\n",
    "        train_df = train_data.to_pandas()\n",
    "        valid_df = valid_data.to_pandas()\n",
    "\n",
    "        print(f\"Use categorical features: {cat_features}\")\n",
    "        print(f\"Train shape: {train_df.shape}\")\n",
    "        print(f\"Valid shape: {valid_df.shape}\")\n",
    "        \n",
    "        catboost_model.fit(\n",
    "            #train_df[cat_features],  # FEAT_COLS 대신 cat_features만 사용\n",
    "            #train_df[FEAT_COLS],\n",
    "            train_df[FEAT_COLS_CAT+['symbol_id', 'weight']],\n",
    "            train_df[TARGET],\n",
    "            # eval_set=(valid_df[cat_features], valid_df[TARGET]),\n",
    "            # eval_set=(valid_df[FEAT_COLS], valid_df[TARGET]),\n",
    "            eval_set=(valid_df[FEAT_COLS_CAT+['symbol_id', 'weight']], valid_df[TARGET]),\n",
    "            sample_weight=train_df['weight']\n",
    "        )\n",
    "        \n",
    "        # 예측값 생성\n",
    "        # valid_df['catboost_pred'] = catboost_model.predict(valid_df[cat_features])\n",
    "        # valid_df['catboost_pred'] = catboost_model.predict(valid_df[FEAT_COLS])\n",
    "        train_df['catboost_pred'] = catboost_model.predict(train_df[FEAT_COLS_CAT+['symbol_id','weight']])\n",
    "        valid_df['catboost_pred'] = catboost_model.predict(valid_df[FEAT_COLS_CAT+['symbol_id','weight']])\n",
    "        combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "        data_fractions.append(combined_df)\n",
    "        \n",
    "        r2_score = calculate_r2(valid_df[TARGET], valid_df['catboost_pred'], valid_df['weight'])\n",
    "        print(f\"Catboost Fold {fold_idx} validation R2 score: {r2_score}\")\n",
    "        \n",
    "        catboost_models.append(catboost_model)\n",
    "\n",
    "    data_with_cat = pd.concat(data_fractions, ignore_index=True)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if save_model:\n",
    "        joblib.dump(catboost_models, os.path.join(save_path, \"catboost_models.pkl\"))\n",
    "        print(\"Saved all models to catboost_models.pkl\")\n",
    "\n",
    "    return data_with_cat, catboost_models\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost_holdout(total_days=1699, train_days=680, validation_days=170, cat_features=None, save_model=True, save_path='modelcat2/'):\n",
    "    if save_model and not os.path.exists(save_path): # save path 없으면 만들기 저장할곳\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Define validation and training range\n",
    "    valid_start = total_days - validation_days\n",
    "    valid_range = (valid_start, total_days - 1)  # Last 170 days\n",
    "    train_start = valid_start - train_days\n",
    "    train_range = (train_start, valid_start - 1)  # Train on last `train_days` before validation\n",
    "    \n",
    "    print(f\"Validation range: {valid_range}\")\n",
    "    print(f\"Training range: {train_range}\")\n",
    "\n",
    "    # Load validation data\n",
    "    valid_data = load_data(\n",
    "        date_id_range=valid_range,\n",
    "        columns=[\"date_id\", \"symbol_id\", \"weight\", \"time_id\"] + FEAT_COLS_CAT + [TARGET],\n",
    "        return_type='pl'\n",
    "    )\n",
    "    \n",
    "    # Load training data\n",
    "    train_data = load_data(\n",
    "        date_id_range=train_range,\n",
    "        columns=[\"date_id\", \"symbol_id\", \"weight\", \"time_id\"] + FEAT_COLS_CAT + [TARGET],\n",
    "        return_type='pl'\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for CatBoost\n",
    "    train_df = train_data.to_pandas()\n",
    "    valid_df = valid_data.to_pandas()\n",
    "\n",
    "    print(f\"Use categorical features: {cat_features}\")\n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Valid shape: {valid_df.shape}\")\n",
    "\n",
    "    # Train CatBoost model\n",
    "    catboost_model = CatBoostRegressor(\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        iterations=1000,\n",
    "        learning_rate=0.03,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100,\n",
    "        cat_features=cat_features,\n",
    "        task_type='GPU'\n",
    "    )\n",
    "\n",
    "    catboost_model.fit(\n",
    "        train_df[FEAT_COLS_CAT + ['symbol_id', 'weight', 'time_id']],\n",
    "        train_df[TARGET],\n",
    "        eval_set=(valid_df[FEAT_COLS_CAT + ['symbol_id', 'weight', 'time_id']], valid_df[TARGET]),\n",
    "        sample_weight=train_df['weight']\n",
    "    )\n",
    "    \n",
    "    # Predict and calculate R2\n",
    "    valid_df['catboost_pred'] = catboost_model.predict(valid_df[FEAT_COLS_CAT + ['symbol_id', 'weight', 'time_id']])\n",
    "    r2_score = calculate_r2(valid_df[TARGET], valid_df['catboost_pred'], valid_df['weight'])\n",
    "    print(f\"CatBoost Hold-out validation R2 score: {r2_score}\")\n",
    "\n",
    "    # Save model\n",
    "    if save_model:\n",
    "        model_path = os.path.join(save_path, \"catboost_holdout_model.pkl\") # 저장할곳\n",
    "        joblib.dump(catboost_model, model_path)\n",
    "        print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    return catboost_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Version: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation range: (1529, 1698)\n",
      "Training range: (849, 1528)\n",
      "Use categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id', 'time_id']\n",
      "Train shape: (24020920, 84)\n",
      "Valid shape: (6312328, 84)\n",
      "0:\tlearn: 0.8380845\ttest: 0.8128596\tbest: 0.8128596 (0)\ttotal: 2.29s\tremaining: 38m 6s\n",
      "100:\tlearn: 0.8335822\ttest: 0.8106257\tbest: 0.8106257 (100)\ttotal: 3m 30s\tremaining: 31m 11s\n",
      "200:\tlearn: 0.8320450\ttest: 0.8101528\tbest: 0.8101528 (200)\ttotal: 6m 57s\tremaining: 27m 41s\n"
     ]
    }
   ],
   "source": [
    "CAT_FEATURES = ['feature_09','feature_10','feature_11', 'symbol_id', 'time_id']\n",
    "# catboost_models = train_catboost_kfold_single(total_days=500, cat_features=CAT_FEATURES)\n",
    "catboost_model = train_catboost_holdout((total_days=1699, train_days=1529, validation_days=170, cat_features=CAT_FEATURES, save_path='modelcatholdout2/')\n",
    "# lgb_models = train_lgb_kfold_single(data_with_cat=data_with_cat, total_days=5)\n",
    "# lgb_models = train_lgb_kfold_single(total_days=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Version: Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거는 catboost 아닐때\n",
    "\n",
    "# # Load the model from the saved file\n",
    "\n",
    "# model_path = '/kaggle/input/jsmodel-chan2'\n",
    "# model_name = 'lgb_model'\n",
    "# models = []\n",
    "# models.append(joblib.load(f'{model_path}/{model_name}.pkl'))\n",
    "\n",
    "# print(f\"Loaded model from the saved file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거는 catboost 일때\n",
    "\n",
    "model_path = '/kaggle/input/jsmodel-chan6'\n",
    "cat_file_name = 'catboost_models'\n",
    "lgb_file_name = 'lgb_models'\n",
    "\n",
    "lgb_models = joblib.load(f'{model_path}/{lgb_file_name}.pkl')\n",
    "catboost_models = joblib.load(f'{model_path}/{cat_file_name}.pkl')\n",
    "\n",
    "print(f\"Loaded model from the saved file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_folds = 5\n",
    "# 加载最佳模型\n",
    "nn_models = []\n",
    "for fold in range(N_folds):\n",
    "    checkpoint_path = f\"{CONFIG.model_paths[0]}/nn_{fold}.model\"\n",
    "    nn_model = NN.load_from_checkpoint(checkpoint_path)\n",
    "    nn_models.append(nn_model.to(\"cuda:0\"))\n",
    "nn_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    \n",
    "    # intersection\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    # Initialize predictions with `row_id`\n",
    "    predictions = test.select('row_id').with_columns(\n",
    "        pl.lit(0.0).alias('responder_6')\n",
    "    )\n",
    "\n",
    "    # Prepare test_nn for NN processing\n",
    "    test_nn = test.clone()\n",
    "    symbol_ids = test_nn.select('symbol_id').to_numpy()[:, 0]\n",
    "\n",
    "    if lags is not None:\n",
    "        lags = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n",
    "        test_nn = test_nn.join(lags, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n",
    "    else:\n",
    "        test_nn = test_nn.with_columns(\n",
    "            (pl.lit(0.0).alias(f'responder_{idx}_lag_1') for idx in range(9))\n",
    "        )\n",
    "\n",
    "    # CatBoost predictions\n",
    "    feat_cat = test[FEAT_COLS_CAT + ['symbol_id', 'weight']].to_pandas()\n",
    "    feat_cat = feat_cat.fillna('NaN').astype(str)\n",
    "    pred_cat = [model.predict(feat_cat) for model in catboost_models]\n",
    "    pred_cat = np.mean(pred_cat, axis=0)\n",
    "\n",
    "    # LightGBM predictions\n",
    "    feat_lgb = test_nn[FEAT_COLS_LGB + ['weight'].to_pandas()\n",
    "    # feat_lgb['pred_cat'] = pred_cat\n",
    "    pred_lgb = [model.predict(feat_lgb) for model in lgb_models]\n",
    "    pred_lgb = np.mean(pred_lgb, axis=0)\n",
    "\n",
    "    # Neural network predictions\n",
    "    preds_nn = np.zeros((test_nn.shape[0],))\n",
    "    test_input = test_nn[CONFIG.feature_cols].to_pandas()\n",
    "    test_input = test_input.fillna(method='ffill').fillna(0)\n",
    "    test_input = torch.FloatTensor(test_input.values).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        for i, nn_model in enumerate(tqdm(nn_models)):\n",
    "            nn_model.eval()\n",
    "            preds_nn += nn_model(test_input).cpu().numpy() / len(nn_models)\n",
    "    print(f\"predict> nn_preds.shape =\", preds_nn.shape)\n",
    "\n",
    "    # Final prediction\n",
    "    pred = pred_cat * 0.3 + pred_lgb * 0.2 + preds_nn * 0.5\n",
    "\n",
    "    # Clip predictions to the range [-5, 5]\n",
    "    predictions = test.select('row_id').with_columns(\n",
    "        pl.Series(\n",
    "            name='responder_6',\n",
    "            values=np.clip(pred, a_min=-5, a_max=5),\n",
    "            dtype=pl.Float64\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(predictions)\n",
    "    \n",
    "    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "    assert list(predictions.columns) == ['row_id', 'responder_6']\n",
    "    assert len(predictions) == len(test)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 6010899,
     "sourceId": 9806342,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6393924,
     "sourceId": 10326505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
