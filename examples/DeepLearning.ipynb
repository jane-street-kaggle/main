{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.673569Z",
     "start_time": "2025-01-09T08:00:47.921109Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch"
   ],
   "id": "389882837ced2f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.690148Z",
     "start_time": "2025-01-09T08:00:48.685553Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, n_target,\n",
    "                 symbol_size=39, f09_size=83, f10_size=13,\n",
    "                 f11_size=540, time_size=967):\n",
    "        super().__init__()\n",
    "\n",
    "        # 임베딩 디멘션 설정\n",
    "        self.embedding_dims = {\n",
    "            'symbol': 8,\n",
    "            'f09': 4,\n",
    "            'f10': 4,\n",
    "            'f11': 4,\n",
    "            'time': 16\n",
    "        }\n",
    "\n",
    "        # 임베딩 레이어 초기화\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            'symbol': nn.Embedding(symbol_size, self.embedding_dims['symbol']),\n",
    "            'f09': nn.Embedding(f09_size, self.embedding_dims['f09']),\n",
    "            'f10': nn.Embedding(f10_size, self.embedding_dims['f10']),\n",
    "            'f11': nn.Embedding(f11_size, self.embedding_dims['f11']),\n",
    "            'time': nn.Embedding(time_size, self.embedding_dims['time'])\n",
    "        })\n",
    "\n",
    "        # 전체 임베딩 차원 계산\n",
    "        self.total_embedding_dim = sum(self.embedding_dims.values())\n",
    "        self.input_dim = self.total_embedding_dim + num_numerical_features\n",
    "\n",
    "        # Dense layers with batch normalization\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 258),\n",
    "            nn.BatchNorm1d(258),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Convolution blocks with residual connections\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv1dBlock(128, 64, kernel_size=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            Conv1dBlock(64, 32, kernel_size=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Final dense layers with batch normalization\n",
    "        self.dense2 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "\n",
    "        self.dense3 = nn.Sequential(\n",
    "            nn.Linear(1024, n_target),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, numerical_features, symbol, feature_09, feature_10, feature_11, time):\n",
    "        # Embedding processing using ModuleDict\n",
    "        embeddings = [\n",
    "            self.embeddings['symbol'](symbol),\n",
    "            self.embeddings['f09'](feature_09),\n",
    "            self.embeddings['f10'](feature_10),\n",
    "            self.embeddings['f11'](feature_11),\n",
    "            self.embeddings['time'](time)\n",
    "        ]\n",
    "\n",
    "        # Concatenate all features\n",
    "        x = torch.cat([numerical_features] + embeddings, dim=1)\n",
    "\n",
    "        # Reshape for convolution\n",
    "        x = x.unsqueeze(-1)  # Add channel dimension\n",
    "        x = self.dense1(x)\n",
    "        x = x.transpose(1, 2)  # (batch_size, channels, sequence_length)\n",
    "\n",
    "        # Apply CNN layers with residual connections\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = x + identity[:, :x.size(1), :x.size(2)]  # Residual connection\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        identity = x\n",
    "        x = self.conv2(x)\n",
    "        x = x + identity[:, :x.size(1), :x.size(2)]  # Residual connection\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Final processing\n",
    "        x = x.flatten(1)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return 5 * x  # Scale to -5 to 5 range\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Xavier/Glorot initialization for linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)"
   ],
   "id": "209d18ed69e6d440"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.741698Z",
     "start_time": "2025-01-09T08:00:48.738634Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, n_target,\n",
    "                 symbol_size=39, f09_size=83, f10_size=13,\n",
    "                 f11_size=540, time_size=967):\n",
    "        super().__init__()\n",
    "\n",
    "        # 임베딩 레이어 초기화 (크기 수정)\n",
    "        self.symbol_embedding = nn.Embedding(symbol_size, 8)\n",
    "        self.feature_09_embedding = nn.Embedding(f09_size, 4)\n",
    "        self.feature_10_embedding = nn.Embedding(f10_size, 4)\n",
    "        self.feature_11_embedding = nn.Embedding(f11_size, 4)\n",
    "        self.time_embedding = nn.Embedding(time_size, 16)\n",
    "\n",
    "        # First dense layer to get to 1024 channels\n",
    "        total_embedding_dim = 8 + 4 + 4 + 4 + 16  # embedding dimensions\n",
    "        self.input_dim = total_embedding_dim + num_numerical_features\n",
    "        self.dense1 = nn.Linear(self.input_dim, 258)\n",
    "\n",
    "        # Convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv1dBlock(128, 64, kernel_size=5),\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=4, stride=1)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            Conv1dBlock(64, 32, kernel_size=3),\n",
    "        )\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=4, stride=1)\n",
    "\n",
    "\n",
    "        # Final dense layers\n",
    "        self.dense2 = nn.Linear(6976, 1024)  # //8은 pooling의 총 효과\n",
    "        self.dense3 = nn.Linear(1024, n_target)\n",
    "        self.tanh = nn.Tanh()  # tanh 활성화 함수 추가\n",
    "\n",
    "    def forward(self, numerical_features, symbol, feature_09, feature_10, feature_11, time):\n",
    "        # Embedding 처리\n",
    "        symbol_emb = self.symbol_embedding(symbol)           # (batch_size, 8)\n",
    "        f09_emb = self.feature_09_embedding(feature_09)     # (batch_size, 4)\n",
    "        f10_emb = self.feature_10_embedding(feature_10)     # (batch_size, 4)\n",
    "        f11_emb = self.feature_11_embedding(feature_11)     # (batch_size, 4)\n",
    "        time_emb = self.time_embedding(time)                # (batch_size, 16)\n",
    "\n",
    "        # 모든 특성을 concatenate\n",
    "        x = torch.cat([\n",
    "            numerical_features,\n",
    "            symbol_emb,\n",
    "            f09_emb,\n",
    "            f10_emb,\n",
    "            f11_emb,\n",
    "            time_emb\n",
    "        ], dim=1)  # (batch_size, total_features)\n",
    "\n",
    "        # 각 feature를 독립적으로 1024 차원으로 확장\n",
    "        x = x.unsqueeze(-1)  # (batch_size, total_features, 1)\n",
    "        x = self.dense1(x)   # (batch_size, total_features, 1024)\n",
    "        x = x.transpose(1, 2)  # (batch_size, 1024, total_features)\n",
    "\n",
    "        # CNN 처리\n",
    "        x = self.conv1(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Dense layers\n",
    "        x = x.flatten(1)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        # tanh 활성화 함수를 적용하고 5를 곱해 -5~5 범위로 조정\n",
    "        x = 5 * self.tanh(x)\n",
    "\n",
    "        return x"
   ],
   "id": "eb0f0550a0f73ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.795568Z",
     "start_time": "2025-01-09T08:00:48.792682Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, n_target,\n",
    "                 symbol_size=39, f09_size=83, f10_size=13,\n",
    "                 f11_size=540, time_size=967):\n",
    "        super().__init__()\n",
    "\n",
    "        # 임베딩 레이어 초기화\n",
    "        self.symbol_embedding = nn.Embedding(symbol_size, 8)\n",
    "        self.feature_09_embedding = nn.Embedding(f09_size, 4)\n",
    "        self.feature_10_embedding = nn.Embedding(f10_size, 4)\n",
    "        self.feature_11_embedding = nn.Embedding(f11_size, 4)\n",
    "        self.time_embedding = nn.Embedding(time_size, 16)\n",
    "\n",
    "        # Calculate total input size\n",
    "        total_embedding_dim = 8 + 4 + 4 + 4 + 16  # embedding dimensions\n",
    "        total_input_size = total_embedding_dim + num_numerical_features\n",
    "\n",
    "        # Dense layers with batch normalization and dropout\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(total_input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(64, n_target)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, numerical_features, symbol, feature_09, feature_10, feature_11, time):\n",
    "        # Embedding 처리\n",
    "        symbol_emb = self.symbol_embedding(symbol)\n",
    "        f09_emb = self.feature_09_embedding(feature_09)\n",
    "        f10_emb = self.feature_10_embedding(feature_10)\n",
    "        f11_emb = self.feature_11_embedding(feature_11)\n",
    "        time_emb = self.time_embedding(time)\n",
    "\n",
    "        # 모든 특성을 concatenate\n",
    "        x = torch.cat([\n",
    "            numerical_features,\n",
    "            symbol_emb,\n",
    "            f09_emb,\n",
    "            f10_emb,\n",
    "            f11_emb,\n",
    "            time_emb\n",
    "        ], dim=1)\n",
    "\n",
    "        # Dense layers\n",
    "        x = self.dense_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # Apply tanh and scale to [-5, 5] range\n",
    "        x = 5 * self.tanh(x)\n",
    "\n",
    "        return x"
   ],
   "id": "4bc4bd7d2d2841cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.828109Z",
     "start_time": "2025-01-09T08:00:48.823256Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, numerical_columns, target_columns, weight_columns=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: pandas DataFrame containing all features\n",
    "            numerical_columns: list of column names for numerical features\n",
    "            target_columns: list of target column names\n",
    "            weight_columns: list of weight column names (optional)\n",
    "        \"\"\"\n",
    "        self.numerical_features = torch.FloatTensor(data[numerical_columns].values)\n",
    "        self.symbol = torch.LongTensor(data['symbol_id'].values)\n",
    "        self.feature_09 = torch.LongTensor(data['feature_09'].values)\n",
    "        self.feature_10 = torch.LongTensor(data['feature_10'].values)\n",
    "        self.feature_11 = torch.LongTensor(data['feature_11'].values)\n",
    "        self.time = torch.LongTensor(data['time_id'].values)\n",
    "\n",
    "        # Multi-target 처리\n",
    "        self.targets = torch.FloatTensor(data[target_columns].values)\n",
    "\n",
    "        # 가중치 처리 (옵션)\n",
    "        if weight_columns:\n",
    "            self.weights = torch.FloatTensor(data[weight_columns].values)\n",
    "        else:\n",
    "            self.weights = torch.ones_like(self.targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'numerical_features': self.numerical_features[idx],\n",
    "            'symbol_id': self.symbol[idx],\n",
    "            'feature_09': self.feature_09[idx],\n",
    "            'feature_10': self.feature_10[idx],\n",
    "            'feature_11': self.feature_11[idx],\n",
    "            'time_id': self.time[idx],\n",
    "            'targets': self.targets[idx],\n",
    "            'weights': self.weights[idx]\n",
    "        }\n",
    "\n",
    "def create_data_loaders(train_data, valid_data, numerical_columns,\n",
    "                        target_columns, weight_columns=None,\n",
    "                        batch_size=256, num_workers=4):\n",
    "    \"\"\"\n",
    "    데이터로더를 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        train_data: 학습 데이터가 담긴 DataFrame\n",
    "        valid_data: 검증 데이터가 담긴 DataFrame\n",
    "        numerical_columns: 수치형 특성들의 컬럼명 리스트\n",
    "        target_columns: 타겟 변수들의 컬럼명 리스트\n",
    "        weight_columns: 가중치 컬럼명 리스트 (옵션)\n",
    "        batch_size: 배치 크기\n",
    "        num_workers: 데이터 로딩에 사용할 워커 수\n",
    "    \"\"\"\n",
    "\n",
    "    # Dataset 객체 생성\n",
    "    train_dataset = CustomDataset(train_data, numerical_columns, target_columns, weight_columns)\n",
    "    valid_dataset = CustomDataset(valid_data, numerical_columns, target_columns, weight_columns)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_r2 = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # 데이터를 디바이스로 이동\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        symbol = batch['symbol_id'].to(device)\n",
    "        feature_09 = batch['feature_09'].to(device)\n",
    "        feature_10 = batch['feature_10'].to(device)\n",
    "        feature_11 = batch['feature_11'].to(device)\n",
    "        time = batch['time_id'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        weights = batch['weights'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(numerical_features, symbol, feature_09, feature_10, feature_11, time)\n",
    "\n",
    "        # 손실과 R2 score 계산\n",
    "        loss = weighted_mse_loss(targets, outputs, weights)\n",
    "        r2 = weighted_r2_score(targets, outputs, weights)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_r2 += r2\n",
    "\n",
    "        # 배치별 진행상황 출력 (10배치마다)\n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_r2 = total_r2 / (batch_idx + 1)\n",
    "            print(f'Batch [{batch_idx+1}/{num_batches}] Loss: {avg_loss:.4f}, R2: {avg_r2:.4f}')\n",
    "\n",
    "    return total_loss / num_batches, total_r2 / num_batches\n",
    "\n",
    "def validate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_r2 = 0\n",
    "    num_batches = len(valid_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_loader):\n",
    "            numerical_features = batch['numerical_features'].to(device)\n",
    "            symbol = batch['symbol_id'].to(device)\n",
    "            feature_09 = batch['feature_09'].to(device)\n",
    "            feature_10 = batch['feature_10'].to(device)\n",
    "            feature_11 = batch['feature_11'].to(device)\n",
    "            time = batch['time_id'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            weights = batch['weights'].to(device)\n",
    "\n",
    "            outputs = model(numerical_features, symbol, feature_09, feature_10, feature_11, time)\n",
    "\n",
    "            loss = weighted_mse_loss(targets, outputs, weights)\n",
    "            r2 = weighted_r2_score(targets, outputs, weights)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_r2 += r2\n",
    "\n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_r2 = total_r2 / (batch_idx + 1)\n",
    "            print(f'Batch [{batch_idx+1}/{num_batches}] Loss: {avg_loss:.4f}, R2: {avg_r2:.4f}')\n",
    "\n",
    "    return total_loss / num_batches, total_r2 / num_batches"
   ],
   "id": "160fc9890cf75f92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:48.861193Z",
     "start_time": "2025-01-09T08:00:48.853420Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "def weighted_mse_loss(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Multi-target weighted MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: target values (batch_size, n_targets)\n",
    "        y_pred: predicted values (batch_size, n_targets)\n",
    "        weights: weights for each target (batch_size, n_targets)\n",
    "    \"\"\"\n",
    "    return torch.mean(weights * (y_true - y_pred)**2)\n",
    "\n",
    "def weighted_r2_score(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Multi-target weighted R2 score\n",
    "\n",
    "    Args:\n",
    "        y_true: target values (batch_size, n_targets)\n",
    "        y_pred: predicted values (batch_size, n_targets)\n",
    "        weights: weights for each target (batch_size, n_targets)\n",
    "\n",
    "    Returns:\n",
    "        weighted R2 score (scalar)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on CPU and converted to numpy\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    weights = weights.detach().cpu().numpy()\n",
    "\n",
    "    weights = np.repeat(weights, y_true.shape[1], axis=1)\n",
    "\n",
    "    # print(y_true.shape, y_pred.shape, weights.shape)\n",
    "    # Calculate weighted means for each target\n",
    "    weighted_mean = np.average(y_true, weights=weights, axis=0)\n",
    "\n",
    "    # Calculate total sum of squares\n",
    "    total_ss = np.sum(weights * (y_true - weighted_mean) ** 2, axis=0)\n",
    "\n",
    "    # Calculate residual sum of squares\n",
    "    residual_ss = np.sum(weights * (y_true - y_pred) ** 2, axis=0)\n",
    "\n",
    "    # Calculate R2 score for each target\n",
    "    r2_scores = 1 - (residual_ss / total_ss)\n",
    "\n",
    "    # Return mean R2 score across all targets\n",
    "    return np.mean(r2_scores)"
   ],
   "id": "48566aaac90259e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:00:57.604146Z",
     "start_time": "2025-01-09T08:00:48.913767Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "def read_parquet_partitions(base_path: str, partitions: int = 10) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for partition in range(partitions):\n",
    "        path = f'{base_path}/partition_id={partition}/part-0.parquet'\n",
    "        df = pd.read_parquet(path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Usage\n",
    "data = read_parquet_partitions('./data/train.parquet')"
   ],
   "id": "512842c70078ec84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:04.271156Z",
     "start_time": "2025-01-09T08:00:57.619031Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": "data.dropna(inplace=True)",
   "id": "75b7eb1627333e65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:04.292733Z",
     "start_time": "2025-01-09T08:01:04.290611Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df Length: 31833739\n",
      "valid_df Length: 3537083\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "data_length = data.shape[0]\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_point = int(data_length * train_ratio)\n",
    "\n",
    "train_df = data[:split_point]\n",
    "valid_df = data[split_point:]\n",
    "\n",
    "print(f\"train_df Length: {train_df.shape[0]}\")\n",
    "print(f\"valid_df Length: {valid_df.shape[0]}\")"
   ],
   "id": "ea71012929b69cbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:04.320269Z",
     "start_time": "2025-01-09T08:01:04.312785Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    "                     'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    "                     'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "                                    76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    "                     'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "                                   20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    "                     'time_id' : {i : i for i in range(968)}}"
   ],
   "id": "f59a7aa28d69c55d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:11.349964Z",
     "start_time": "2025-01-09T08:01:04.335729Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_09 range after mapping:\n",
      "Train: 0 to 21\n",
      "Valid: 0 to 21\n",
      "\n",
      "feature_10 range after mapping:\n",
      "Train: 0 to 8\n",
      "Valid: 0 to 8\n",
      "\n",
      "feature_11 range after mapping:\n",
      "Train: 0 to 30\n",
      "Valid: 0 to 30\n",
      "\n",
      "symbol_id range after mapping:\n",
      "Train: 0 to 38\n",
      "Valid: 0 to 38\n",
      "\n",
      "time_id range after mapping:\n",
      "Train: 68 to 967\n",
      "Valid: 68 to 967\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "def apply_category_mappings(df, category_mappings):\n",
    "    \"\"\"\n",
    "    주어진 매핑 딕셔너리를 사용하여 범주형 변수들을 변환하는 함수\n",
    "\n",
    "    Args:\n",
    "        df: 변환할 데이터프레임\n",
    "        category_mappings: 각 컬럼별 매핑 딕셔너리\n",
    "    \"\"\"\n",
    "    df = df.copy()  # 원본 데이터 보존\n",
    "\n",
    "    for column, mapping in category_mappings.items():\n",
    "        # 매핑되지 않은 값이 있는지 체크\n",
    "        unmapped_values = set(df[column].unique()) - set(mapping.keys())\n",
    "        if unmapped_values:\n",
    "            print(f\"Warning: {column}에서 매핑되지 않은 값 발견: {unmapped_values}\")\n",
    "\n",
    "        # 매핑 적용\n",
    "        df[column] = df[column].map(mapping)\n",
    "\n",
    "        # NA 값 체크\n",
    "        na_count = df[column].isna().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"Warning: {column}에서 {na_count}개의 NA 값 발견\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 데이터로더 생성 전에 매핑 적용\n",
    "train_df = apply_category_mappings(train_df, category_mappings)\n",
    "valid_df = apply_category_mappings(valid_df, category_mappings)\n",
    "\n",
    "# 매핑 후 값 범위 확인\n",
    "for column in category_mappings.keys():\n",
    "    print(f\"\\n{column} range after mapping:\")\n",
    "    print(f\"Train: {train_df[column].min()} to {train_df[column].max()}\")\n",
    "    print(f\"Valid: {valid_df[column].min()} to {valid_df[column].max()}\")"
   ],
   "id": "6491b5be03336335"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:11.376195Z",
     "start_time": "2025-01-09T08:01:11.364532Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "e4c46b054df06ecb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:17.379722Z",
     "start_time": "2025-01-09T08:01:11.429699Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "# 컬럼 정의\n",
    "numerical_columns = data.columns[data.columns.str.contains('feature')]\n",
    "target_columns = ['responder_0', 'responder_3', 'responder_6']  # 예측할 타겟들\n",
    "weight_columns = ['weight']  # 각 타겟에 대한 가중치 (옵션)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader, valid_loader = create_data_loaders(\n",
    "    train_data=train_df,\n",
    "    valid_data=valid_df,\n",
    "    numerical_columns=numerical_columns,\n",
    "    target_columns=target_columns,\n",
    "    weight_columns=weight_columns,\n",
    "    batch_size=2048\n",
    ")"
   ],
   "id": "72e18ee57714630"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:01:17.450666Z",
     "start_time": "2025-01-09T08:01:17.389147Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": [
    "# 모델 초기화\n",
    "model = DNNModel(\n",
    "    num_numerical_features=len(numerical_columns),\n",
    "    n_target=len(target_columns),\n",
    "    symbol_size=39,    # 0-38: 39개\n",
    "    f09_size=22,      # 0-21: 22개\n",
    "    f10_size=9,       # 0-8: 9개\n",
    "    f11_size=31,      # 0-30: 31개\n",
    "    time_size=968     # 0-967: 968개\n",
    ").to(device)"
   ],
   "id": "f57450394e332991"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:07:23.780331Z",
     "start_time": "2025-01-09T08:01:17.462064Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1000/15544] Loss: 0.9055, R2: -0.0247\n",
      "Batch [2000/15544] Loss: 0.8758, R2: 0.0151\n",
      "Batch [3000/15544] Loss: 0.8617, R2: 0.0331\n",
      "Batch [4000/15544] Loss: 0.8531, R2: 0.0439\n",
      "Batch [5000/15544] Loss: 0.8461, R2: 0.0516\n",
      "Batch [6000/15544] Loss: 0.8411, R2: 0.0577\n",
      "Batch [7000/15544] Loss: 0.8370, R2: 0.0625\n",
      "Batch [8000/15544] Loss: 0.8338, R2: 0.0663\n",
      "Batch [9000/15544] Loss: 0.8308, R2: 0.0697\n",
      "Batch [10000/15544] Loss: 0.8286, R2: 0.0723\n",
      "Batch [11000/15544] Loss: 0.8267, R2: 0.0747\n",
      "Batch [12000/15544] Loss: 0.8251, R2: 0.0769\n",
      "Batch [13000/15544] Loss: 0.8236, R2: 0.0787\n",
      "Batch [14000/15544] Loss: 0.8221, R2: 0.0804\n",
      "Batch [15000/15544] Loss: 0.8209, R2: 0.0819\n",
      "Epoch 1/5:\n",
      "Train Loss: 0.8201, Train R2: 0.0826\n",
      "Valid Loss: 1.1504, Valid R2: -1.3336\n",
      "--------------------------------------------------\n",
      "Batch [1000/15544] Loss: 0.8012, R2: 0.1028\n",
      "Batch [2000/15544] Loss: 0.8009, R2: 0.1040\n",
      "Batch [3000/15544] Loss: 0.8009, R2: 0.1050\n",
      "Batch [4000/15544] Loss: 0.8005, R2: 0.1052\n",
      "Batch [5000/15544] Loss: 0.8007, R2: 0.1052\n",
      "Batch [6000/15544] Loss: 0.8002, R2: 0.1057\n",
      "Batch [7000/15544] Loss: 0.7998, R2: 0.1062\n",
      "Batch [8000/15544] Loss: 0.7995, R2: 0.1066\n",
      "Batch [9000/15544] Loss: 0.7993, R2: 0.1066\n",
      "Batch [10000/15544] Loss: 0.7989, R2: 0.1067\n",
      "Batch [11000/15544] Loss: 0.7988, R2: 0.1069\n",
      "Batch [12000/15544] Loss: 0.7986, R2: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x704a96688a70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m----> 6\u001B[0m     train_loss, train_r2 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     valid_loss, valid_r2 \u001B[38;5;241m=\u001B[39m validate(model, valid_loader, device)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 91\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, valid_loader, optimizer, device)\u001B[0m\n\u001B[1;32m     88\u001B[0m total_r2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     89\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[0;32m---> 91\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# 데이터를 디바이스로 이동\u001B[39;49;00m\n\u001B[1;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumerical_features\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumerical_features\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m    \u001B[49m\u001B[43msymbol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msymbol_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1445\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1448\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1449\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1451\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1402\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1400\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m   1401\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[0;32m-> 1402\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1403\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1404\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1231\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1240\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1241\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1243\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1245\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1246\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1247\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1248\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.12/queue.py:180\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m--> 180\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get()\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnot_full\u001B[38;5;241m.\u001B[39mnotify()\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.12/threading.py:359\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 359\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    361\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "# 학습 루프 수정\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_r2 = train_epoch(model, train_loader, valid_loader, optimizer, device)\n",
    "    valid_loss, valid_r2 = validate(model, valid_loader, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train R2: {train_r2:.4f}')\n",
    "    print(f'Valid Loss: {valid_loss:.4f}, Valid R2: {valid_r2:.4f}')\n",
    "    print('-' * 50)"
   ],
   "id": "bfc33e3c6bc11c8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "b6fc89025d9af784",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
